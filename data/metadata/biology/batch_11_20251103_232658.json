[
  {
    "title": "Reconstruction of Partial Dissimilarity Matrices for Cognitive\n  Neuroscience",
    "abstract": "In cognitive neuroscience research, Representational Dissimilarity Matrices\n(RDMs) are often incomplete because pairwise similarity judgments cannot always\nbe exhaustively collected as the number of pairs rapidly increases with the\nnumber of conditions. Existing methods to fill these missing values, such as\ndeep neural network imputation, are powerful but computationally demanding and\nrelatively opaque. We introduce a simple algorithm based on geometric inference\nthat fills missing dissimilarity matrix entries using known distances. We use\ntests on publicly available empirical cognitive neuroscience datasets, as well\nas simulations, to demonstrate the method's effectiveness and robustness across\nvarying sparsity and matrix sizes. We have made this geometric reconstruction\nalgorithm, implemented in Python and MATLAB, publicly available. This method\nprovides a fast and accurate solution for completing partial dissimilarity\nmatrices in the cognitive neurosciences.",
    "authors": [
      "Denise Moerel",
      "Tijl Grootswagers"
    ],
    "publication_date": "2025-05-31T09:44:14Z",
    "arxiv_id": "http://arxiv.org/abs/2506.00484v2",
    "download_url": "http://arxiv.org/abs/2506.00484v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Biological detail and graph structure in network neuroscience",
    "abstract": "Endowing brain anatomy, dynamics, and function with a network structure is\nbecoming standard in neuroscience. In its simplest form, a network is a\ncollection of units and relationships between them. The pattern of relations\namong the units encodes numerous properties which have been shown to have a\nprofound effect on networked systems' dynamics and function. In an effort to\nstrike a balance between idealization and detail, network neuroscience studies\ntypically involve simplifying assumptions at both neural and network modeling\nlevels. However, the extent to which existing neural models depend on such\napproximations is as yet poorly understood. Here, we discuss whether and how\nincreasing neurophysiological detail and generalizing the basic simple network\nstructure often adopted in network neuroscience may help improve our\nunderstanding of brain phenomenology and function.",
    "authors": [
      "David Papo",
      "Javier M. Buldú"
    ],
    "publication_date": "2025-07-21T16:48:12Z",
    "arxiv_id": "http://arxiv.org/abs/2507.15789v1",
    "download_url": "http://arxiv.org/abs/2507.15789v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Exploring the Human Connectome Topology in Group Studies",
    "abstract": "Visually comparing brain networks, or connectomes, is an essential task in\nthe field of neuroscience. Especially relevant to the field of clinical\nneuroscience, group studies that examine differences between populations or\nchanges over time within a population enable neuroscientists to reason about\neffective diagnoses and treatments for a range of neuropsychiatric disorders.\nIn this paper, we specifically explore how visual analytics tools can be used\nto facilitate various clinical neuroscience tasks, in which observation and\nanalysis of meaningful patterns in the connectome can support patient diagnosis\nand treatment. We conduct a survey of visualization tasks that enable clinical\nneuroscience activities, and further explore how existing connectome\nvisualization tools support or fail to support these tasks. Based on our\ninvestigation of these tasks, we introduce a novel visualization tool,\nNeuroCave, to support group studies analyses. We discuss how our design\ndecisions (the use of immersive visualization, the use of hierarchical\nclustering and dimensionality reduction techniques, and the choice of visual\nencodings) are motivated by these tasks. We evaluate NeuroCave through two use\ncases that illustrate the utility of interactive connectome visualization in\nclinical neuroscience contexts. In the first use case, we study sex differences\nusing functional connectomes and discover hidden connectome patterns associated\nwith well-known cognitive differences in spatial and verbal abilities. In the\nsecond use case, we show how the utility of visualizing the brain in different\ntopological space coupled with clustering information can reveal the brain's\nintrinsic structure.",
    "authors": [
      "Johnson J. G. Keiriz",
      "Liang Zhan",
      "Morris Chukhman",
      "Olu Ajilore",
      "Alex D. Leow",
      "Angus G. Forbes"
    ],
    "publication_date": "2017-06-30T17:59:14Z",
    "arxiv_id": "http://arxiv.org/abs/1706.10297v1",
    "download_url": "http://arxiv.org/abs/1706.10297v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Technological Competence is a Precondition for Effective Implementation\n  of Virtual Reality Head Mounted Displays in Human Neuroscience: A\n  Technological Review and Meta-analysis",
    "abstract": "Immersive virtual reality (VR) emerges as a promising research and clinical\ntool. However, several studies suggest that VR induced adverse symptoms and\neffects (VRISE) may undermine the health and safety standards, and the\nreliability of the scientific results. In the current literature review, the\ntechnical reasons for the adverse symptomatology are investigated to provide\nsuggestions and technological knowledge for the implementation of VR\nhead-mounted display (HMD) systems in cognitive neuroscience. The technological\nsystematic literature indicated features pertinent to display, sound, motion\ntracking, navigation, ergonomic interactions, user experience, and computer\nhardware that should be considered by the researchers. Subsequently, a\nmeta-analysis of 44 neuroscientific or neuropsychological studies involving VR\nHMD systems was performed. The meta-analysis of the VR studies demonstrated\nthat new generation HMDs induced significantly less VRISE and marginally fewer\ndropouts.Importantly, the commercial versions of the new generation HMDs with\nergonomic interactions had zero incidents of adverse symptomatology and\ndropouts. HMDs equivalent to or greater than the commercial versions of\ncontemporary HMDs accompanied with ergonomic interactions are suitable for\nimplementation in cognitive neuroscience. In conclusion, researchers\ntechnological competency, along with meticulous methods and reports pertinent\nto software, hardware, and VRISE, are paramount to ensure the health and safety\nstandards and the reliability of neuroscientific results.",
    "authors": [
      "Panagiotis Kourtesis",
      "Simona Collina",
      "Leonidas A. A. Doumas",
      "Sarah E. MacPherson"
    ],
    "publication_date": "2021-01-20T13:48:11Z",
    "arxiv_id": "http://arxiv.org/abs/2101.08123v1",
    "download_url": "http://arxiv.org/abs/2101.08123v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Development of theoretical frameworks in neuroscience: a pressing need\n  in a sea of data",
    "abstract": "Neuroscience is undergoing dramatic progress because of the vast data streams\nderived from the new technologies product of the BRAIN initiative and other\nenterprises. As any other scientific field, neuroscience benefits from having\nclear definitions of its theoretical components and their interactions. This\nallows generating theories that integrate knowledge, provide mechanistic\ninsights, and predict results under new experimental conditions. However,\ntheoretical neuroscience is a heterogeneous field that has not yet agreed on\nhow to build theories or whether it is desirable to have an overarching theory\nor whether theories are simply tools to understand the brain. Here we advocate\nfor the need of developing theoretical frameworks as a basis of generating\ncommon theoretical structures. We enumerate the elements of theoretical\nframeworks we deem necessary for any theory in neuroscience. In particular, we\naddress the notions of paradigms, models, and scales of organizations. We then\nidentify areas with pressing needs to develop brain theories: integration of\nstatistical and dynamic approaches; multi-scale integration; coding; and\ninterpretability in the context of Artificial Intelligence. We also point out\nthat future theoretical frameworks would benefit from the incorporation of the\nprinciples of Evolution as a fundamental structure rather than purely\nmathematical or engineering principles. Rather than providing definite answers,\nthe objective of this paper is to serve as an initial and succinct presentation\nof these topics to encourage discussion and further in depth development of\neach topic.",
    "authors": [
      "Horacio G. Rotstein",
      "Fidel Santamaria"
    ],
    "publication_date": "2022-09-20T19:07:55Z",
    "arxiv_id": "http://arxiv.org/abs/2209.09953v1",
    "download_url": "http://arxiv.org/abs/2209.09953v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Brain as a complex system, harnessing systems neuroscience tools &\n  notions for an empirical approach",
    "abstract": "Finding general principles underlying brain function has been appealing to\nscientists. Indeed, in some branches of science like physics and chemistry (and\nto some degree biology) a general theory often can capture the essence of a\nwide range of phenomena. Whether we can find such principles in neuroscience,\nand [assuming they do exist] what those principles are, are important\nquestions. Abstracting the brain as a complex system is one of the perspectives\nthat may help us answer this question.\n  While it is commonly accepted that the brain is a (or even the) prominent\nexample of a complex system, the far reaching implications of this are still\narguably overlooked in our approaches to neuroscientific questions. One of the\nreasons for the lack of attention could be the apparent difference in foci of\ninvestigations in these two fields -- neuroscience and complex systems. This\nthesis is an effort toward providing a bridge between systems neuroscience and\ncomplex systems by harnessing systems neuroscience tools & notions for building\nempirical approaches toward the brain as a complex system.\n  Perhaps, in the spirit of searching for principles, we should abstract and\napproach the brain as a complex adaptive system as the more complete\nperspective (rather than just a complex system). In the end, the brain, even\nthe most \"complex system\", need to survive in the environment. Indeed, in the\nfield of complex adaptive systems, the intention is understanding very similar\nquestions in nature. As an outlook, we also touch on some research directions\npertaining to the adaptivity of the brain as well.",
    "authors": [
      "Shervin Safavi"
    ],
    "publication_date": "2023-12-20T23:18:10Z",
    "arxiv_id": "http://arxiv.org/abs/2312.13478v1",
    "download_url": "http://arxiv.org/abs/2312.13478v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Universal Differential Equations as a Common Modeling Language for\n  Neuroscience",
    "abstract": "The unprecedented availability of large-scale datasets in neuroscience has\nspurred the exploration of artificial deep neural networks (DNNs) both as\nempirical tools and as models of natural neural systems. Their appeal lies in\ntheir ability to approximate arbitrary functions directly from observations,\ncircumventing the need for cumbersome mechanistic modeling. However, without\nappropriate constraints, DNNs risk producing implausible models, diminishing\ntheir scientific value. Moreover, the interpretability of DNNs poses a\nsignificant challenge, particularly with the adoption of more complex\nexpressive architectures. In this perspective, we argue for universal\ndifferential equations (UDEs) as a unifying approach for model development and\nvalidation in neuroscience. UDEs view differential equations as\nparameterizable, differentiable mathematical objects that can be augmented and\ntrained with scalable deep learning techniques. This synergy facilitates the\nintegration of decades of extensive literature in calculus, numerical analysis,\nand neural modeling with emerging advancements in AI into a potent framework.\nWe provide a primer on this burgeoning topic in scientific machine learning and\ndemonstrate how UDEs fill in a critical gap between mechanistic,\nphenomenological, and data-driven models in neuroscience. We outline a flexible\nrecipe for modeling neural systems with UDEs and discuss how they can offer\nprincipled solutions to inherent challenges across diverse neuroscience\napplications such as understanding neural computation, controlling neural\nsystems, neural decoding, and normative modeling.",
    "authors": [
      "Ahmed ElGazzar",
      "Marcel van Gerven"
    ],
    "publication_date": "2024-03-21T16:07:30Z",
    "arxiv_id": "http://arxiv.org/abs/2403.14510v1",
    "download_url": "http://arxiv.org/abs/2403.14510v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing\n  Environments",
    "abstract": "Modern AI models, such as large language models, are usually trained once on\na huge corpus of data, potentially fine-tuned for a specific task, and then\ndeployed with fixed parameters. Their training is costly, slow, and gradual,\nrequiring billions of repetitions. In stark contrast, animals continuously\nadapt to the ever-changing contingencies in their environments. This is\nparticularly important for social species, where behavioral policies and reward\noutcomes may frequently change in interaction with peers. The underlying\ncomputational processes are often marked by rapid shifts in an animal's\nbehaviour and rather sudden transitions in neuronal population activity. Such\ncomputational capacities are of growing importance for AI systems operating in\nthe real world, like those guiding robots or autonomous vehicles, or for\nagentic AI interacting with humans online. Can AI learn from neuroscience? This\nPerspective explores this question, integrating the literature on continual and\nin-context learning in AI with the neuroscience of learning on behavioral tasks\nwith shifting rules, reward probabilities, or outcomes. We will outline an\nagenda for how specifically insights from neuroscience may inform current\ndevelopments in AI in this area, and - vice versa - what neuroscience may learn\nfrom AI, contributing to the evolving field of NeuroAI.",
    "authors": [
      "Daniel Durstewitz",
      "Bruno Averbeck",
      "Georgia Koppe"
    ],
    "publication_date": "2025-07-02T19:30:57Z",
    "arxiv_id": "http://arxiv.org/abs/2507.02103v1",
    "download_url": "http://arxiv.org/abs/2507.02103v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Mind Meets Space: Rethinking Agentic Spatial Intelligence from a\n  Neuroscience-inspired Perspective",
    "abstract": "Recent advances in agentic AI have led to systems capable of autonomous task\nexecution and language-based reasoning, yet their spatial reasoning abilities\nremain limited and underexplored, largely constrained to symbolic and\nsequential processing. In contrast, human spatial intelligence, rooted in\nintegrated multisensory perception, spatial memory, and cognitive maps, enables\nflexible, context-aware decision-making in unstructured environments.\nTherefore, bridging this gap is critical for advancing Agentic Spatial\nIntelligence toward better interaction with the physical 3D world. To this end,\nwe first start from scrutinizing the spatial neural models as studied in\ncomputational neuroscience, and accordingly introduce a novel computational\nframework grounded in neuroscience principles. This framework maps core\nbiological functions to six essential computation modules: bio-inspired\nmultimodal sensing, multi-sensory integration, egocentric-allocentric\nconversion, an artificial cognitive map, spatial memory, and spatial reasoning.\nTogether, these modules form a perspective landscape for agentic spatial\nreasoning capability across both virtual and physical environments. On top, we\nconduct a framework-guided analysis of recent methods, evaluating their\nrelevance to each module and identifying critical gaps that hinder the\ndevelopment of more neuroscience-grounded spatial reasoning modules. We further\nexamine emerging benchmarks and datasets and explore potential application\ndomains ranging from virtual to embodied systems, such as robotics. Finally, we\noutline potential research directions, emphasizing the promising roadmap that\ncan generalize spatial reasoning across dynamic or unstructured environments.\nWe hope this work will benefit the research community with a\nneuroscience-grounded perspective and a structured pathway. Our project page\ncan be found at Github.",
    "authors": [
      "Bui Duc Manh",
      "Soumyaratna Debnath",
      "Zetong Zhang",
      "Shriram Damodaran",
      "Arvind Kumar",
      "Yueyi Zhang",
      "Lu Mi",
      "Erik Cambria",
      "Lin Wang"
    ],
    "publication_date": "2025-09-11T05:23:22Z",
    "arxiv_id": "http://arxiv.org/abs/2509.09154v1",
    "download_url": "http://arxiv.org/abs/2509.09154v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "An analysis of the abstracts presented at the annual meetings of the\n  Society for Neuroscience from 2001 to 2006",
    "abstract": "We extracted and processed abstract data from the SFN annual meeting\nabstracts during the period 2001-2006, using techniques and software from\nnatural language processing, database management, and data visualization and\nanalysis. An important first step in the process was the application of data\ncleaning and disambiguation methods to construct a unified database, since the\ndata were too noisy to be of full utility in the raw form initially available.\nThe resulting co-author graph in 2006, for example, had 39,645 nodes (with an\nestimated 6% error rate in our disambiguation of similar author names) and\n13,979 abstracts, with an average of 1.5 abstracts per author, 4.3 authors per\nabstract, and 5.96 collaborators per author (including all authors on shared\nabstracts). Recent work in related areas has focused on reputational indices\nsuch as highly cited papers or scientists and journal impact factors, and to a\nlesser extent on creating visual maps of the knowledge space. In contrast,\nthere has been relatively less work on the demographics and community\nstructure, the dynamics of the field over time to examine major research trends\nand the structure of the sources of research funding. In this paper we examined\neach of these areas in order to gain an objective overview of contemporary\nneuroscience. Some interesting findings include a high geographical\nconcentration of neuroscience research in north eastern United States, a\nsurprisingly large transient population (60% of the authors appear in only one\nout of the six studied years), the central role played by the study of\nneurodegenerative disorders in the neuroscience community structure, and an\napparent growth of behavioral/systems neuroscience with a corresponding\nshrinkage of cellular/molecular neuroscience over the six year period.",
    "authors": [
      "J. M. Lin",
      "J. W. Bohland",
      "P. Andrews",
      "G. Burns",
      "C. B. Allen",
      "P. P. Mitra"
    ],
    "publication_date": "2007-10-12T17:27:21Z",
    "arxiv_id": "http://arxiv.org/abs/0710.2523v2",
    "download_url": "http://arxiv.org/abs/0710.2523v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Neurosciences and 6G: Lessons from and Needs of Communicative Brains",
    "abstract": "This paper presents the first comprehensive tutorial on a promising research\nfield located at the frontier of two well-established domains: Neurosciences\nand wireless communications, motivated by the ongoing efforts to define how the\nsixth generation of mobile networks (6G) will be. In particular, this tutorial\nfirst provides a novel integrative approach that bridges the gap between these\ntwo, seemingly disparate fields. Then, we present the state-of-the-art and key\nchallenges of these two topics. In particular, we propose a novel\nsystematization that divides the contributions into two groups, one focused on\nwhat neurosciences will offer to 6G in terms of new applications and systems\narchitecture (Neurosciences for Wireless), and the other focused on how\nwireless communication theory and 6G systems can provide new ways to study the\nbrain (Wireless for Neurosciences). For the first group, we concretely explain\nhow current scientific understanding of the brain would enable new application\nfor 6G within the context of a new type of service that we dub braintype\ncommunications and that has more stringent requirements than human- and\nmachine-type communication. In this regard, we expose the key requirements of\nbrain-type communication services and we discuss how future wireless networks\ncan be equipped to deal with such services. Meanwhile, for the second group, we\nthoroughly explore modern communication system paradigms, including Internet of\nBio-nano Things and chaosbased communications, in addition to highlighting how\ncomplex systems tools can help bridging 6G and neuroscience applications.\nBrain-controlled vehicles are then presented as our case study. All in all,\nthis tutorial is expected to provide a largely missing articulation between\nthese two emerging fields while delineating concrete ways to move forward in\nsuch an interdisciplinary endeavor.",
    "authors": [
      "Renan C. Moioli",
      "Pedro H. J. Nardelli",
      "Michael Taynnan Barros",
      "Walid Saad",
      "Amin Hekmatmanesh",
      "Pedro Gória",
      "Arthur S. de Sena",
      "Merim Dzaferagic",
      "Harun Siljak",
      "Werner van Leekwijck",
      "Dick Carrillo",
      "Steven Latré"
    ],
    "publication_date": "2020-04-04T01:58:54Z",
    "arxiv_id": "http://arxiv.org/abs/2004.01834v1",
    "download_url": "http://arxiv.org/abs/2004.01834v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Category Theory and Higher Dimensional Algebra: potential descriptive\n  tools in neuroscience",
    "abstract": "We explain the notion of colimit in category theory as a potential tool for\ndescribing structures and their communication, and the notion of higher\ndimensional algebra as a potential yoga for dealing with processes and\nprocesses of processes.",
    "authors": [
      "R. Brown",
      "T. Porter"
    ],
    "publication_date": "2003-06-13T15:26:09Z",
    "arxiv_id": "http://arxiv.org/abs/math/0306223v2",
    "download_url": "http://arxiv.org/abs/math/0306223v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Promise and Pitfalls of Extending Google's PageRank Algorithm to\n  Citation Networks",
    "abstract": "We review our recent work on applying the Google PageRank algorithm to find\nscientific \"gems\" among all Physical Review publications, and its extension to\nCiteRank, to find currently popular research directions. These metrics provide\na meaningful extension to traditionally-used importance measures, such as the\nnumber of citations and journal impact factor. We also point out some pitfalls\nof over-relying on quantitative metrics to evaluate scientific quality.",
    "authors": [
      "Sergei Maslov",
      "S. Redner"
    ],
    "publication_date": "2009-01-17T14:35:53Z",
    "arxiv_id": "http://arxiv.org/abs/0901.2640v1",
    "download_url": "http://arxiv.org/abs/0901.2640v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A first passage problem for a bivariate diffusion process: numerical\n  solution with an application to neuroscience",
    "abstract": "We consider a bivariate diffusion process and we study the first passage time\nof one component through a boundary. We prove that its probability density is\nthe unique solution of a new integral equation and we propose a numerical\nalgorithm for its solution. Convergence properties of this algorithm are\ndiscussed and the method is applied to the study of the integrated Brownian\nMotion and to the integrated Ornstein Uhlenbeck process. Finally a model of\nneuroscience interest is also discussed.",
    "authors": [
      "Elisa Benedetto",
      "Laura Sacerdote",
      "Cristina Zucca"
    ],
    "publication_date": "2012-04-24T08:50:14Z",
    "arxiv_id": "http://arxiv.org/abs/1204.5307v2",
    "download_url": "http://arxiv.org/abs/1204.5307v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Inadequate experimental methods and erroneous epilepsy diagnostic\n  criteria result in confounding acquired focal epilepsy with genetic absence\n  epilepsy",
    "abstract": "Here we provide a thorough discussion of the study conducted by Rodgers et\nal. (J Neurosci. 2015; 35(24):9194-204. doi: 10.1523/JNEUROSCI.0919-15.2015) to\ninvestigate focal seizures and acquired epileptogenesis induced by head injury\nin the rat. This manuscript serves as supplementary document for our letter to\nthe Editor to appear in the Journal of Neuroscience. We find that the subject\narticle suffers from poor experimental design, very selective consideration of\nantecedent literature, and application of inappropriate epilepsy diagnostic\ncriteria which, together, lead to unwarranted conclusions.",
    "authors": [
      "Raimondo D'Ambrosio",
      "Clifford L. Eastman",
      "John W. Miller"
    ],
    "publication_date": "2015-09-03T18:48:46Z",
    "arxiv_id": "http://arxiv.org/abs/1509.01206v1",
    "download_url": "http://arxiv.org/abs/1509.01206v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Beware of the Small-World neuroscientist!",
    "abstract": "The SW has undeniably been one of the most popular network descriptors in the\nneuroscience literature. Two main reasons for its lasting popularity are its\napparent ease of computation and the intuitions it is thought to provide on how\nnetworked systems operate. Over the last few years, some pitfalls of the SW\nconstruct and, more generally, of network summary measures, have widely been\nacknowledged.",
    "authors": [
      "David Papo",
      "Massimiliano Zanin",
      "Johann H. Martínez",
      "Javier M. Buldú"
    ],
    "publication_date": "2016-03-01T09:44:36Z",
    "arxiv_id": "http://arxiv.org/abs/1603.00200v1",
    "download_url": "http://arxiv.org/abs/1603.00200v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Building machines that adapt and compute like brains",
    "abstract": "Building machines that learn and think like humans is essential not only for\ncognitive science, but also for computational neuroscience, whose ultimate goal\nis to understand how cognition is implemented in biological brains. A new\ncognitive computational neuroscience should build cognitive-level and neural-\nlevel models, understand their relationships, and test both types of models\nwith both brain and behavioral data.",
    "authors": [
      "Nikolaus Kriegeskorte",
      "Robert M. Mok"
    ],
    "publication_date": "2017-11-11T22:02:52Z",
    "arxiv_id": "http://arxiv.org/abs/1711.04203v1",
    "download_url": "http://arxiv.org/abs/1711.04203v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Before and beyond the Wilson-Cowan equations",
    "abstract": "The Wilson-Cowan equations represent a landmark in the history of\ncomputational neuroscience. Among the insights Wilson and Cowan offered for\nneuroscience, they crystallized an approach to modeling neural dynamics and\nbrain function. Although their iconic equations are used in various guises\ntoday, the ideas that led to their formulation and the relationship to other\napproaches are not well known. Here, we give a little context to some of the\nbiological and theoretical concepts that lead to the Wilson-Cowan equations and\ndiscuss how to extend beyond them.",
    "authors": [
      "Carson C. Chow",
      "Yahya Karimipanah"
    ],
    "publication_date": "2019-07-18T00:26:04Z",
    "arxiv_id": "http://arxiv.org/abs/1907.07821v2",
    "download_url": "http://arxiv.org/abs/1907.07821v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Deep learning tools for the measurement of animal behavior in\n  neuroscience",
    "abstract": "Recent advances in computer vision have made accurate, fast and robust\nmeasurement of animal behavior a reality. In the past years powerful tools\nspecifically designed to aid the measurement of behavior have come to fruition.\nHere we discuss how capturing the postures of animals - pose estimation - has\nbeen rapidly advancing with new deep learning methods. While challenges still\nremain, we envision that the fast-paced development of new deep learning tools\nwill rapidly change the landscape of realizable real-world neuroscience.",
    "authors": [
      "Mackenzie W. Mathis",
      "Alexander Mathis"
    ],
    "publication_date": "2019-09-30T17:50:48Z",
    "arxiv_id": "http://arxiv.org/abs/1909.13868v2",
    "download_url": "http://arxiv.org/abs/1909.13868v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Network Dynamics Governed by Lyapunov Functions: From Memory to\n  Classification",
    "abstract": "In 1982 John Hopfield published a neural network model for memory retrieval,\na model that became a cornerstone in theoretical neuroscience. A key ingredient\nof the Hopfield model was the use of a network dynamics that is governed by a\nLyapunov function. In a recent paper, Krotov and Hopfield showed how a Lyapunov\nfunction governs a biological plausible learning rule for the neural networks'\nconnectivity. By doing so, they bring an intriguing approach to classification\ntasks, and show the relevance of the broader framework across decades in the\nfield.",
    "authors": [
      "Merav Stern",
      "Eric Shea-Brown"
    ],
    "publication_date": "2020-04-17T07:26:06Z",
    "arxiv_id": "http://arxiv.org/abs/2004.08091v2",
    "download_url": "http://arxiv.org/abs/2004.08091v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Predictive Coding, Variational Autoencoders, and Biological Connections",
    "abstract": "This paper reviews predictive coding, from theoretical neuroscience, and\nvariational autoencoders, from machine learning, identifying the common origin\nand mathematical framework underlying both areas. As each area is prominent\nwithin its respective field, more firmly connecting these areas could prove\nuseful in the dialogue between neuroscience and machine learning. After\nreviewing each area, we discuss two possible correspondences implied by this\nperspective: cortical pyramidal dendrites as analogous to (non-linear) deep\nnetworks and lateral inhibition as analogous to normalizing flows. These\nconnections may provide new directions for further investigations in each\nfield.",
    "authors": [
      "Joseph Marino"
    ],
    "publication_date": "2020-11-15T07:15:18Z",
    "arxiv_id": "http://arxiv.org/abs/2011.07464v2",
    "download_url": "http://arxiv.org/abs/2011.07464v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Meta-learning in natural and artificial intelligence",
    "abstract": "Meta-learning, or learning to learn, has gained renewed interest in recent\nyears within the artificial intelligence community. However, meta-learning is\nincredibly prevalent within nature, has deep roots in cognitive science and\npsychology, and is currently studied in various forms within neuroscience. The\naim of this review is to recast previous lines of research in the study of\nbiological intelligence within the lens of meta-learning, placing these works\ninto a common framework. More recent points of interaction between AI and\nneuroscience will be discussed, as well as interesting new directions that\narise under this perspective.",
    "authors": [
      "Jane X. Wang"
    ],
    "publication_date": "2020-11-26T20:21:39Z",
    "arxiv_id": "http://arxiv.org/abs/2011.13464v1",
    "download_url": "http://arxiv.org/abs/2011.13464v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Synaptic metaplasticity in binarized neural networks",
    "abstract": "Unlike the brain, artificial neural networks, including state-of-the-art deep\nneural networks for computer vision, are subject to \"catastrophic forgetting\":\nthey rapidly forget the previous task when trained on a new one. Neuroscience\nsuggests that biological synapses avoid this issue through the process of\nsynaptic consolidation and metaplasticity: the plasticity itself changes upon\nrepeated synaptic events. In this work, we show that this concept of\nmetaplasticity can be transferred to a particular type of deep neural networks,\nbinarized neural networks, to reduce catastrophic forgetting.",
    "authors": [
      "Axel Laborieux",
      "Maxence Ernoult",
      "Tifenn Hirtzlin",
      "Damien Querlioz"
    ],
    "publication_date": "2021-01-19T12:32:07Z",
    "arxiv_id": "http://arxiv.org/abs/2101.07592v1",
    "download_url": "http://arxiv.org/abs/2101.07592v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Overfitting the literature to one set of stimuli and data",
    "abstract": "The fast-growing field of Computational Cognitive Neuroscience is on track to\nmeet its first crisis. A large number of papers in this nascent field are\ndeveloping and testing novel analysis methods using the same stimuli and\nneuroimaging datasets. Publication bias and confirmatory exploration will\nresult in overfitting to the limited available data. The field urgently needs\nto collect more good quality open neuroimaging data using a variety of\nexperimental stimuli, to test the generalisability of current published\nresults, and allow for more robust results in future work.",
    "authors": [
      "Tijl Grootswagers",
      "Amanda K Robinson"
    ],
    "publication_date": "2021-02-19T04:06:39Z",
    "arxiv_id": "http://arxiv.org/abs/2102.09729v2",
    "download_url": "http://arxiv.org/abs/2102.09729v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "ExBrainable: An Open-Source GUI for CNN-based EEG Decoding and Model\n  Interpretation",
    "abstract": "We have developed a graphic user interface (GUI), ExBrainable, dedicated to\nconvolutional neural networks (CNN) model training and visualization in\nelectroencephalography (EEG) decoding. Available functions include model\ntraining, evaluation, and parameter visualization in terms of temporal and\nspatial representations. We demonstrate these functions using a well-studied\npublic dataset of motor-imagery EEG and compare the results with existing\nknowledge of neuroscience. The primary objective of ExBrainable is to provide a\nfast, simplified, and user-friendly solution of EEG decoding for investigators\nacross disciplines to leverage cutting-edge methods in brain/neuroscience\nresearch.",
    "authors": [
      "Ya-Lin Huang",
      "Chia-Ying Hsieh",
      "Jian-Xue Huang",
      "Chun-Shu Wei"
    ],
    "publication_date": "2022-01-10T15:21:21Z",
    "arxiv_id": "http://arxiv.org/abs/2201.04065v1",
    "download_url": "http://arxiv.org/abs/2201.04065v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Context sequence theory: a common explanation for multiple types of\n  learning",
    "abstract": "Although principles of neuroscience like reinforcement learning, visual\nperception and attention have been applied in machine learning models, there is\na huge gap between machine learning and mammalian learning. Based on the\nadvances in neuroscience, we propose the context sequence theory to give a\ncommon explanation for multiple types of learning in mammals and hope that can\nprovide a new insight into the construct of machine learning models.",
    "authors": [
      "Yu Mingcan",
      "Wang Junying"
    ],
    "publication_date": "2022-07-17T12:51:52Z",
    "arxiv_id": "http://arxiv.org/abs/2208.04707v1",
    "download_url": "http://arxiv.org/abs/2208.04707v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Coupling Functions in Neuroscience",
    "abstract": "The interactions play one of the central roles in the brain mediating various\nprocesses and functions. They are particularly important for the brain as a\ncomplex system that has many different functions from the same structural\nconnectivity. When studying such neural interactions the coupling functions are\nvery suitable, as inherently they can reveal the underlaying functional\nmechanism. This chapter overviews some recent and widely used aspects of\ncoupling functions for studying neural interactions. Coupling functions are\ndiscussed in connection to two different levels of brain interactions - that of\nneuron interactions and brainwave cross-frequency interactions. Aspects\nrelevant to this from both, theory and methods, are presented. Although the\ndiscussion is based on neuroscience, there are strong implications from, and\nto, other fields as well.",
    "authors": [
      "Tomislav Stankovski"
    ],
    "publication_date": "2020-08-17T20:39:47Z",
    "arxiv_id": "http://arxiv.org/abs/2008.07612v1",
    "download_url": "http://arxiv.org/abs/2008.07612v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "naplib-python: Neural Acoustic Data Processing and Analysis Tools in\n  Python",
    "abstract": "Recently, the computational neuroscience community has pushed for more\ntransparent and reproducible methods across the field. In the interest of\nunifying the domain of auditory neuroscience, naplib-python provides an\nintuitive and general data structure for handling all neural recordings and\nstimuli, as well as extensive preprocessing, feature extraction, and analysis\ntools which operate on that data structure. The package removes many of the\ncomplications associated with this domain, such as varying trial durations and\nmulti-modal stimuli, and provides a general-purpose analysis framework that\ninterfaces easily with existing toolboxes used in the field.",
    "authors": [
      "Gavin Mischler",
      "Vinay Raghavan",
      "Menoua Keshishian",
      "Nima Mesgarani"
    ],
    "publication_date": "2023-04-04T13:56:32Z",
    "arxiv_id": "http://arxiv.org/abs/2304.01799v1",
    "download_url": "http://arxiv.org/abs/2304.01799v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Bifurcation and periodic solutions to neuroscience models with a small\n  parameter",
    "abstract": "The existence of periodic solutions is proven for some neuroscience models\nwith a small parameter. Moreover, the stability of such solutions is\ninvestigated, as well. The results are based on a theoretical research dealing\nwith the functional differential equation with parameters $$ \\dot{x}(t)=L(\\tau)\nx_t + \\varepsilon f(t, x_t), $$ where $L: \\mathbb{R}_+\\rightarrow\n\\mathcal{L}(C; \\mathbb{R})$ and $f: \\mathbb{R} \\times C \\rightarrow \\mathbb{R}$\nare, respectively, linear and nonlinear operators, and $\\varepsilon>0$ is a\nsmall enough parameter. The theoretical results are applied to a Parkinson's\ndisease model, where the obtained conclusions are illustrated by numerical\nsimulations.",
    "authors": [
      "José Oyarce"
    ],
    "publication_date": "2023-09-12T17:01:50Z",
    "arxiv_id": "http://arxiv.org/abs/2309.06398v1",
    "download_url": "http://arxiv.org/abs/2309.06398v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A scaling limit for additive functionals",
    "abstract": "Inspired by models for synchronous spiking activity in neuroscience,\n  we consider a scaling-limit framework for sequences of strong Markov\n  processes. Within this framework, we establish the convergence of\n  certain additive functionals toward L\\'evy subordinators, which are\n  of interest in synchronous input drive modeling in neuronal models.\n  After proving an abstract theorem in full generality, we provide\n  detailed and explicit conclusions in the case of reflected\n  one-dimensional diffusions. Specializing even further, we provide an\n  in-depth analysis of the limiting behavior of a sequence of\n  integrated Wright-Fisher diffusions. In neuroscience, such\n  diffusions serve to parametrize synchrony in doubly-stochastic\n  models of spiking activity. Additional explicit examples involving\n  the Feller diffusion and the Brownian motion with drift are also\n  given.",
    "authors": [
      "Thibaud Taillefumier",
      "Gordan Zitkovic"
    ],
    "publication_date": "2024-10-08T21:31:41Z",
    "arxiv_id": "http://arxiv.org/abs/2410.06383v1",
    "download_url": "http://arxiv.org/abs/2410.06383v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Human Creativity and AI",
    "abstract": "With the advancement of science and technology, the philosophy of creativity\nhas undergone significant reinterpretation. This paper investigates\ncontemporary research in the fields of psychology, cognitive neuroscience, and\nthe philosophy of creativity, particularly in the context of the development of\nartificial intelligence (AI) techniques. It aims to address the central\nquestion: Can AI exhibit creativity? The paper reviews the historical\nperspectives on the philosophy of creativity and explores the influence of\npsychological advancements on the study of creativity. Furthermore, it analyzes\nvarious definitions of creativity and examines the responses of naturalism and\ncognitive neuroscience to the concept of creativity.",
    "authors": [
      "Shengyi Xie"
    ],
    "publication_date": "2025-04-25T17:10:03Z",
    "arxiv_id": "http://arxiv.org/abs/2507.08001v1",
    "download_url": "http://arxiv.org/abs/2507.08001v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Dyadic Neural Dynamics: Extending Representation Learning to Social\n  Neuroscience",
    "abstract": "Social communication fundamentally involves at least two interacting brains,\ncreating a unique modeling problem. We present the first application of\nContrastive Embedding for Behavioral and Neural Analysis (CEBRA) to dyadic EEG\nhyperscanning data, extending modeling paradigms to interpersonal neural\ndynamics. Using structured social interactions between participants, we\ndemonstrate that CEBRA can learn meaningful representations of joint neural\nactivity that captures individual roles (speaker-listener) and other behavioral\nmetrics. Our approach to characterizing interactions, as opposed to individual\nneural responses to stimuli, addresses the key principles of foundational model\ndevelopment: scalability and cross-subject generalization, opening new\ndirections for representation learning in social neuroscience and clinical\napplications.",
    "authors": [
      "Maria Glushanina",
      "Jeffrey Huang",
      "Michelle McCleod",
      "Brendan Ames",
      "Evie Malaia"
    ],
    "publication_date": "2025-09-27T20:02:44Z",
    "arxiv_id": "http://arxiv.org/abs/2509.23479v1",
    "download_url": "http://arxiv.org/abs/2509.23479v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Why are probabilistic laws governing quantum mechanics and neurobiology?",
    "abstract": "We address the question: Why are dynamical laws governing in quantum\nmechanics and in neuroscience of probabilistic nature instead of being\ndeterministic? We discuss some ideas showing that the probabilistic option\noffers advantages over the deterministic one.",
    "authors": [
      "H. Kroger"
    ],
    "publication_date": "2004-06-14T20:19:48Z",
    "arxiv_id": "http://arxiv.org/abs/quant-ph/0406098v1",
    "download_url": "http://arxiv.org/abs/quant-ph/0406098v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Do we understand the emergent dynamics of grid cell activity?",
    "abstract": "We examine the qualitative and quantitative properties of continuous\nattractor networks in explaining the dynamics of grid cells.",
    "authors": [
      "Yoram Burak",
      "Ila R. Fiete"
    ],
    "publication_date": "2007-08-04T00:34:47Z",
    "arxiv_id": "http://arxiv.org/abs/0708.0594v1",
    "download_url": "http://arxiv.org/abs/0708.0594v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Notes on Leibniz thought experiment",
    "abstract": "Leibniz thought experiment of perception, sensing, and thinking is\nreconsidered. We try to understand Leibniz picture in view of our knowledge of\nbasic neuroscience. In particular we can see how the emergence of consciousness\ncould in principle be understood.",
    "authors": [
      "Markos Maniatis"
    ],
    "publication_date": "2013-09-03T21:30:05Z",
    "arxiv_id": "http://arxiv.org/abs/1309.0846v1",
    "download_url": "http://arxiv.org/abs/1309.0846v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "The OpenPicoAmp : an open-source planar lipid bilayer amplifier for\n  hands-on learning of neuroscience",
    "abstract": "Neuroscience education can be promoted by the availability of low cost and\nengaging teaching materials. To address this issue, we developed an open-source\nlipid bilayer amplifier, the OpenPicoAmp, which is appropriate for use in\nintroductory courses in biophysics or neurosciences dealing with the electrical\nproperties of the cell membrane. The amplifier is designed using the common\nlithographic printed circuit board fabrication process and off-the-shelf\nelectronic components. In addition, we propose a specific design for\nexperimental chambers allowing the insertion of a commercially available\npolytetrafluoroethylene film. This experimental setup can be used in simple\nexperiments in which students monitor the bilayer formation by capacitance\nmeasurement and record unitary currents produced by ionic channels like\ngramicidin A. Used in combination with a low-cost data acquisition board this\nsystem provides a complete solution for hands-on lessons, therefore improving\nthe effectiveness in teaching basic neurosciences or biophysics.",
    "authors": [
      "Vadim Shlyonsky",
      "Freddy Dupuis",
      "David Gall"
    ],
    "publication_date": "2014-03-28T16:28:54Z",
    "arxiv_id": "http://arxiv.org/abs/1403.7439v4",
    "download_url": "http://arxiv.org/abs/1403.7439v4",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "BluePyOpt: Leveraging open source software and cloud infrastructure to\n  optimise model parameters in neuroscience",
    "abstract": "At many scales in neuroscience, appropriate mathematical models take the form\nof complex dynamical systems. Parametrising such models to conform to the\nmultitude of available experimental constraints is a global nonlinear\noptimisation problem with a complex fitness landscape, requiring numerical\ntechniques to find suitable approximate solutions. Stochastic optimisation\napproaches, such as evolutionary algorithms, have been shown to be effective,\nbut often the setting up of such optimisations and the choice of a specific\nsearch algorithm and its parameters is non-trivial, requiring domain-specific\nexpertise. Here we describe BluePyOpt, a Python package targeted at the broad\nneuroscience community to simplify this task. BluePyOpt is an extensible\nframework for data-driven model parameter optimisation that wraps and\nstandardises several existing open-source tools. It simplifies the task of\ncreating and sharing these optimisations, and the associated techniques and\nknowledge. This is achieved by abstracting the optimisation and evaluation\ntasks into various reusable and flexible discrete elements according to\nestablished best-practices. Further, BluePyOpt provides methods for setting up\nboth small- and large-scale optimisations on a variety of platforms, ranging\nfrom laptops to Linux clusters and cloud-based compute infrastructures. The\nversatility of the BluePyOpt framework is demonstrated by working through three\nrepresentative neuroscience specific use cases.",
    "authors": [
      "Werner Van Geit",
      "Michael Gevaert",
      "Giuseppe Chindemi",
      "Christian Rössert",
      "Jean-Denis Courcol",
      "Eilif Muller",
      "Felix Schürmann",
      "Idan Segev",
      "Henry Markram"
    ],
    "publication_date": "2016-03-01T21:43:14Z",
    "arxiv_id": "http://arxiv.org/abs/1603.00500v1",
    "download_url": "http://arxiv.org/abs/1603.00500v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Brain Network Architecture: Implications for Human Learning",
    "abstract": "Human learning is a complex phenomenon that requires adaptive processes\nacross a range of temporal and spacial scales. While our understanding of those\nprocesses at single scales has increased exponentially over the last few years,\na mechanistic understanding of the entire phenomenon has remained elusive. We\npropose that progress has been stymied by the lack of a quantitative framework\nthat can account for the full range of neurophysiological and behavioral\ndynamics both across scales in the systems and also across different types of\nlearning. We posit that network neuroscience offers promise in meeting this\nchallenge. Built on the mathematical fields of complex systems science and\ngraph theory, network neuroscience embraces the interconnected and hierarchical\nnature of human learning, offering insights into the emergent properties of\nadaptability. In this review, we discuss the utility of network neuroscience as\na tool to build a quantitative framework in which to study human learning,\nwhich seeks to explain the full chain of events in the brain from sensory input\nto motor output, being both biologically plausible and able to make predictions\nabout how an intervention at a single level of the chain may cause alterations\nin another level of the chain. We close by laying out important remaining\nchallenges in network neuroscience in explicitly bridging spatial scales at\nwhich neurophysiological processes occur, and underscore the utility of such a\nquantitative framework for education and therapy.",
    "authors": [
      "Marcelo G. Mattar",
      "Danielle S. Bassett"
    ],
    "publication_date": "2016-09-07T00:22:40Z",
    "arxiv_id": "http://arxiv.org/abs/1609.01790v1",
    "download_url": "http://arxiv.org/abs/1609.01790v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Generative Models for Network Neuroscience: Prospects and Promise",
    "abstract": "Network neuroscience is the emerging discipline concerned with investigating\nthe complex patterns of interconnections found in neural systems, and to\nidentify principles with which to understand them. Within this discipline, one\nparticularly powerful approach is network generative modeling, in which wiring\nrules are algorithmically implemented to produce synthetic network\narchitectures with the same properties as observed in empirical network data.\nSuccessful models can highlight the principles by which a network is organized\nand potentially uncover the mechanisms by which it grows and develops. Here we\nreview the prospects and promise of generative models for network neuroscience.\nWe begin with a primer on network generative models, with a discussion of\ncompressibility and predictability, utility in intuiting mechanisms, and a\nshort history on their use in network science broadly. We then discuss\ngenerative models in practice and application, paying particular attention to\nthe critical need for cross-validation. Next, we review generative models of\nbiological neural networks, both at the cellular and large-scale level, and\nacross a variety of species including \\emph{C. elegans}, \\emph{Drosophila},\nmouse, rat, cat, macaque, and human. We offer a careful treatment of a few\nrelevant distinctions, including differences between generative models and null\nmodels, sufficiency and redundancy, inferring and claiming mechanism, and\nfunctional and structural connectivity. We close with a discussion of future\ndirections, outlining exciting frontiers both in empirical data collection\nefforts as well as in method and theory development that, together, further the\nutility of the generative network modeling approach for network neuroscience.",
    "authors": [
      "Richard F. Betzel",
      "Danielle S. Bassett"
    ],
    "publication_date": "2017-08-26T11:30:35Z",
    "arxiv_id": "http://arxiv.org/abs/1708.07958v1",
    "download_url": "http://arxiv.org/abs/1708.07958v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Solved problems and remaining challenges for Granger causality analysis\n  in neuroscience: A response to Stokes and Purdon (2017)",
    "abstract": "Granger-Geweke causality (GGC) is a powerful and popular method for\nidentifying directed functional (`causal') connectivity in neuroscience. In a\nrecent paper, Stokes and Purdon [1] raise several concerns about its use. They\nmake two primary claims: (1) that GGC estimates may be severely biased or of\nhigh variance, and (2) that GGC fails to reveal the full structural/causal\nmechanisms of a system. However, these claims rest, respectively, on an\nincomplete evaluation of the literature, and a misconception about what GGC can\nbe said to measure. Here we explain how existing approaches (as implemented,\nfor example, in our popular MVGC software [2,3]) resolve the first issue, and\ndiscuss the frequently-misunderstood distinction between functional and\neffective neural connectivity which underlies Stokes and Purdon's second claim.\n  [1] Patrick A. Stokes and Patrick. L. Purdon (2017), A study of problems\nencountered in Granger causality analysis from a neuroscience perspective,\nProc. Natl. Acad. Sci. USA 114(34):7063-7072.\n  [2] Lionel Barnett and Anil K. Seth (2012), The MVGC Multivariate Granger\nCausality Matlab toolbox, http://users.sussex.ac.uk/~lionelb/MVGC/\n  [3] Lionel Barnett and Anil K. Seth (2014), The MVGC multivariate Granger\ncausality toolbox: A new approach to Granger-causal inference, J. Neurosci.\nMethods 223:50-68",
    "authors": [
      "Lionel Barnett",
      "Adam B. Barrett",
      "Anil K. Seth"
    ],
    "publication_date": "2017-08-26T17:49:31Z",
    "arxiv_id": "http://arxiv.org/abs/1708.08001v2",
    "download_url": "http://arxiv.org/abs/1708.08001v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Network models in neuroscience",
    "abstract": "From interacting cellular components to networks of neurons and neural\nsystems, interconnected units comprise a fundamental organizing principle of\nthe nervous system. Understanding how their patterns of connections and\ninteractions give rise to the many functions of the nervous system is a primary\ngoal of neuroscience. Recently, this pursuit has begun to benefit from the\ndevelopment of new mathematical tools that can relate a system's architecture\nto its dynamics and function. These tools, which are known collectively as\nnetwork science, have been used with increasing success to build models of\nneural systems across spatial scales and species. Here we discuss the nature of\nnetwork models in neuroscience. We begin with a review of model theory from a\nphilosophical perspective to inform our view of networks as models of complex\nsystems in general, and of the brain in particular. We then summarize the types\nof models that are frequently studied in network neuroscience along three\nprimary dimensions: from data representations to first-principles theory, from\nbiophysical realism to functional phenomenology, and from elementary\ndescriptions to coarse-grained approximations. We then consider ways to\nvalidate these models, focusing on approaches that perturb a system to probe\nits function. We close with a description of important frontiers in the\nconstruction of network models and their relevance for understanding\nincreasingly complex functions of neural systems.",
    "authors": [
      "Danielle S. Bassett",
      "Perry Zurn",
      "Joshua I. Gold"
    ],
    "publication_date": "2018-07-31T17:51:16Z",
    "arxiv_id": "http://arxiv.org/abs/1807.11935v1",
    "download_url": "http://arxiv.org/abs/1807.11935v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "The extent and drivers of gender imbalance in neuroscience reference\n  lists",
    "abstract": "Like many scientific disciplines, neuroscience has increasingly attempted to\nconfront pervasive gender imbalances within the field. While much of the\nconversation has centered around publishing and conference participation,\nrecent research in other fields has called attention to the prevalence of\ngender bias in citation practices. Because of the downstream effects that\ncitations can have on visibility and career advancement, understanding and\neliminating gender bias in citation practices is vital for addressing inequity\nin a scientific community. In this study, we sought to determine whether there\nis evidence of gender bias in the citation practices of neuroscientists. Using\ndata from five top neuroscience journals, we find that reference lists tend to\ninclude more papers with men as first and last author than would be expected if\ngender were not a factor in referencing. Importantly, we show that this\novercitation of men and undercitation of women is driven largely by the\ncitation practices of men, and is increasing over time as the field becomes\nmore diverse. We develop a co-authorship network to assess homophily in\nresearchers' social networks, and we find that men tend to overcite men even\nwhen their social networks are representative. We discuss possible mechanisms\nand consider how individual researchers might address these findings in their\nown practices.",
    "authors": [
      "Jordan D. Dworkin",
      "Kristin A. Linn",
      "Erin G. Teich",
      "Perry Zurn",
      "Russell T. Shinohara",
      "Danielle S. Bassett"
    ],
    "publication_date": "2020-01-03T22:15:46Z",
    "arxiv_id": "http://arxiv.org/abs/2001.01002v2",
    "download_url": "http://arxiv.org/abs/2001.01002v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Bridging the neuroscience and physics of time",
    "abstract": "As a neuroscientist and a theoretical physicist, both working on time, we\nhave decided to open a direct dialogue to examine if the apparent discrepancies\nregarding the nature of time can be composed.",
    "authors": [
      "Dean Buonomano",
      "Carlo Rovelli"
    ],
    "publication_date": "2021-09-05T05:49:51Z",
    "arxiv_id": "http://arxiv.org/abs/2110.01976v1",
    "download_url": "http://arxiv.org/abs/2110.01976v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Brain-Inspired Continual Learning-Robust Feature Distillation and\n  Re-Consolidation for Class Incremental Learning",
    "abstract": "Artificial intelligence (AI) and neuroscience share a rich history, with\nadvancements in neuroscience shaping the development of AI systems capable of\nhuman-like knowledge retention. Leveraging insights from neuroscience and\nexisting research in adversarial and continual learning, we introduce a novel\nframework comprising two core concepts: feature distillation and\nre-consolidation. Our framework, named Robust Rehearsal, addresses the\nchallenge of catastrophic forgetting inherent in continual learning (CL)\nsystems by distilling and rehearsing robust features. Inspired by the mammalian\nbrain's memory consolidation process, Robust Rehearsal aims to emulate the\nrehearsal of distilled experiences during learning tasks. Additionally, it\nmimics memory re-consolidation, where new experiences influence the integration\nof past experiences to mitigate forgetting. Extensive experiments conducted on\nCIFAR10, CIFAR100, and real-world helicopter attitude datasets showcase the\nsuperior performance of CL models trained with Robust Rehearsal compared to\nbaseline methods. Furthermore, examining different optimization training\nobjectives-joint, continual, and adversarial learning-we highlight the crucial\nrole of feature learning in model performance. This underscores the\nsignificance of rehearsing CL-robust samples in mitigating catastrophic\nforgetting. In conclusion, aligning CL approaches with neuroscience insights\noffers promising solutions to the challenge of catastrophic forgetting, paving\nthe way for more robust and human-like AI systems.",
    "authors": [
      "Hikmat Khan",
      "Nidhal Carla Bouaynaya",
      "Ghulam Rasool"
    ],
    "publication_date": "2024-04-22T21:30:11Z",
    "arxiv_id": "http://arxiv.org/abs/2404.14588v1",
    "download_url": "http://arxiv.org/abs/2404.14588v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Report on Candidate Computational Indicators for Conscious Valenced\n  Experience",
    "abstract": "This report enlists 13 functional conditions cashed out in computational\nterms that have been argued to be constituent of conscious valenced experience.\nThese are extracted from existing empirical and theoretical literature on,\namong others, animal sentience, medical disorders, anaesthetics, philosophy,\nevolution, neuroscience, and artificial intelligence.",
    "authors": [
      "Andres Campero"
    ],
    "publication_date": "2024-04-25T15:58:09Z",
    "arxiv_id": "http://arxiv.org/abs/2404.16696v1",
    "download_url": "http://arxiv.org/abs/2404.16696v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Real-Time Machine Learning Strategies for a New Kind of Neuroscience\n  Experiments",
    "abstract": "Function and dysfunctions of neural systems are tied to the temporal\nevolution of neural states. The current limitations in showing their causal\nrole stem largely from the absence of tools capable of probing the brain's\ninternal state in real-time. This gap restricts the scope of experiments vital\nfor advancing both fundamental and clinical neuroscience. Recent advances in\nreal-time machine learning technologies, particularly in analyzing neural time\nseries as nonlinear stochastic dynamical systems, are beginning to bridge this\ngap. These technologies enable immediate interpretation of and interaction with\nneural systems, offering new insights into neural computation. However, several\nsignificant challenges remain. Issues such as slow convergence rates,\nhigh-dimensional data complexities, structured noise, non-identifiability, and\na general lack of inductive biases tailored for neural dynamics are key\nhurdles. Overcoming these challenges is crucial for the full realization of\nreal-time neural data analysis for the causal investigation of neural\ncomputation and advanced perturbation based brain machine interfaces. In this\npaper, we provide a comprehensive perspective on the current state of the\nfield, focusing on these persistent issues and outlining potential paths\nforward. We emphasize the importance of large-scale integrative neuroscience\ninitiatives and the role of meta-learning in overcoming these challenges. These\napproaches represent promising research directions that could redefine the\nlandscape of neuroscience experiments and brain-machine interfaces,\nfacilitating breakthroughs in understanding brain function, and treatment of\nneurological disorders.",
    "authors": [
      "Ayesha Vermani",
      "Matthew Dowling",
      "Hyungju Jeon",
      "Ian Jordan",
      "Josue Nassar",
      "Yves Bernaerts",
      "Yuan Zhao",
      "Steven Van Vaerenbergh",
      "Il Memming Park"
    ],
    "publication_date": "2024-09-02T14:24:04Z",
    "arxiv_id": "http://arxiv.org/abs/2409.01280v2",
    "download_url": "http://arxiv.org/abs/2409.01280v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "NeuroAI for AI Safety",
    "abstract": "As AI systems become increasingly powerful, the need for safe AI has become\nmore pressing. Humans are an attractive model for AI safety: as the only known\nagents capable of general intelligence, they perform robustly even under\nconditions that deviate significantly from prior experiences, explore the world\nsafely, understand pragmatics, and can cooperate to meet their intrinsic goals.\nIntelligence, when coupled with cooperation and safety mechanisms, can drive\nsustained progress and well-being. These properties are a function of the\narchitecture of the brain and the learning algorithms it implements.\nNeuroscience may thus hold important keys to technical AI safety that are\ncurrently underexplored and underutilized. In this roadmap, we highlight and\ncritically evaluate several paths toward AI safety inspired by neuroscience:\nemulating the brain's representations, information processing, and\narchitecture; building robust sensory and motor systems from imitating brain\ndata and bodies; fine-tuning AI systems on brain data; advancing\ninterpretability using neuroscience methods; and scaling up\ncognitively-inspired architectures. We make several concrete recommendations\nfor how neuroscience can positively impact AI safety.",
    "authors": [
      "Patrick Mineault",
      "Niccolò Zanichelli",
      "Joanne Zichen Peng",
      "Anton Arkhipov",
      "Eli Bingham",
      "Julian Jara-Ettinger",
      "Emily Mackevicius",
      "Adam Marblestone",
      "Marcelo Mattar",
      "Andrew Payne",
      "Sophia Sanborn",
      "Karen Schroeder",
      "Zenna Tavares",
      "Andreas Tolias",
      "Anthony Zador"
    ],
    "publication_date": "2024-11-27T17:18:51Z",
    "arxiv_id": "http://arxiv.org/abs/2411.18526v2",
    "download_url": "http://arxiv.org/abs/2411.18526v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Towards Chip-on-Chip Neuroscience: Fast Mining of Frequent Episodes\n  Using Graphics Processors",
    "abstract": "Computational neuroscience is being revolutionized with the advent of\nmulti-electrode arrays that provide real-time, dynamic, perspectives into brain\nfunction. Mining event streams from these chips is critical to understanding\nthe firing patterns of neurons and to gaining insight into the underlying\ncellular activity. We present a GPGPU solution to mining spike trains. We focus\non mining frequent episodes which captures coordinated events across time even\nin the presence of intervening background/\"junk\" events. Our algorithmic\ncontributions are two-fold: MapConcatenate, a new computation-to-core mapping\nscheme, and a two-pass elimination approach to quickly find supported episodes\nfrom a large number of candidates. Together, they help realize a real-time\n\"chip-on-chip\" solution to neuroscience data mining, where one chip (the\nmulti-electrode array) supplies the spike train data and another (the GPGPU)\nmines it at a scale unachievable previously. Evaluation on both synthetic and\nreal datasets demonstrate the potential of our approach.",
    "authors": [
      "Yong Cao",
      "Debprakash Patnaik",
      "Sean Ponce",
      "Jeremy Archuleta",
      "Patrick Butler",
      "Wu-chun Feng",
      "Naren Ramakrishnan"
    ],
    "publication_date": "2009-05-13T21:04:03Z",
    "arxiv_id": "http://arxiv.org/abs/0905.2200v1",
    "download_url": "http://arxiv.org/abs/0905.2200v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "An Integrated e-science Analysis Base for Computation Neuroscience\n  Experiments and Analysis",
    "abstract": "Recent developments in data management and imaging technologies have\nsignificantly affected diagnostic and extrapolative research in the\nunderstanding of neurodegenerative diseases. However, the impact of these new\ntechnologies is largely dependent on the speed and reliability with which the\nmedical data can be visualised, analysed and interpreted. The EUs neuGRID for\nUsers (N4U) is a follow-on project to neuGRID, which aims to provide an\nintegrated environment to carry out computational neuroscience experiments.\nThis paper reports on the design and development of the N4U Analysis Base and\nrelated Information Services, which addresses existing research and practical\nchallenges by offering an integrated medical data analysis environment with the\nnecessary building blocks for neuroscientists to optimally exploit neuroscience\nworkflows, large image datasets and algorithms in order to conduct analyses.\nThe N4U Analysis Base enables such analyses by indexing and interlinking the\nneuroimaging and clinical study datasets stored on the N4U Grid infrastructure,\nalgorithms and scientific workflow definitions along with their associated\nprovenance information.",
    "authors": [
      "Kamran Munir",
      "Saad Liaquat Kiani",
      "Khawar Hasham",
      "Richard McClatchey",
      "Andrew Branson",
      "Jetendr Shamdasani",
      "the N4U Consortium"
    ],
    "publication_date": "2014-02-24T09:14:44Z",
    "arxiv_id": "http://arxiv.org/abs/1402.5757v1",
    "download_url": "http://arxiv.org/abs/1402.5757v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Applications of Information Theory to Analysis of Neural Data",
    "abstract": "Information theory is a practical and theoretical framework developed for the\nstudy of communication over noisy channels. Its probabilistic basis and\ncapacity to relate statistical structure to function make it ideally suited for\nstudying information flow in the nervous system. It has a number of useful\nproperties: it is a general measure sensitive to any relationship, not only\nlinear effects; it has meaningful units which in many cases allow direct\ncomparison between different experiments; and it can be used to study how much\ninformation can be gained by observing neural responses in single trials,\nrather than in averages over multiple trials. A variety of information\ntheoretic quantities are commonly used in neuroscience - (see entry\n\"Definitions of Information-Theoretic Quantities\"). In this entry we review\nsome applications of information theory in neuroscience to study encoding of\ninformation in both single neurons and neuronal populations.",
    "authors": [
      "Simon R. Schultz",
      "Robin A. A. Ince",
      "Stefano Panzeri"
    ],
    "publication_date": "2015-01-08T14:16:02Z",
    "arxiv_id": "http://arxiv.org/abs/1501.01860v1",
    "download_url": "http://arxiv.org/abs/1501.01860v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  }
]