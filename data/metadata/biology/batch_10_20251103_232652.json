[
  {
    "title": "Definition of Cybernetical Neuroscience",
    "abstract": "A new scientific field is introduced and discussed, named cybernetical\nneuroscience, which studies mathematical models adopted in computational\nneuroscience by methods of cybernetics -- the science of control and\ncommunication in a living organism, machine and society. It also considers the\npractical application of the results obtained when studying mathematical\nmodels. The main tasks and methods, as well as some results of cybernetic\nneuroscience are considered.",
    "authors": [
      "Alexander Fradkov"
    ],
    "publication_date": "2024-09-14T13:35:59Z",
    "arxiv_id": "http://arxiv.org/abs/2409.16314v1",
    "download_url": "http://arxiv.org/abs/2409.16314v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Teaching Computational Neuroscience",
    "abstract": "The problems and beauty of teaching computational neuroscience are discussed\nby reviewing three new textbooks.",
    "authors": [
      "Péter Érdi"
    ],
    "publication_date": "2014-12-18T15:52:08Z",
    "arxiv_id": "http://arxiv.org/abs/1412.5909v2",
    "download_url": "http://arxiv.org/abs/1412.5909v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Paradigm Shift in Neuroscience Driven by Big Data: State of art,\n  Challenges, and Proof of Concept",
    "abstract": "A recent editorial in Nature noted that cognitive neuroscience is at a\ncrossroads where it is a thorny issue to reliably reveal brain-behavior\nassociations. This commentary sketches a big data science way out for cognitive\nneuroscience, namely population neuroscience. In terms of design, analysis, and\ninterpretations, population neuroscience research takes the design control to\nan unprecedented level, greatly expands the dimensions of the data analysis\nspace, and paves a paradigm shift for exploring mechanisms on brain-behavior\nassociations.",
    "authors": [
      "Zi-Xuan Zhou",
      "Xi-Nian Zuo"
    ],
    "publication_date": "2022-12-08T11:23:07Z",
    "arxiv_id": "http://arxiv.org/abs/2212.04195v2",
    "download_url": "http://arxiv.org/abs/2212.04195v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and\n  Semantic Understanding Capability of LLM",
    "abstract": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources.\nHowever, existing methods for constructing KGs in neuroscience often rely on\nlabeled data and require domain expertise. Acquiring large-scale, labeled data\nfor a specialized area like neuroscience presents significant challenges. This\nwork proposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches. The results demonstrate that our methods significantly\nenhance knowledge discovery from the unlabeled neuroscience research corpus.\nThe performance of the proposed entity and relation extraction method is\ncomparable to the existing supervised method. It achieves an F1 score of 0.84\nfor entity extraction from the unlabeled data. The knowledge obtained from the\nKG improves answers to over 52% of neuroscience questions from the PubMedQA\ndataset and questions generated using selected neuroscience entities.",
    "authors": [
      "Pralaypati Ta",
      "Sriram Venkatesaperumal",
      "Keerthi Ram",
      "Mohanasankar Sivaprakasam"
    ],
    "publication_date": "2025-06-03T17:59:18Z",
    "arxiv_id": "http://arxiv.org/abs/2506.03145v2",
    "download_url": "http://arxiv.org/abs/2506.03145v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "What can topology tell us about the neural code?",
    "abstract": "Neuroscience is undergoing a period of rapid experimental progress and\nexpansion. New mathematical tools, previously unknown in the neuroscience\ncommunity, are now being used to tackle fundamental questions and analyze\nemerging data sets. Consistent with this trend, the last decade has seen an\nuptick in the use of topological ideas and methods in neuroscience. In this\ntalk I will survey recent applications of topology in neuroscience, and explain\nwhy topology is an especially natural tool for understanding neural codes.\nNote: This is a write-up of my talk for the Current Events Bulletin, held at\nthe 2016 Joint Math Meetings in Seattle, WA.",
    "authors": [
      "Carina Curto"
    ],
    "publication_date": "2016-05-06T12:06:15Z",
    "arxiv_id": "http://arxiv.org/abs/1605.01905v1",
    "download_url": "http://arxiv.org/abs/1605.01905v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Is neuroscience facing up to statistical power?",
    "abstract": "It has been demonstrated that the statistical power of many neuroscience\nstudies is very low, so that the results are unlikely to be robustly\nreproducible. How are neuroscientists and the journals in which they publish\nresponding to this problem? Here I review the sample size justifications\nprovided for all 15 papers published in one recent issue of the leading journal\nNature Neuroscience. Of these, only one claimed it was adequately powered. The\nothers mostly appealed to the sample sizes used in earlier studies, despite a\nlack of evidence that these earlier studies were adequately powered. Thus,\nconcerns regarding statistical power in neuroscience have mostly not yet been\naddressed.",
    "authors": [
      "Geoffrey J Goodhill"
    ],
    "publication_date": "2017-01-05T06:07:48Z",
    "arxiv_id": "http://arxiv.org/abs/1701.01219v1",
    "download_url": "http://arxiv.org/abs/1701.01219v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "The Roles of Supervised Machine Learning in Systems Neuroscience",
    "abstract": "Over the last several years, the use of machine learning (ML) in neuroscience\nhas been rapidly increasing. Here, we review ML's contributions, both realized\nand potential, across several areas of systems neuroscience. We describe four\nprimary roles of ML within neuroscience: 1) creating solutions to engineering\nproblems, 2) identifying predictive variables, 3) setting benchmarks for simple\nmodels of the brain, and 4) serving itself as a model for the brain. The\nbreadth and ease of its applicability suggests that machine learning should be\nin the toolbox of most systems neuroscientists.",
    "authors": [
      "Joshua I. Glaser",
      "Ari S. Benjamin",
      "Roozbeh Farhoodi",
      "Konrad P. Kording"
    ],
    "publication_date": "2018-05-21T18:11:26Z",
    "arxiv_id": "http://arxiv.org/abs/1805.08239v2",
    "download_url": "http://arxiv.org/abs/1805.08239v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Connecting levels of analysis in the computational era",
    "abstract": "Neuroscience and artificial intelligence are closely intertwined, but so are\nthe physics of dynamical system, philosophy and psychology. Each of these\nfields try in their own way to relate observations at the level of molecules,\nsynapses, neurons or behavior, to a function. An influential conceptual\napproach to this end was popularized by David Marr, which focused on the\ninteraction between three theoretical 'levels of analysis'. With the\nconvergence of simulation-based approaches, algorithm-oriented Neuro-AI and\nhigh-throughput data, we currently see much research organized around four\nlevels of analysis: observations, models, algorithms and functions.\nBidirectional interaction between these levels influences how we undertake\ninterdisciplinary science.",
    "authors": [
      "Richard Naud",
      "André Longtin"
    ],
    "publication_date": "2023-05-10T10:42:23Z",
    "arxiv_id": "http://arxiv.org/abs/2305.06037v2",
    "download_url": "http://arxiv.org/abs/2305.06037v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Playing With Neuroscience: Past, Present and Future of Neuroimaging and\n  Games",
    "abstract": "Videogames have been a catalyst for advances in many research fields, such as\nartificial intelligence, human-computer interaction or virtual reality. Over\nthe years, research in fields such as artificial intelligence has enabled the\ndesign of new types of games, while games have often served as a powerful tool\nfor testing and simulation. Can this also happen with neuroscience? What is the\ncurrent relationship between neuroscience and games research? what can we\nexpect from the future? In this article, we'll try to answer these questions,\nanalysing the current state-of-the-art at the crossroads between neuroscience\nand games and envisioning future directions.",
    "authors": [
      "Paolo Burelli",
      "Laurits Dixen"
    ],
    "publication_date": "2024-03-06T12:38:18Z",
    "arxiv_id": "http://arxiv.org/abs/2403.15413v1",
    "download_url": "http://arxiv.org/abs/2403.15413v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Cortex simulation system proposal using distributed computer network\n  environments",
    "abstract": "In the dawn of computer science and the eve of neuroscience we participate in\nrebirth of neuroscience due to new technology that allows us to deeply and\nprecisely explore whole new world that dwells in our brains.",
    "authors": [
      "Boris Tomas"
    ],
    "publication_date": "2014-03-22T20:30:55Z",
    "arxiv_id": "http://arxiv.org/abs/1403.5701v1",
    "download_url": "http://arxiv.org/abs/1403.5701v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "An overview of open source Deep Learning-based libraries for\n  Neuroscience",
    "abstract": "In recent years, deep learning revolutionized machine learning and its\napplications, producing results comparable to human experts in several domains,\nincluding neuroscience. Each year, hundreds of scientific publications present\napplications of deep neural networks for biomedical data analysis. Due to the\nfast growth of the domain, it could be a complicated and extremely\ntime-consuming task for worldwide researchers to have a clear perspective of\nthe most recent and advanced software libraries. This work contributes to\nclarify the current situation in the domain, outlining the most useful\nlibraries that implement and facilitate deep learning application to\nneuroscience, allowing scientists to identify the most suitable options for\ntheir research or clinical projects. This paper summarizes the main\ndevelopments in Deep Learning and their relevance to Neuroscience; it then\nreviews neuroinformatic toolboxes and libraries, collected from the literature\nand from specific hubs of software projects oriented to neuroscience research.\nThe selected tools are presented in tables detailing key features grouped by\ndomain of application (e.g. data type, neuroscience area, task), model\nengineering (e.g. programming language, model customization) and technological\naspect (e.g. interface, code source). The results show that, among a high\nnumber of available software tools, several libraries are standing out in terms\nof functionalities for neuroscience applications. The aggregation and\ndiscussion of this information can help the neuroscience community to devolop\ntheir research projects more efficiently and quickly, both by means of readily\navailable tools, and by knowing which modules may be improved, connected or\nadded.",
    "authors": [
      "Louis Fabrice Tshimanga",
      "Manfredo Atzori",
      "Federico Del Pup",
      "Maurizio Corbetta"
    ],
    "publication_date": "2022-12-19T09:09:40Z",
    "arxiv_id": "http://arxiv.org/abs/2301.05057v1",
    "download_url": "http://arxiv.org/abs/2301.05057v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Multilayer Brain Networks",
    "abstract": "The field of neuroscience is facing an unprecedented expanse in the volume\nand diversity of available data. Traditionally, network models have provided\nkey insights into the structure and function of the brain. With the advent of\nbig data in neuroscience, both more sophisticated models capable of\ncharacterizing the increasing complexity of the data and novel methods of\nquantitative analysis are needed. Recently multilayer networks, a mathematical\nextension of traditional networks, have gained increasing popularity in\nneuroscience due to their ability to capture the full information of\nmulti-model, multi-scale, spatiotemporal data sets. Here, we review multilayer\nnetworks and their applications in neuroscience, showing how incorporating the\nmultilayer framework into network neuroscience analysis has uncovered\npreviously hidden features of brain networks. We specifically highlight the use\nof multilayer networks to model disease, structure-function relationships,\nnetwork evolution, and link multi-scale data. Finally, we close with a\ndiscussion of promising new directions of multilayer network neuroscience\nresearch and propose a modified definition of multilayer networks designed to\nunite and clarify the use of the multilayer formalism in describing real-world\nsystems.",
    "authors": [
      "Michael Vaiana",
      "Sarah Muldoon"
    ],
    "publication_date": "2017-09-07T16:03:48Z",
    "arxiv_id": "http://arxiv.org/abs/1709.02325v1",
    "download_url": "http://arxiv.org/abs/1709.02325v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "An interdisciplinary approach to high school curriculum development:\n  Swarming Powered by Neuroscience",
    "abstract": "This article discusses how to create an interactive virtual training program\nat the intersection of neuroscience, robotics, and computer science for high\nschool students. A four-day microseminar, titled Swarming Powered by\nNeuroscience (SPN), was conducted virtually through a combination of\npresentations and interactive computer game simulations, delivered by subject\nmatter experts in neuroscience, mathematics, multi-agent swarm robotics, and\neducation. The objective of this research was to determine if taking an\ninterdisciplinary approach to high school education would enhance the students\nlearning experiences in fields such as neuroscience, robotics, or computer\nscience. This study found an improvement in student engagement for neuroscience\nby 16.6%, while interest in robotics and computer science improved respectively\nby 2.7% and 1.8%. The curriculum materials, developed for the SPN microseminar,\ncan be used by high school teachers to further evaluate interdisciplinary\ninstructions across life and physical sciences and computer science.",
    "authors": [
      "Elise Buckley",
      "Joseph D. Monaco",
      "Kevin M. Schultz",
      "Robert Chalmers",
      "Armin Hadzic",
      "Kechen Zhang",
      "Grace M. Hwang",
      "M. Dwight Carr"
    ],
    "publication_date": "2021-09-12T16:00:00Z",
    "arxiv_id": "http://arxiv.org/abs/2109.05545v1",
    "download_url": "http://arxiv.org/abs/2109.05545v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Unifying equivalences across unsupervised learning, network science, and\n  imaging/network neuroscience",
    "abstract": "Modern scientific fields face the challenge of integrating a wealth of data,\nanalyses, and results. We recently showed that a neglect of this integration\ncan lead to circular analyses and redundant explanations. Here, we help advance\nscientific integration by describing equivalences that unify diverse analyses\nof datasets and networks. We describe equivalences across analyses of\nclustering and dimensionality reduction, network centrality and dynamics, and\npopular models in imaging and network neuroscience. First, we equate\nfoundational objectives across unsupervised learning and network science (from\nk means to modularity to UMAP), fuse classic algorithms for optimizing these\nobjectives, and extend these objectives to simplify interpretations of popular\ndimensionality reduction methods. Second, we equate basic measures of\nconnectional magnitude and dispersion with six measures of communication,\ncontrol, and diversity in network science and network neuroscience. Third, we\ndescribe three semi-analytical vignettes that clarify and simplify the\ninterpretation of structural and dynamical analyses in imaging and network\nneuroscience. We illustrate our results on example brain-imaging data and\nprovide abct, an open multi-language toolbox that implements our analyses.\nTogether, our study unifies diverse analyses across unsupervised learning,\nnetwork science, imaging neuroscience, and network neuroscience.",
    "authors": [
      "Mika Rubinov"
    ],
    "publication_date": "2025-08-12T06:28:49Z",
    "arxiv_id": "http://arxiv.org/abs/2508.10045v1",
    "download_url": "http://arxiv.org/abs/2508.10045v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "On the nature of explanations offered by network science: A perspective\n  from and for practicing neuroscientists",
    "abstract": "Network neuroscience represents the brain as a collection of regions and\ninter-regional connections. Given its ability to formalize systems-level\nmodels, network neuroscience has generated unique explanations of neural\nfunction and behavior. The mechanistic status of these explanations and how\nthey can contribute to and fit within the field of neuroscience as a whole has\nreceived careful treatment from philosophers. However, these philosophical\ncontributions have not yet reached many neuroscientists. Here we complement\nformal philosophical efforts by providing an applied perspective from and for\nneuroscientists. We discuss the mechanistic status of the explanations offered\nby network neuroscience and how they contribute to, enhance, and interdigitate\nwith other types of explanations in neuroscience. In doing so, we rely on\nphilosophical work concerning the role of causality, scale, and mechanisms in\nscientific explanations. In particular, we make the distinction between an\nexplanation and the evidence supporting that explanation, and we argue for a\nscale-free nature of mechanistic explanations. In the course of these\ndiscussions, we hope to provide a useful applied framework in which network\nneuroscience explanations can be exercised across scales and combined with\nother fields of neuroscience to gain deeper insights into the brain and\nbehavior.",
    "authors": [
      "Maxwell A. Bertolero",
      "Danielle S. Bassett"
    ],
    "publication_date": "2019-11-12T17:49:10Z",
    "arxiv_id": "http://arxiv.org/abs/1911.05031v1",
    "download_url": "http://arxiv.org/abs/1911.05031v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Epistemic integration and social segregation of AI in neuroscience",
    "abstract": "In recent years, Artificial Intelligence (AI) shows a spectacular ability of\ninsertion inside a variety of disciplines which use it for scientific\nadvancements and which sometimes improve it for their conceptual and\nmethodological needs. According to the transverse science framework originally\nconceived by Shinn and Joerges, AI can be seen as an instrument which is\nprogressively acquiring a universal character through its diffusion across\nscience. In this paper we address empirically one aspect of this diffusion,\nnamely the penetration of AI into a specific field of research. Taking\nneuroscience as a case study, we conduct a scientometric analysis of the\ndevelopment of AI in this field. We especially study the temporal egocentric\ncitation network around the articles included in this literature, their\nrepresented journals and their authors linked together by a temporal\ncollaboration network. We find that AI is driving the constitution of a\nparticular disciplinary ecosystem in neuroscience which is distinct from other\nsubfields, and which is gathering atypical scientific profiles who are coming\nfrom neuroscience or outside it. Moreover we observe that this AI community in\nneuroscience is socially confined in a specific subspace of the neuroscience\ncollaboration network, which also publishes in a small set of dedicated\njournals that are mostly active in AI research. According to these results, the\ndiffusion of AI in a discipline such as neuroscience didn't really challenge\nits disciplinary orientations but rather induced the constitution of a\ndedicated socio-cognitive environment inside this field.",
    "authors": [
      "Sylvain Fontaine",
      "Floriana Gargiulo",
      "Michel Dubois",
      "Paola Tubaro"
    ],
    "publication_date": "2023-10-02T09:48:42Z",
    "arxiv_id": "http://arxiv.org/abs/2310.01046v2",
    "download_url": "http://arxiv.org/abs/2310.01046v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive\n  Review",
    "abstract": "The advent of large-scale artificial intelligence (AI) models has a\ntransformative effect on neuroscience research, which represents a paradigm\nshift from the traditional computational methods through the facilitation of\nend-to-end learning from raw brain signals and neural data. In this paper, we\nexplore the transformative effects of large-scale AI models on five major\nneuroscience domains: neuroimaging and data processing, brain-computer\ninterfaces and neural decoding, molecular neuroscience and genomic modeling,\nclinical assistance and translational frameworks, and disease-specific\napplications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including\nmultimodal neural data integration, spatiotemporal pattern interpretation, and\nthe derivation of translational frameworks for clinical deployment. Moreover,\nthe interaction between neuroscience and AI has become increasingly reciprocal,\nas biologically informed architectural constraints are now incorporated to\ndevelop more interpretable and computationally efficient models. This review\nhighlights both the notable promise of such technologies and key implementation\nconsiderations, with particular emphasis on rigorous evaluation frameworks,\neffective domain knowledge integration, and comprehensive ethical guidelines\nfor clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse\nresearch applications is provided.",
    "authors": [
      "Shihao Yang",
      "Xiying Huang",
      "Danilo Bernardo",
      "Jun-En Ding",
      "Andrew Michael",
      "Jingmei Yang",
      "Patrick Kwan",
      "Ashish Raj",
      "Feng Liu"
    ],
    "publication_date": "2025-10-18T22:45:59Z",
    "arxiv_id": "http://arxiv.org/abs/2510.16658v1",
    "download_url": "http://arxiv.org/abs/2510.16658v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Philosophical Understanding of Representation for Neuroscience",
    "abstract": "Neuroscientists often describe neural activity as a representation of\nsomething, or claim to have found evidence for a neural representation. But\nwhat do these statements mean? The reasons to call some neural activity a\nrepresentation and the assumptions that come with this term are not generally\nmade clear from its common uses in neuroscience. Representation is a central\nconcept in philosophy of mind, with a rich history going back to the ancient\nperiod. In order to clarify its usage in neuroscience, here we advance a link\nbetween the connotations of this term across these disciplines. We draw on a\nbroad range of discourse in philosophy to distinguish three key aspects of\nrepresentation: correspondence, functional role, and teleology. We argue that\neach of these aspects are implied by the explanatory role the term plays in\nneuroscience. However, evidence related to all three aspects is rarely\npresented or discussed in the course of individual studies that aim to identify\nrepresentations. Overlooking the significance of all three aspects hinders\ncommunication in neuroscience, as it obscures the limitations of experimental\nparadigms and conceals gaps in our understanding of the phenomena of primary\ninterest. Working from this three-part view, we discuss how to move toward\nclearer communication about representations in the brain.",
    "authors": [
      "Ben Baker",
      "Benjamin Lansdell",
      "Konrad Kording"
    ],
    "publication_date": "2021-02-12T16:01:24Z",
    "arxiv_id": "http://arxiv.org/abs/2102.06592v2",
    "download_url": "http://arxiv.org/abs/2102.06592v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A learning gap between neuroscience and reinforcement learning",
    "abstract": "Historically, artificial intelligence has drawn much inspiration from\nneuroscience to fuel advances in the field. However, current progress in\nreinforcement learning is largely focused on benchmark problems that fail to\ncapture many of the aspects that are of interest in neuroscience today. We\nillustrate this point by extending a T-maze task from neuroscience for use with\nreinforcement learning algorithms, and show that state-of-the-art algorithms\nare not capable of solving this problem. Finally, we point out where insights\nfrom neuroscience could help explain some of the issues encountered.",
    "authors": [
      "Samuel T. Wauthier",
      "Pietro Mazzaglia",
      "Ozan Çatal",
      "Cedric De Boom",
      "Tim Verbelen",
      "Bart Dhoedt"
    ],
    "publication_date": "2021-04-22T11:25:21Z",
    "arxiv_id": "http://arxiv.org/abs/2104.10995v3",
    "download_url": "http://arxiv.org/abs/2104.10995v3",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Recommendations for repositories and scientific gateways from a\n  neuroscience perspective",
    "abstract": "Digital services such as repositories and science gateways have become key\nresources for the neuroscience community, but users often have a hard time\norienting themselves in the service landscape to find the best fit for their\nparticular needs. INCF (International Neuroinformatics Coordinating Facility)\nhas developed a set of recommendations and associated criteria for choosing or\nsetting up and running a repository or scientific gateway, intended for the\nneuroscience community, with a FAIR neuroscience perspective. These\nrecommendations have neurosciences as their primary use case but are often\ngeneral. Considering the perspectives of researchers and providers of\nrepositories as well as scientific gateways, the recommendations harmonize and\ncomplement existing work on criteria for repositories and best practices. The\nrecommendations cover a range of important areas including accessibility,\nlicensing, community responsibility and technical and financial sustainability\nof a service.",
    "authors": [
      "Malin Sandström",
      "Mathew Abrams",
      "Jan Bjaalie",
      "Mona Hicks",
      "David Kennedy",
      "Arvind Kumar",
      "JB Poline",
      "Prasun Roy",
      "Paul Tiesinga",
      "Thomas Wachtler",
      "Wojtek Goscinski"
    ],
    "publication_date": "2022-01-03T16:01:26Z",
    "arxiv_id": "http://arxiv.org/abs/2201.00727v1",
    "download_url": "http://arxiv.org/abs/2201.00727v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A perspective on neuroscience data standardization with Neurodata\n  Without Borders",
    "abstract": "Neuroscience research has evolved to generate increasingly large and complex\nexperimental data sets, and advanced data science tools are taking on central\nroles in neuroscience research. Neurodata Without Borders (NWB), a standard\nlanguage for neurophysiology data, has recently emerged as a powerful solution\nfor data management, analysis, and sharing. We here discuss our efforts to\nimplement NWB data science pipelines. We describe general principles and\nspecific use cases that illustrate successes, challenges, and non-trivial\ndecisions in software engineering. We hope that our experience can provide\nguidance for the neuroscience community and help bridge the gap between\nexperimental neuroscience and data science.",
    "authors": [
      "Andrea Pierré",
      "Tuan Pham",
      "Jonah Pearl",
      "Sandeep Robert Datta",
      "Jason T. Ritt",
      "Alexander Fleischmann"
    ],
    "publication_date": "2023-10-06T15:28:51Z",
    "arxiv_id": "http://arxiv.org/abs/2310.04317v2",
    "download_url": "http://arxiv.org/abs/2310.04317v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "An introduction to reinforcement learning for neuroscience",
    "abstract": "Reinforcement learning (RL) has a rich history in neuroscience, from early\nwork on dopamine as a reward prediction error signal (Schultz et al., 1997) to\nrecent work proposing that the brain could implement a form of 'distributional\nreinforcement learning' popularized in machine learning (Dabney et al., 2020).\nThere has been a close link between theoretical advances in reinforcement\nlearning and neuroscience experiments throughout this literature, and the\ntheories describing the experimental data have therefore become increasingly\ncomplex. Here, we provide an introduction and mathematical background to many\nof the methods that have been used in systems neroscience. We start with an\noverview of the RL problem and classical temporal difference algorithms,\nfollowed by a discussion of 'model-free', 'model-based', and intermediate RL\nalgorithms. We then introduce deep reinforcement learning and discuss how this\nframework has led to new insights in neuroscience. This includes a particular\nfocus on meta-reinforcement learning (Wang et al., 2018) and distributional RL\n(Dabney et al., 2020). Finally, we discuss potential shortcomings of the RL\nformalism for neuroscience and highlight open questions in the field. Code that\nimplements the methods discussed and generates the figures is also provided.",
    "authors": [
      "Kristopher T. Jensen"
    ],
    "publication_date": "2023-11-13T13:10:52Z",
    "arxiv_id": "http://arxiv.org/abs/2311.07315v3",
    "download_url": "http://arxiv.org/abs/2311.07315v3",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Matching domain experts by training from scratch on domain knowledge",
    "abstract": "Recently, large language models (LLMs) have outperformed human experts in\npredicting the results of neuroscience experiments (Luo et al., 2024). What is\nthe basis for this performance? One possibility is that statistical patterns in\nthat specific scientific literature, as opposed to emergent reasoning abilities\narising from broader training, underlie LLMs' performance. To evaluate this\npossibility, we trained (next word prediction) a relatively small\n124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge.\nDespite being orders of magnitude smaller than larger LLMs trained on trillions\nof tokens, small models achieved expert-level performance in predicting\nneuroscience results. Small models trained on the neuroscience literature\nsucceeded when they were trained from scratch using a tokenizer specifically\ntrained on neuroscience text or when the neuroscience literature was used to\nfinetune a pretrained GPT-2. Our results indicate that expert-level performance\nmay be attained by even small LLMs through domain-specific, auto-regressive\ntraining approaches.",
    "authors": [
      "Xiaoliang Luo",
      "Guangzhi Sun",
      "Bradley C. Love"
    ],
    "publication_date": "2024-05-15T14:50:51Z",
    "arxiv_id": "http://arxiv.org/abs/2405.09395v2",
    "download_url": "http://arxiv.org/abs/2405.09395v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Dynamical Cartography of the Epistemic Diffusion of Artificial\n  Intelligence in Neuroscience",
    "abstract": "Neuroscience and AI have an intertwined history, largely relayed in the\nliterature of both fields. In recent years, due to the engineering orientations\nof AI research and the monopoly of industry for its large-scale applications,\nthe mutual expansion of neuroscience and AI in fundamental research seems\nchallenged. In this paper, we bring some empirical evidences that, on the\ncontrary, AI and neuroscience are continuing to grow together, but with a\npronounced interest in the fields of study related to neurodegenerative\ndiseases since the 1990s. With a temporal knowledge cartography of neuroscience\ndrawn with advanced document embedding techniques, we draw the dynamical\nshaping of the discipline since the 1970s and identified the conceptual\narticulation of AI with this particular subfield mentioned before. However, a\nfurther analysis of the underlying citation network of the studied corpus shows\nthat the produced AI technologies remain confined in the different subfields\nand are not transferred from one subfield to another. This invites us to\ndiscuss the genericity capability of AI in the context of an intradisciplinary\ndevelopment, especially in the diffusion of its associated metrology.",
    "authors": [
      "Sylvain Fontaine"
    ],
    "publication_date": "2025-07-02T12:24:44Z",
    "arxiv_id": "http://arxiv.org/abs/2507.01651v1",
    "download_url": "http://arxiv.org/abs/2507.01651v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for\n  Domain-Specific Retrieval",
    "abstract": "We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector\nembedding model engineered for high-precision information retrieval tasks. Our\nmethodology encompasses the curation of an extensive domain-specific training\ncorpus comprising 500,000 carefully constructed triplets\n(query-positive-negative configurations), augmented with 250,000\nneuroscience-specific definitional entries and 250,000 structured\nknowledge-graph triplets derived from authoritative neurological ontologies. We\nemploy a sophisticated fine-tuning approach utilizing the\nFremyCompany/BioLORD-2023 foundation model, implementing a multi-objective\noptimization framework combining contrastive learning with triplet-based metric\nlearning paradigms. Comprehensive evaluation on a held-out test dataset\ncomprising approximately 24,000 neuroscience-specific queries demonstrates\nsubstantial performance improvements over state-of-the-art general-purpose and\nbiomedical embedding models. These empirical findings underscore the critical\nimportance of domain-specific embedding architectures for neuroscience-oriented\nRAG systems and related clinical natural language processing applications.",
    "authors": [
      "Devendra Patel",
      "Aaditya Jain",
      "Jayant Verma",
      "Divyansh Rajput",
      "Sunil Mahala",
      "Ketki Suresh Khapare",
      "Jayateja Kalla"
    ],
    "publication_date": "2025-07-04T06:28:53Z",
    "arxiv_id": "http://arxiv.org/abs/2507.03329v1",
    "download_url": "http://arxiv.org/abs/2507.03329v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "On directed information theory and Granger causality graphs",
    "abstract": "Directed information theory deals with communication channels with feedback.\nWhen applied to networks, a natural extension based on causal conditioning is\nneeded. We show here that measures built from directed information theory in\nnetworks can be used to assess Granger causality graphs of stochastic\nprocesses. We show that directed information theory includes measures such as\nthe transfer entropy, and that it is the adequate information theoretic\nframework needed for neuroscience applications, such as connectivity inference\nproblems.",
    "authors": [
      "P. O. Amblard",
      "O. J. J. Michel"
    ],
    "publication_date": "2010-02-07T12:42:18Z",
    "arxiv_id": "http://arxiv.org/abs/1002.1446v1",
    "download_url": "http://arxiv.org/abs/1002.1446v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "An Introductory Review of Information Theory in the Context of\n  Computational Neuroscience",
    "abstract": "This paper introduces several fundamental concepts in information theory from\nthe perspective of their origins in engineering. Understanding such concepts is\nimportant in neuroscience for two reasons. Simply applying formulae from\ninformation theory without understanding the assumptions behind their\ndefinitions can lead to erroneous results and conclusions. Furthermore, this\ncentury will see a convergence of information theory and neuroscience;\ninformation theory will expand its foundations to incorporate more\ncomprehensively biological processes thereby helping reveal how neuronal\nnetworks achieve their remarkable information processing abilities.",
    "authors": [
      "Mark D. McDonnell",
      "Shiro Ikeda",
      "Jonathan H. Manton"
    ],
    "publication_date": "2011-07-15T02:58:18Z",
    "arxiv_id": "http://arxiv.org/abs/1107.2984v1",
    "download_url": "http://arxiv.org/abs/1107.2984v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Speed and accuracy in a visual motion discrimination task as performed\n  by rats",
    "abstract": "We find that rats, like primates and humans, perform better on the random dot\nmotion task when they take more time to respond. We provide evidence that this\nimprovement is due to stimulus integration. Rats increase their response\nlatency modestly as a function of trial difficulty. Rats can modulate response\nlatency more strongly on a trial by trial basis, apparently on the basis of\nreward-related parameters.",
    "authors": [
      "Pamela Reinagel",
      "Emily Mankin",
      "Adam Calhoun"
    ],
    "publication_date": "2012-06-01T21:26:53Z",
    "arxiv_id": "http://arxiv.org/abs/1206.0311v1",
    "download_url": "http://arxiv.org/abs/1206.0311v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "What can a mathematician do in neuroscience?",
    "abstract": "Mammalian brain is one of the most complex objects in the known universe, as\nit governs every aspect of animal's and human behavior. It is fair to say that\nwe have a very limited knowledge of how the brain operates and functions.\nComputational Neuroscience is a scientific discipline that attempts to\nunderstand and describe the brain in terms of mathematical modeling. This\nuser-friendly review tries to introduce this relatively new field to\nmathematicians and physicists by showing examples of recent trends. It also\ndiscusses briefly future prospects for constructing an integrated theory of\nbrain function.",
    "authors": [
      "Jan Karbowski"
    ],
    "publication_date": "2014-05-16T16:41:45Z",
    "arxiv_id": "http://arxiv.org/abs/1405.4239v1",
    "download_url": "http://arxiv.org/abs/1405.4239v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Network neuroscience and the connectomics revolution",
    "abstract": "Connectomics and network neuroscience offer quantitative scientific\nframeworks for modeling and analyzing networks of structurally and functionally\ninteracting neurons, neuronal populations, and macroscopic brain areas. This\nshift in perspective and emphasis on distributed brain function has provided\nfundamental insight into the role played by the brain's network architecture in\ncognition, disease, development, and aging. In this chapter, we review the core\nconcepts of human connectomics at the macroscale. From the construction of\nnetworks using functional and diffusion MRI data, to their subsequent analysis\nusing methods from network neuroscience, this review highlights key findings,\ncommonly-used methodologies, and discusses several emerging frontiers in\nconnectomics.",
    "authors": [
      "Richard Betzel"
    ],
    "publication_date": "2020-10-04T14:36:03Z",
    "arxiv_id": "http://arxiv.org/abs/2010.01591v1",
    "download_url": "http://arxiv.org/abs/2010.01591v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Data Processing of Functional Optical Microscopy for Neuroscience",
    "abstract": "Functional optical imaging in neuroscience is rapidly growing with the\ndevelopment of new optical systems and fluorescence indicators. To realize the\npotential of these massive spatiotemporal datasets for relating neuronal\nactivity to behavior and stimuli and uncovering local circuits in the brain,\naccurate automated processing is increasingly essential. In this review, we\ncover recent computational developments in the full data processing pipeline of\nfunctional optical microscopy for neuroscience data and discuss ongoing and\nemerging challenges.",
    "authors": [
      "Hadas Benisty",
      "Alexander Song",
      "Gal Mishne",
      "Adam S. Charles"
    ],
    "publication_date": "2022-01-10T18:53:03Z",
    "arxiv_id": "http://arxiv.org/abs/2201.03537v1",
    "download_url": "http://arxiv.org/abs/2201.03537v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "PiEEG-16 to Measure 16 EEG Channels with Raspberry Pi for Brain-Computer\n  Interfaces and EEG devices",
    "abstract": "This article introduces a cost-effective gateway into the fascinating world\nof neuroscience: the PIEEG-16, a versatile shield for RaspberryPi designed to\nmeasure 16 channels of various biosignals, including EEG\n(electroencephalography), EMG (electromyography), and ECG (electrocardiography)\nwithout any data transfer over the network (Wi-Fi, Bluetooth) and processing\nand feature ectraction directly on the Raspberry in real-time. This innovative\ntool opens up new possibilities for neuroscience research and brain-computer\ninterface experiments. By combining the power of RaspberryPi with specialized\nbiosignal measurement capabilities, the PIEEG-16 represents a significant step\nforward in democratizing neuroscience research and exploration.",
    "authors": [
      "Ildar Rakhmatulin"
    ],
    "publication_date": "2024-09-08T22:06:56Z",
    "arxiv_id": "http://arxiv.org/abs/2409.07491v1",
    "download_url": "http://arxiv.org/abs/2409.07491v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Separating minimal from radical embodied cognitive neuroscience",
    "abstract": "Mougenot and Matheson (2024) make a compelling case for the development of a\nmechanistic cognitive neuroscience that is embodied. However, their analysis of\nextant work under this header plays down important distinctions between\n\"minimal\" and \"radical\" embodiment. The former remains firmly neurocentric and\ntherefore has limited potential to move the needle in understanding the\nfunctional contributions of neural dynamics to cognition in the context of\nwider organism-environment dynamics.",
    "authors": [
      "Matthieu M. de Wit"
    ],
    "publication_date": "2024-09-17T15:24:16Z",
    "arxiv_id": "http://arxiv.org/abs/2410.01830v1",
    "download_url": "http://arxiv.org/abs/2410.01830v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "BrainKnow -- Extracting, Linking, and Synthesizing Neuroscience\n  Knowledge",
    "abstract": "The exponential growth of neuroscience literature presents a significant\nchallenge for researchers seeking to efficiently access and utilize relevant\ninformation. To address this issue, we introduce the Brain Knowledge Engine\n(BrainKnow), an automated system designed to extract, link, and synthesize\nneuroscience knowledge from scientific publications. BrainKnow constructs a\ncomprehensive knowledge graph encompassing 3,626,931 relationships across\n37,011 neuroscience concepts, derived from 1,817,744 articles. This vast\nrepository of knowledge is accessible through a user-friendly web interface,\nfacilitating efficient navigation and data retrieval. BrainKnow employs\nadvanced graph network algorithms, specifically Node2Vec, to enhance knowledge\nrecommendation and visualization. This enables users to explore semantic\nrelationships between concepts, predict potential new relationships, and gain a\ndeeper understanding of the interconnectedness within neuroscience.\nAdditionally, BrainKnow ensures real-time updates by synchronizing with PubMed,\nproviding researchers with access to the most current information. BrainKnow\nserves as a valuable resource for neuroscience researchers, offering a powerful\ntool for exploring, synthesizing, and leveraging the vast and complex knowledge\nbase of the field.",
    "authors": [
      "Cunqing Huangfu",
      "Kang Sun",
      "Yi Zeng",
      "Yuwei Wang",
      "Dongsheng Wang",
      "Zizhe Ruan"
    ],
    "publication_date": "2024-03-07T09:20:38Z",
    "arxiv_id": "http://arxiv.org/abs/2403.04346v5",
    "download_url": "http://arxiv.org/abs/2403.04346v5",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Bio-Inspired Research Paradigm of Collision Perception Neurons\n  Enabling Neuro-Robotic Integration: The LGMD Case",
    "abstract": "Compared to human vision, locust visual systems excel at rapid and precise\ncollision detection, despite relying on only hundreds of thousands of neurons\norganized through a few neuropils. This efficiency makes them an attractive\nmodel system for developing artificial collision-detecting systems.\nSpecifically, researchers have identified collision-selective neurons in the\nlocust's optic lobe, called lobula giant movement detectors (LGMDs), which\nrespond specifically to approaching objects. Research upon LGMD neurons began\nin the early 1970s. Initially, due to their large size, these neurons were\nidentified as motion detectors, but their role as looming detectors was\nrecognized over time. Since then, progress in neuroscience, computational\nmodeling of LGMD's visual neural circuits, and LGMD-based robotics have\nadvanced in tandem, each field supporting and driving the others. Today, with a\ndeeper understanding of LGMD neurons, LGMD-based models have significantly\nimproved collision-free navigation in mobile robots including ground and aerial\nrobots. This review highlights recent developments in LGMD research from the\nperspectives of neuroscience, computational modeling, and robotics. It\nemphasizes a biologically plausible research paradigm, where insights from\nneuroscience inform real-world applications, which would in turn validate and\nadvance neuroscience. With strong support from extensive research and growing\napplication demand, this paradigm has reached a mature stage and demonstrates\nversatility across different areas of neuroscience research, thereby enhancing\nour understanding of the interconnections between neuroscience, computational\nmodeling, and robotics. Furthermore, this paradigm would shed light upon the\nmodeling and robotic research into other motion-sensitive neurons or neural\ncircuits.",
    "authors": [
      "Ziyan Qin",
      "Jigen Peng",
      "Shigang Yue",
      "Qinbing Fu"
    ],
    "publication_date": "2025-01-06T12:44:48Z",
    "arxiv_id": "http://arxiv.org/abs/2501.02982v2",
    "download_url": "http://arxiv.org/abs/2501.02982v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Brighter than the sun: Powerscape visualizations illustrate power needs\n  in neuroscience and psychology",
    "abstract": "Participant needs to achieve a given power are frequently underestimated.\nThis is particularly problematic when effect sizes are small, such as is common\nin neuroscience and psychology. We provide tools to make these demands\nimmediately obvious in the form of a powerscape visualization.",
    "authors": [
      "Pascal Wallisch"
    ],
    "publication_date": "2015-12-31T20:57:10Z",
    "arxiv_id": "http://arxiv.org/abs/1512.09368v1",
    "download_url": "http://arxiv.org/abs/1512.09368v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Summary of Information Theoretic Quantities",
    "abstract": "Information theory is a practical and theoretical framework developed for the\nstudy of communication over noisy channels. Its probabilistic basis and\ncapacity to relate statistical structure to function make it ideally suited for\nstudying information flow in the nervous system. As a framework it has a number\nof useful properties: it provides a general measure sensitive to any\nrelationship, not only linear effects; its quantities have meaningful units\nwhich in many cases allow direct comparison between different experiments; and\nit can be used to study how much information can be gained by observing neural\nresponses in single experimental trials, rather than in averages over multiple\ntrials. A variety of information theoretic quantities are in common use in\nneuroscience - including the Shannon entropy, Kullback-Leibler divergence, and\nmutual information. In this entry, we introduce and define these quantities.\nFurther details on how these quantities can be estimated in practice are\nprovided in the entry \"Estimation of Information-Theoretic Quantities\" and\nexamples of application of these techniques in neuroscience can be found in the\nentry \"Applications of Information-Theoretic Quantities in Neuroscience\".",
    "authors": [
      "Robin A. A. Ince",
      "Stefano Panzeri",
      "Simon R. Schultz"
    ],
    "publication_date": "2015-01-08T14:09:01Z",
    "arxiv_id": "http://arxiv.org/abs/1501.01854v1",
    "download_url": "http://arxiv.org/abs/1501.01854v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Detecting and Tracking The Real-time Hot Topics: A Study on\n  Computational Neuroscience",
    "abstract": "In this study, following the idea of our previous paper (Wang, et al.,\n2013a), we improve the method to detect and track hot topics in a specific\nfield by using the real-time article usage data. With the \"usage count\" data\nprovided by Web of Science, we take the field of computational neuroscience as\nan example to make analysis. About 10 thousand articles in the field of\nComputational Neuroscience are queried in Web of Science, when the records,\nincluding the usage count data of each paper, have been harvested and updated\nweekly from October 19, 2015 to March 21, 2016. The hot topics are defined by\nthe most frequently used keywords aggregated from the articles. The analysis\nreveals that hot topics in Computational Neuroscience are related to the key\ntechnologies, like \"fmri\", \"eeg\", \"erp\", etc. Furthermore, using the weekly\nupdated data, we track the dynamical changes of the topics. The characteristic\nof immediacy of usage data makes it possible to track the \"heat\" of hot topics\ntimely and dynamically.",
    "authors": [
      "Xianwen Wang",
      "Zhichao Fang"
    ],
    "publication_date": "2016-08-19T07:29:34Z",
    "arxiv_id": "http://arxiv.org/abs/1608.05517v1",
    "download_url": "http://arxiv.org/abs/1608.05517v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Biological Blueprints for Next Generation AI Systems",
    "abstract": "Diverse subfields of neuroscience have enriched artificial intelligence for\nmany decades. With recent advances in machine learning and artificial neural\nnetworks, many neuroscientists are partnering with AI researchers and machine\nlearning experts to analyze data and construct models. This paper attempts to\ndemonstrate the value of such collaborations by providing examples of how\ninsights derived from neuroscience research are helping to develop new machine\nlearning algorithms and artificial neural network architectures. We survey the\nrelevant neuroscience necessary to appreciate these insights and then describe\nhow we can translate our current understanding of the relevant neurobiology\ninto algorithmic techniques and architectural designs. Finally, we characterize\nsome of the major challenges facing current AI technology and suggest avenues\nfor overcoming these challenges that draw upon research in developmental and\ncomparative cognitive neuroscience.",
    "authors": [
      "Thomas Dean",
      "Chaofei Fan",
      "Francis E. Lewis",
      "Megumi Sano"
    ],
    "publication_date": "2019-12-01T14:50:23Z",
    "arxiv_id": "http://arxiv.org/abs/1912.00421v1",
    "download_url": "http://arxiv.org/abs/1912.00421v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Causality in cognitive neuroscience: concepts, challenges, and\n  distributional robustness",
    "abstract": "While probabilistic models describe the dependence structure between observed\nvariables, causal models go one step further: they predict, for example, how\ncognitive functions are affected by external interventions that perturb\nneuronal activity. In this review and perspective article, we introduce the\nconcept of causality in the context of cognitive neuroscience and review\nexisting methods for inferring causal relationships from data. Causal inference\nis an ambitious task that is particularly challenging in cognitive\nneuroscience. We discuss two difficulties in more detail: the scarcity of\ninterventional data and the challenge of finding the right variables. We argue\nfor distributional robustness as a guiding principle to tackle these problems.\nRobustness (or invariance) is a fundamental principle underlying causal\nmethodology. A causal model of a target variable generalises across\nenvironments or subjects as long as these environments leave the causal\nmechanisms intact. Consequently, if a candidate model does not generalise, then\neither it does not consist of the target variable's causes or the underlying\nvariables do not represent the correct granularity of the problem. In this\nsense, assessing generalisability may be useful when defining relevant\nvariables and can be used to partially compensate for the lack of\ninterventional data.",
    "authors": [
      "Sebastian Weichwald",
      "Jonas Peters"
    ],
    "publication_date": "2020-02-14T14:49:34Z",
    "arxiv_id": "http://arxiv.org/abs/2002.06060v2",
    "download_url": "http://arxiv.org/abs/2002.06060v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Present and future frameworks of theoretical neuroscience: outcomes of a\n  community discussion",
    "abstract": "We organized a workshop on the \"Present and Future Frameworks of Theoretical\nNeuroscience\", with the support of the National Science Foundation. The\nobjective was to identify the challenges and strategies that this field will\nneed to tackle in order to incorporate vast and multi-scale streams of\nexperimental data from the technologies developed by the BRAIN initiative. The\nparticipants, divided in workgroups, identified five key areas that, while not\nexhaustive, cover multiple aspects of current challenges needed to be\ndeveloped: Dynamics-statistics; multi-scale integration; coding; brain-body\nintegration; and structure of neuroscience theories. While each area is\ndifferent, there were coincidences on finding theoretical paths to incorporate\nbiophysics, energetics, and ethology with more abstract coding and\ncomputational approaches. Each workgroup has continued to work after the\nmeeting to develop the ideas seeded there, which are started to being\npublished. Here, we provide a perspective of the discussions of each workgroup\nthat point to building on the present foundations of theoretical neuroscience\nand extend them by incorporating multi-scale information with the objective of\nproviding mechanistic insights into the nervous system.",
    "authors": [
      "Horacio G. Rotstein",
      "Fidel Santamaria"
    ],
    "publication_date": "2020-04-03T16:38:31Z",
    "arxiv_id": "http://arxiv.org/abs/2004.01665v1",
    "download_url": "http://arxiv.org/abs/2004.01665v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Reinforcement Learning and its Connections with Neuroscience and\n  Psychology",
    "abstract": "Reinforcement learning methods have recently been very successful at\nperforming complex sequential tasks like playing Atari games, Go and Poker.\nThese algorithms have outperformed humans in several tasks by learning from\nscratch, using only scalar rewards obtained through interaction with their\nenvironment. While there certainly has been considerable independent innovation\nto produce such results, many core ideas in reinforcement learning are inspired\nby phenomena in animal learning, psychology and neuroscience. In this paper, we\ncomprehensively review a large number of findings in both neuroscience and\npsychology that evidence reinforcement learning as a promising candidate for\nmodeling learning and decision making in the brain. In doing so, we construct a\nmapping between various classes of modern RL algorithms and specific findings\nin both neurophysiological and behavioral literature. We then discuss the\nimplications of this observed relationship between RL, neuroscience and\npsychology and its role in advancing research in both AI and brain science.",
    "authors": [
      "Ajay Subramanian",
      "Sharad Chitlangia",
      "Veeky Baths"
    ],
    "publication_date": "2020-06-25T04:29:15Z",
    "arxiv_id": "http://arxiv.org/abs/2007.01099v5",
    "download_url": "http://arxiv.org/abs/2007.01099v5",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Probing artificial neural networks: insights from neuroscience",
    "abstract": "A major challenge in both neuroscience and machine learning is the\ndevelopment of useful tools for understanding complex information processing\nsystems. One such tool is probes, i.e., supervised models that relate features\nof interest to activation patterns arising in biological or artificial neural\nnetworks. Neuroscience has paved the way in using such models through numerous\nstudies conducted in recent decades. In this work, we draw insights from\nneuroscience to help guide probing research in machine learning. We highlight\ntwo important design choices for probes $-$ direction and expressivity $-$ and\nrelate these choices to research goals. We argue that specific research goals\nplay a paramount role when designing a probe and encourage future probing\nstudies to be explicit in stating these goals.",
    "authors": [
      "Anna A. Ivanova",
      "John Hewitt",
      "Noga Zaslavsky"
    ],
    "publication_date": "2021-04-16T16:13:23Z",
    "arxiv_id": "http://arxiv.org/abs/2104.08197v1",
    "download_url": "http://arxiv.org/abs/2104.08197v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Graph Neural Networks in Network Neuroscience",
    "abstract": "Noninvasive medical neuroimaging has yielded many discoveries about the brain\nconnectivity. Several substantial techniques mapping morphological, structural\nand functional brain connectivities were developed to create a comprehensive\nroad map of neuronal activities in the human brain -namely brain graph. Relying\non its non-Euclidean data type, graph neural network (GNN) provides a clever\nway of learning the deep graph structure and it is rapidly becoming the\nstate-of-the-art leading to enhanced performance in various network\nneuroscience tasks. Here we review current GNN-based methods, highlighting the\nways that they have been used in several applications related to brain graphs\nsuch as missing brain graph synthesis and disease classification. We conclude\nby charting a path toward a better application of GNN models in network\nneuroscience field for neurological disorder diagnosis and population graph\nintegration. The list of papers cited in our work is available at\nhttps://github.com/basiralab/GNNs-in-Network-Neuroscience.",
    "authors": [
      "Alaa Bessadok",
      "Mohamed Ali Mahjoub",
      "Islem Rekik"
    ],
    "publication_date": "2021-06-07T11:49:57Z",
    "arxiv_id": "http://arxiv.org/abs/2106.03535v2",
    "download_url": "http://arxiv.org/abs/2106.03535v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning to learn online with neuromodulated synaptic plasticity in\n  spiking neural networks",
    "abstract": "We propose that in order to harness our understanding of neuroscience toward\nmachine learning, we must first have powerful tools for training brain-like\nmodels of learning. Although substantial progress has been made toward\nunderstanding the dynamics of learning in the brain, neuroscience-derived\nmodels of learning have yet to demonstrate the same performance capabilities as\nmethods in deep learning such as gradient descent. Inspired by the successes of\nmachine learning using gradient descent, we demonstrate that models of\nneuromodulated synaptic plasticity from neuroscience can be trained in Spiking\nNeural Networks (SNNs) with a framework of learning to learn through gradient\ndescent to address challenging online learning problems. This framework opens a\nnew path toward developing neuroscience inspired online learning algorithms.",
    "authors": [
      "Samuel Schmidgall",
      "Joe Hays"
    ],
    "publication_date": "2022-06-25T00:28:40Z",
    "arxiv_id": "http://arxiv.org/abs/2206.12520v2",
    "download_url": "http://arxiv.org/abs/2206.12520v2",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Large-scale Foundation Models and Generative AI for BigData Neuroscience",
    "abstract": "Recent advances in machine learning have made revolutionary breakthroughs in\ncomputer games, image and natural language understanding, and scientific\ndiscovery. Foundation models and large-scale language models (LLMs) have\nrecently achieved human-like intelligence thanks to BigData. With the help of\nself-supervised learning (SSL) and transfer learning, these models may\npotentially reshape the landscapes of neuroscience research and make a\nsignificant impact on the future. Here we present a mini-review on recent\nadvances in foundation models and generative AI models as well as their\napplications in neuroscience, including natural language and speech, semantic\nmemory, brain-machine interfaces (BMIs), and data augmentation. We argue that\nthis paradigm-shift framework will open new avenues for many neuroscience\nresearch directions and discuss the accompanying challenges and opportunities.",
    "authors": [
      "Ran Wang",
      "Zhe Sage Chen"
    ],
    "publication_date": "2023-10-27T00:44:40Z",
    "arxiv_id": "http://arxiv.org/abs/2310.18377v1",
    "download_url": "http://arxiv.org/abs/2310.18377v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Launching Your VR Neuroscience Laboratory",
    "abstract": "The proliferation and refinement of affordable virtual reality (VR)\ntechnologies and wearable sensors have opened new frontiers in cognitive and\nbehavioral neuroscience. This chapter offers a broad overview of VR for anyone\ninterested in leveraging it as a research tool. In the first section, it\nexamines the fundamental functionalities of VR and outlines important\nconsiderations that inform the development of immersive content that stimulates\nthe senses. In the second section, the focus of the discussion shifts to the\nimplementation of VR in the context of the neuroscience lab. Practical advice\nis offered on adapting commercial, off-theshelf devices to specific research\npurposes. Further, methods are explored for recording, synchronizing, and\nfusing heterogeneous forms of data obtained through the VR system or add-on\nsensors, as well as for labeling events and capturing game play.",
    "authors": [
      "Ying Choon Wu",
      "Christopher Maymon",
      "Jonathon Paden",
      "Weichen Liu"
    ],
    "publication_date": "2024-05-21T19:37:09Z",
    "arxiv_id": "http://arxiv.org/abs/2405.13171v1",
    "download_url": "http://arxiv.org/abs/2405.13171v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A theory of neural emulators",
    "abstract": "A central goal in neuroscience is to provide explanations for how animal\nnervous systems can generate actions and cognitive states such as consciousness\nwhile artificial intelligence (AI) and machine learning (ML) seek to provide\nmodels that are increasingly better at prediction. Despite many decades of\nresearch we have made limited progress on providing neuroscience explanations\nyet there is an increased use of AI and ML methods in neuroscience for\nprediction of behavior and even cognitive states. Here we propose emulator\ntheory (ET) and neural emulators as circuit- and scale-independent predictive\nmodels of biological brain activity and emulator theory (ET) as an alternative\nresearch paradigm in neuroscience. ET proposes that predictive models trained\nsolely on neural dynamics and behaviors can generate functionally\nindistinguishable systems from their sources. That is, compared to the\nbiological organisms which they model, emulators may achieve indistinguishable\nbehavior and cognitive states - including consciousness - without any\nmechanistic explanations. We posit ET via several conjectures, discuss the\nnature of endogenous and exogenous activation of neural circuits, and discuss\nneural causality of phenomenal states. ET provides the conceptual and empirical\nframework for prediction-based models of neural dynamics and behavior without\nexplicit representations of idiosyncratically evolved nervous systems.",
    "authors": [
      "Catalin C. Mitelut"
    ],
    "publication_date": "2024-05-22T07:12:03Z",
    "arxiv_id": "http://arxiv.org/abs/2405.13394v1",
    "download_url": "http://arxiv.org/abs/2405.13394v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Efficient coding with chaotic neural networks: A journey from\n  neuroscience to physics and back",
    "abstract": "This essay, derived from a lecture at \"The Physics Modeling of Thought\"\nworkshop in Berlin in winter 2023, explores the mutually beneficial\nrelationship between theoretical neuroscience and statistical physics through\nthe lens of efficient coding and computation in cortical circuits. It\nhighlights how the study of neural networks has enhanced our understanding of\ncomplex, nonequilibrium, and disordered systems, while also demonstrating how\nneuroscientific challenges have spurred novel developments in physics. The\npaper traces the evolution of ideas from seminal work on chaos in random neural\nnetworks to recent developments in efficient coding and the partial suppression\nof chaotic fluctuations. It emphasizes how concepts from statistical physics,\nsuch as phase transitions and critical phenomena, have been instrumental in\nelucidating the computational capabilities of neural networks.\n  By examining the interplay between order and disorder in neural computation,\nthe essay illustrates the deep connection between theoretical neuroscience and\nthe statistical physics of nonequilibrium systems. This synthesis underscores\nthe ongoing importance of interdisciplinary approaches in advancing both\nfields, offering fresh perspectives on the fundamental principles governing\ninformation processing in biological and artificial systems. This\nmultidisciplinary approach not only advances our understanding of neural\ncomputation and complex systems but also points toward future challenges at the\nintersection of neuroscience and physics.",
    "authors": [
      "Jonathan Kadmon"
    ],
    "publication_date": "2024-08-04T07:34:35Z",
    "arxiv_id": "http://arxiv.org/abs/2408.01949v1",
    "download_url": "http://arxiv.org/abs/2408.01949v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Information thermodynamics: from physics to neuroscience",
    "abstract": "This paper provides a perspective on applying the concepts of information\nthermodynamics, developed recently in non-equilibrium statistical physics, to\nproblems in theoretical neuroscience. Historically, information and energy in\nneuroscience have been treated separately, in contrast to physics approaches,\nwhere the relationship of entropy production with heat is a central idea. It is\nargued here that also in neural systems information and energy can be\nconsidered within the same theoretical framework. Starting from basic ideas of\nthermodynamics and information theory on a classic Brownian particle, it is\nshown how noisy neural networks can infer its probabilistic motion. The\ndecoding of the particle motion by neurons is performed with some accuracy and\nit has some energy cost, and both can be determined using information\nthermodynamics. In a similar fashion, we also discuss how neural networks in\nthe brain can learn the particle velocity, and maintain that information in the\nweights of plastic synapses from a physical point of view. Generally, it is\nshown how the framework of stochastic and information thermodynamics can be\nused practically to study neural inference, learning, and information storing.",
    "authors": [
      "Jan Karbowski"
    ],
    "publication_date": "2024-09-26T07:28:13Z",
    "arxiv_id": "http://arxiv.org/abs/2409.17599v1",
    "download_url": "http://arxiv.org/abs/2409.17599v1",
    "field_of_study": "biology",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  }
]