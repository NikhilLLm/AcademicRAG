[
  {
    "title": "Classic machine learning methods",
    "abstract": "In this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.",
    "authors": [
      "Johann Faouzi",
      "Olivier Colliot"
    ],
    "publication_date": "2023-05-24T13:38:38Z",
    "arxiv_id": "http://arxiv.org/abs/2310.11470v1",
    "download_url": "http://arxiv.org/abs/2310.11470v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Information Theory and its Relation to Machine Learning",
    "abstract": "In this position paper, I first describe a new perspective on machine\nlearning (ML) by four basic problems (or levels), namely, \"What to learn?\",\n\"How to learn?\", \"What to evaluate?\", and \"What to adjust?\". The paper stresses\nmore on the first level of \"What to learn?\", or \"Learning Target Selection\".\nTowards this primary problem within the four levels, I briefly review the\nexisting studies about the connection between information theoretical learning\n(ITL [1]) and machine learning. A theorem is given on the relation between the\nempirically-defined similarity measure and information measures. Finally, a\nconjecture is proposed for pursuing a unified mathematical interpretation to\nlearning target selection.",
    "authors": [
      "Bao-Gang Hu"
    ],
    "publication_date": "2015-01-18T14:57:02Z",
    "arxiv_id": "http://arxiv.org/abs/1501.04309v1",
    "download_url": "http://arxiv.org/abs/1501.04309v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Discussion on Mechanical Learning and Learning Machine",
    "abstract": "Mechanical learning is a computing system that is based on a set of simple\nand fixed rules, and can learn from incoming data. A learning machine is a\nsystem that realizes mechanical learning. Importantly, we emphasis that it is\nbased on a set of simple and fixed rules, contrasting to often called machine\nlearning that is sophisticated software based on very complicated mathematical\ntheory, and often needs human intervene for software fine tune and manual\nadjustments. Here, we discuss some basic facts and principles of such system,\nand try to lay down a framework for further study. We propose 2 directions to\napproach mechanical learning, just like Church-Turing pair: one is trying to\nrealize a learning machine, another is trying to well describe the mechanical\nlearning.",
    "authors": [
      "Chuyu Xiong"
    ],
    "publication_date": "2016-01-31T04:05:50Z",
    "arxiv_id": "http://arxiv.org/abs/1602.00198v1",
    "download_url": "http://arxiv.org/abs/1602.00198v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A systematic review of fuzzing based on machine learning techniques",
    "abstract": "Security vulnerabilities play a vital role in network security system.\nFuzzing technology is widely used as a vulnerability discovery technology to\nreduce damage in advance. However, traditional fuzzing techniques have many\nchallenges, such as how to mutate input seed files, how to increase code\ncoverage, and how to effectively bypass verification. Machine learning\ntechnology has been introduced as a new method into fuzzing test to alleviate\nthese challenges. This paper reviews the research progress of using machine\nlearning technology for fuzzing test in recent years, analyzes how machine\nlearning improve the fuzz process and results, and sheds light on future work\nin fuzzing. Firstly, this paper discusses the reasons why machine learning\ntechniques can be used for fuzzing scenarios and identifies six different\nstages in which machine learning have been used. Then this paper systematically\nstudy the machine learning based fuzzing models from selection of machine\nlearning algorithm, pre-processing methods, datasets, evaluation metrics, and\nhyperparameters setting. Next, this paper assesses the performance of the\nmachine learning models based on the frequently used evaluation metrics. The\nresults of the evaluation prove that machine learning technology has an\nacceptable capability of categorize predictive for fuzzing. Finally, the\ncomparison on capability of discovering vulnerabilities between traditional\nfuzzing tools and machine learning based fuzzing tools is analyzed. The results\ndepict that the introduction of machine learning technology can improve the\nperformance of fuzzing. However, there are still some limitations, such as\nunbalanced training samples and difficult to extract the characteristics\nrelated to vulnerabilities.",
    "authors": [
      "Yan Wang",
      "Peng Jia",
      "Luping Liu",
      "Jiayong Liu"
    ],
    "publication_date": "2019-08-04T02:51:53Z",
    "arxiv_id": "http://arxiv.org/abs/1908.01262v1",
    "download_url": "http://arxiv.org/abs/1908.01262v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Human-Like Active Learning: Machines Simulating the Human Learning\n  Process",
    "abstract": "Although the use of active learning to increase learners' engagement has\nrecently been introduced in a variety of methods, empirical experiments are\nlacking. In this study, we attempted to align two experiments in order to (1)\nmake a hypothesis for machine and (2) empirically confirm the effect of active\nlearning on learning. In Experiment 1, we compared the effect of a passive form\nof learning to active form of learning. The results showed that active learning\nhad a greater learning outcomes than passive learning. In the machine\nexperiment based on the human result, we imitated the human active learning as\na form of knowledge distillation. The active learning framework performed\nbetter than the passive learning framework. In the end, we showed not only that\nwe can make build better machine training framework through the human\nexperiment result, but also empirically confirm the result of human experiment\nthrough imitated machine experiments; human-like active learning have crucial\neffect on learning performance.",
    "authors": [
      "Jaeseo Lim",
      "Hwiyeol Jo",
      "Byoung-Tak Zhang",
      "Jooyong Park"
    ],
    "publication_date": "2020-11-07T09:32:49Z",
    "arxiv_id": "http://arxiv.org/abs/2011.03733v1",
    "download_url": "http://arxiv.org/abs/2011.03733v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Matched Machine Learning: A Generalized Framework for Treatment Effect\n  Inference With Learned Metrics",
    "abstract": "We introduce Matched Machine Learning, a framework that combines the\nflexibility of machine learning black boxes with the interpretability of\nmatching, a longstanding tool in observational causal inference.\nInterpretability is paramount in many high-stakes application of causal\ninference. Current tools for nonparametric estimation of both average and\nindividualized treatment effects are black-boxes that do not allow for human\nauditing of estimates. Our framework uses machine learning to learn an optimal\nmetric for matching units and estimating outcomes, thus achieving the\nperformance of machine learning black-boxes, while being interpretable. Our\ngeneral framework encompasses several published works as special cases. We\nprovide asymptotic inference theory for our proposed framework, enabling users\nto construct approximate confidence intervals around estimates of both\nindividualized and average treatment effects. We show empirically that\ninstances of Matched Machine Learning perform on par with black-box machine\nlearning methods and better than existing matching methods for similar\nproblems. Finally, in our application we show how Matched Machine Learning can\nbe used to perform causal inference even when covariate data are highly\ncomplex: we study an image dataset, and produce high quality matches and\nestimates of treatment effects.",
    "authors": [
      "Marco Morucci",
      "Cynthia Rudin",
      "Alexander Volfovsky"
    ],
    "publication_date": "2023-04-03T19:32:30Z",
    "arxiv_id": "http://arxiv.org/abs/2304.01316v1",
    "download_url": "http://arxiv.org/abs/2304.01316v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Quantum-enhanced machine learning",
    "abstract": "The emerging field of quantum machine learning has the potential to\nsubstantially aid in the problems and scope of artificial intelligence. This is\nonly enhanced by recent successes in the field of classical machine learning.\nIn this work we propose an approach for the systematic treatment of machine\nlearning, from the perspective of quantum information. Our approach is general\nand covers all three main branches of machine learning: supervised,\nunsupervised and reinforcement learning. While quantum improvements in\nsupervised and unsupervised learning have been reported, reinforcement learning\nhas received much less attention. Within our approach, we tackle the problem of\nquantum enhancements in reinforcement learning as well, and propose a\nsystematic scheme for providing improvements. As an example, we show that\nquadratic improvements in learning efficiency, and exponential improvements in\nperformance over limited time periods, can be obtained for a broad class of\nlearning problems.",
    "authors": [
      "Vedran Dunjko",
      "Jacob M. Taylor",
      "Hans J. Briegel"
    ],
    "publication_date": "2016-10-26T09:35:11Z",
    "arxiv_id": "http://arxiv.org/abs/1610.08251v1",
    "download_url": "http://arxiv.org/abs/1610.08251v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Generalization Guarantees for a Binary Classification Framework for\n  Two-Stage Multiple Kernel Learning",
    "abstract": "We present generalization bounds for the TS-MKL framework for two stage\nmultiple kernel learning. We also present bounds for sparse kernel learning\nformulations within the TS-MKL framework.",
    "authors": [
      "Purushottam Kar"
    ],
    "publication_date": "2013-02-02T17:20:47Z",
    "arxiv_id": "http://arxiv.org/abs/1302.0406v1",
    "download_url": "http://arxiv.org/abs/1302.0406v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Beneficial and Harmful Explanatory Machine Learning",
    "abstract": "Given the recent successes of Deep Learning in AI there has been increased\ninterest in the role and need for explanations in machine learned theories. A\ndistinct notion in this context is that of Michie's definition of Ultra-Strong\nMachine Learning (USML). USML is demonstrated by a measurable increase in human\nperformance of a task following provision to the human of a symbolic machine\nlearned theory for task performance. A recent paper demonstrates the beneficial\neffect of a machine learned logic theory for a classification task, yet no\nexisting work to our knowledge has examined the potential harmfulness of\nmachine's involvement for human comprehension during learning. This paper\ninvestigates the explanatory effects of a machine learned theory in the context\nof simple two person games and proposes a framework for identifying the\nharmfulness of machine explanations based on the Cognitive Science literature.\nThe approach involves a cognitive window consisting of two quantifiable bounds\nand it is supported by empirical evidence collected from human trials. Our\nquantitative and qualitative results indicate that human learning aided by a\nsymbolic machine learned theory which satisfies a cognitive window has achieved\nsignificantly higher performance than human self learning. Results also\ndemonstrate that human learning aided by a symbolic machine learned theory that\nfails to satisfy this window leads to significantly worse performance than\nunaided human learning.",
    "authors": [
      "Lun Ai",
      "Stephen H. Muggleton",
      "Céline Hocquette",
      "Mark Gromowski",
      "Ute Schmid"
    ],
    "publication_date": "2020-09-09T19:14:38Z",
    "arxiv_id": "http://arxiv.org/abs/2009.06410v2",
    "download_url": "http://arxiv.org/abs/2009.06410v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Logistic Regression as Soft Perceptron Learning",
    "abstract": "We comment on the fact that gradient ascent for logistic regression has a\nconnection with the perceptron learning algorithm. Logistic learning is the\n\"soft\" variant of perceptron learning.",
    "authors": [
      "Raul Rojas"
    ],
    "publication_date": "2017-08-24T20:19:20Z",
    "arxiv_id": "http://arxiv.org/abs/1708.07826v1",
    "download_url": "http://arxiv.org/abs/1708.07826v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning proofs for the classification of nilpotent semigroups",
    "abstract": "Machine learning is applied to find proofs, with smaller or smallest numbers\nof nodes, for the classification of 4-nilpotent semigroups.",
    "authors": [
      "Carlos Simpson"
    ],
    "publication_date": "2021-06-06T03:03:04Z",
    "arxiv_id": "http://arxiv.org/abs/2106.03015v1",
    "download_url": "http://arxiv.org/abs/2106.03015v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python",
    "abstract": "Machine learning is a general-purpose technology holding promises for many\ninterdisciplinary research problems. However, significant barriers exist in\ncrossing disciplinary boundaries when most machine learning tools are developed\nin different areas separately. We present Pykale - a Python library for\nknowledge-aware machine learning on graphs, images, texts, and videos to enable\nand accelerate interdisciplinary research. We formulate new green machine\nlearning guidelines based on standard software engineering practices and\npropose a novel pipeline-based application programming interface (API). PyKale\nfocuses on leveraging knowledge from multiple sources for accurate and\ninterpretable prediction, thus supporting multimodal learning and transfer\nlearning (particularly domain adaptation) with latest deep learning and\ndimensionality reduction models. We build PyKale on PyTorch and leverage the\nrich PyTorch ecosystem. Our pipeline-based API design enforces standardization\nand minimalism, embracing green machine learning concepts via reducing\nrepetitions and redundancy, reusing existing resources, and recycling learning\nmodels across areas. We demonstrate its interdisciplinary nature via examples\nin bioinformatics, knowledge graph, image/video recognition, and medical\nimaging.",
    "authors": [
      "Haiping Lu",
      "Xianyuan Liu",
      "Robert Turner",
      "Peizhen Bai",
      "Raivo E Koot",
      "Shuo Zhou",
      "Mustafa Chasmai",
      "Lawrence Schobs"
    ],
    "publication_date": "2021-06-17T18:35:37Z",
    "arxiv_id": "http://arxiv.org/abs/2106.09756v1",
    "download_url": "http://arxiv.org/abs/2106.09756v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Is 'Unsupervised Learning' a Misconceived Term?",
    "abstract": "Is all of machine learning supervised to some degree? The field of machine\nlearning has traditionally been categorized pedagogically into\n$supervised~vs~unsupervised~learning$; where supervised learning has typically\nreferred to learning from labeled data, while unsupervised learning has\ntypically referred to learning from unlabeled data. In this paper, we assert\nthat all machine learning is in fact supervised to some degree, and that the\nscope of supervision is necessarily commensurate to the scope of learning\npotential. In particular, we argue that clustering algorithms such as k-means,\nand dimensionality reduction algorithms such as principal component analysis,\nvariational autoencoders, and deep belief networks are each internally\nsupervised by the data itself to learn their respective representations of its\nfeatures. Furthermore, these algorithms are not capable of external inference\nuntil their respective outputs (clusters, principal components, or\nrepresentation codes) have been identified and externally labeled in effect. As\nsuch, they do not suffice as examples of unsupervised learning. We propose that\nthe categorization `supervised vs unsupervised learning' be dispensed with, and\ninstead, learning algorithms be categorized as either\n$internally~or~externally~supervised$ (or both). We believe this change in\nperspective will yield new fundamental insights into the structure and\ncharacter of data and of learning algorithms.",
    "authors": [
      "Stephen G. Odaibo"
    ],
    "publication_date": "2019-04-05T20:05:46Z",
    "arxiv_id": "http://arxiv.org/abs/1904.03259v1",
    "download_url": "http://arxiv.org/abs/1904.03259v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Deep Learning and Its Applications to Machine Health Monitoring: A\n  Survey",
    "abstract": "Since 2006, deep learning (DL) has become a rapidly growing research\ndirection, redefining state-of-the-art performances in a wide range of areas\nsuch as object recognition, image segmentation, speech recognition and machine\ntranslation. In modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread deployment of\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\nprovides useful tools for processing and analyzing these big machinery data.\nThe main purpose of this paper is to review and summarize the emerging research\nwork of deep learning on machine health monitoring. After the brief\nintroduction of deep learning techniques, the applications of deep learning in\nmachine health monitoring systems are reviewed mainly from the following\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\nFinally, some new trends of DL-based machine health monitoring methods are\ndiscussed.",
    "authors": [
      "Rui Zhao",
      "Ruqiang Yan",
      "Zhenghua Chen",
      "Kezhi Mao",
      "Peng Wang",
      "Robert X. Gao"
    ],
    "publication_date": "2016-12-16T04:56:30Z",
    "arxiv_id": "http://arxiv.org/abs/1612.07640v1",
    "download_url": "http://arxiv.org/abs/1612.07640v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Diversity in Machine Learning",
    "abstract": "Machine learning methods have achieved good performance and been widely\napplied in various real-world applications. They can learn the model adaptively\nand be better fit for special requirements of different tasks. Generally, a\ngood machine learning system is composed of plentiful training data, a good\nmodel training process, and an accurate inference. Many factors can affect the\nperformance of the machine learning process, among which the diversity of the\nmachine learning process is an important one. The diversity can help each\nprocedure to guarantee a total good machine learning: diversity of the training\ndata ensures that the training data can provide more discriminative information\nfor the model, diversity of the learned model (diversity in parameters of each\nmodel or diversity among different base models) makes each parameter/model\ncapture unique or complement information and the diversity in inference can\nprovide multiple choices each of which corresponds to a specific plausible\nlocal optimal result. Even though the diversity plays an important role in\nmachine learning process, there is no systematical analysis of the\ndiversification in machine learning system. In this paper, we systematically\nsummarize the methods to make data diversification, model diversification, and\ninference diversification in the machine learning process, respectively. In\naddition, the typical applications where the diversity technology improved the\nmachine learning performance have been surveyed, including the remote sensing\nimaging tasks, machine translation, camera relocalization, image segmentation,\nobject detection, topic modeling, and others. Finally, we discuss some\nchallenges of the diversity technology in machine learning and point out some\ndirections in future work.",
    "authors": [
      "Zhiqiang Gong",
      "Ping Zhong",
      "Weidong Hu"
    ],
    "publication_date": "2018-07-04T08:25:17Z",
    "arxiv_id": "http://arxiv.org/abs/1807.01477v2",
    "download_url": "http://arxiv.org/abs/1807.01477v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Engineering problems in machine learning systems",
    "abstract": "Fatal accidents are a major issue hindering the wide acceptance of\nsafety-critical systems that employ machine learning and deep learning models,\nsuch as automated driving vehicles. In order to use machine learning in a\nsafety-critical system, it is necessary to demonstrate the safety and security\nof the system through engineering processes. However, thus far, no such widely\naccepted engineering concepts or frameworks have been established for these\nsystems. The key to using a machine learning model in a deductively engineered\nsystem is decomposing the data-driven training of machine learning models into\nrequirement, design, and verification, particularly for machine learning models\nused in safety-critical systems. Simultaneously, open problems and relevant\ntechnical fields are not organized in a manner that enables researchers to\nselect a theme and work on it. In this study, we identify, classify, and\nexplore the open problems in engineering (safety-critical) machine learning\nsystems --- that is, in terms of requirement, design, and verification of\nmachine learning models and systems --- as well as discuss related works and\nresearch directions, using automated driving vehicles as an example. Our\nresults show that machine learning models are characterized by a lack of\nrequirements specification, lack of design specification, lack of\ninterpretability, and lack of robustness. We also perform a gap analysis on a\nconventional system quality standard SQuARE with the characteristics of machine\nlearning models to study quality models for machine learning systems. We find\nthat a lack of requirements specification and lack of robustness have the\ngreatest impact on conventional quality models.",
    "authors": [
      "Hiroshi Kuwajima",
      "Hirotoshi Yasuoka",
      "Toshihiro Nakae"
    ],
    "publication_date": "2019-04-01T13:57:27Z",
    "arxiv_id": "http://arxiv.org/abs/1904.00001v2",
    "download_url": "http://arxiv.org/abs/1904.00001v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Automated Graph Machine Learning: Approaches, Libraries, Benchmarks and\n  Directions",
    "abstract": "Graph machine learning has been extensively studied in both academic and\nindustry. However, as the literature on graph learning booms with a vast number\nof emerging methods and techniques, it becomes increasingly difficult to\nmanually design the optimal machine learning algorithm for different\ngraph-related tasks. To tackle the challenge, automated graph machine learning,\nwhich aims at discovering the best hyper-parameter and neural architecture\nconfiguration for different graph tasks/data without manual design, is gaining\nan increasing number of attentions from the research community. In this paper,\nwe extensively discuss automated graph machine learning approaches, covering\nhyper-parameter optimization (HPO) and neural architecture search (NAS) for\ngraph machine learning. We briefly overview existing libraries designed for\neither graph machine learning or automated machine learning respectively, and\nfurther in depth introduce AutoGL, our dedicated and the world's first\nopen-source library for automated graph machine learning. Also, we describe a\ntailored benchmark that supports unified, reproducible, and efficient\nevaluations. Last but not least, we share our insights on future research\ndirections for automated graph machine learning. This paper is the first\nsystematic and comprehensive discussion of approaches, libraries as well as\ndirections for automated graph machine learning.",
    "authors": [
      "Xin Wang",
      "Ziwei Zhang",
      "Haoyang Li",
      "Wenwu Zhu"
    ],
    "publication_date": "2022-01-04T18:31:31Z",
    "arxiv_id": "http://arxiv.org/abs/2201.01288v2",
    "download_url": "http://arxiv.org/abs/2201.01288v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "On the impact of measure pre-conditionings on general parametric ML\n  models and transfer learning via domain adaptation",
    "abstract": "We study a new technique for understanding convergence of learning agents\nunder small modifications of data. We show that such convergence can be\nunderstood via an analogue of Fatou's lemma which yields gamma-convergence. We\nshow it's relevance and applications in general machine learning tasks and\ndomain adaptation transfer learning.",
    "authors": [
      "Joaquín Sánchez García"
    ],
    "publication_date": "2024-03-04T19:26:39Z",
    "arxiv_id": "http://arxiv.org/abs/2403.02432v1",
    "download_url": "http://arxiv.org/abs/2403.02432v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Semi-supervised Learning on Large Graphs: is Poisson Learning a\n  Game-Changer?",
    "abstract": "We explain Poisson learning on graph-based semi-supervised learning to see if\nit could avoid the problem of global information loss problem as Laplace-based\nlearning methods on large graphs. From our analysis, Poisson learning is simply\nLaplace regularization with thresholding, cannot overcome the problem.",
    "authors": [
      "Canh Hao Nguyen"
    ],
    "publication_date": "2022-02-28T08:30:24Z",
    "arxiv_id": "http://arxiv.org/abs/2202.13608v2",
    "download_url": "http://arxiv.org/abs/2202.13608v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and\n  Concept Formation in Deep Learning",
    "abstract": "Machine learning is usually defined in behaviourist terms, where external\nvalidation is the primary mechanism of learning. In this paper, I argue for a\nmore holistic interpretation in which finding more probable, efficient and\nabstract representations is as central to learning as performance. In other\nwords, machine learning should be extended with strategies to reason over its\nown learning process, leading to so-called meta-cognitive machine learning. As\nsuch, the de facto definition of machine learning should be reformulated in\nthese intrinsically multi-objective terms, taking into account not only the\ntask performance but also internal learning objectives. To this end, we suggest\na \"model entropy function\" to be defined that quantifies the efficiency of the\ninternal learning processes. It is conjured that the minimization of this model\nentropy leads to concept formation. Besides philosophical aspects, some initial\nillustrations are included to support the claims.",
    "authors": [
      "Johan Loeckx"
    ],
    "publication_date": "2017-11-04T12:54:35Z",
    "arxiv_id": "http://arxiv.org/abs/1711.01431v1",
    "download_url": "http://arxiv.org/abs/1711.01431v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Low-Shot Classification: A Comparison of Classical and Deep Transfer\n  Machine Learning Approaches",
    "abstract": "Despite the recent success of deep transfer learning approaches in NLP, there\nis a lack of quantitative studies demonstrating the gains these models offer in\nlow-shot text classification tasks over existing paradigms. Deep transfer\nlearning approaches such as BERT and ULMFiT demonstrate that they can beat\nstate-of-the-art results on larger datasets, however when one has only 100-1000\nlabelled examples per class, the choice of approach is less clear, with\nclassical machine learning and deep transfer learning representing valid\noptions. This paper compares the current best transfer learning approach with\ntop classical machine learning approaches on a trinary sentiment classification\ntask to assess the best paradigm. We find that BERT, representing the best of\ndeep transfer learning, is the best performing approach, outperforming top\nclassical machine learning algorithms by 9.7% on average when trained with 100\nexamples per class, narrowing to 1.8% at 1000 labels per class. We also show\nthe robustness of deep transfer learning in moving across domains, where the\nmaximum loss in accuracy is only 0.7% in similar domain tasks and 3.2% cross\ndomain, compared to classical machine learning which loses up to 20.6%.",
    "authors": [
      "Peter Usherwood",
      "Steven Smit"
    ],
    "publication_date": "2019-07-17T14:23:15Z",
    "arxiv_id": "http://arxiv.org/abs/1907.07543v1",
    "download_url": "http://arxiv.org/abs/1907.07543v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine Learning with a Reject Option: A survey",
    "abstract": "Machine learning models always make a prediction, even when it is likely to\nbe inaccurate. This behavior should be avoided in many decision support\napplications, where mistakes can have severe consequences. Albeit already\nstudied in 1970, machine learning with rejection recently gained interest. This\nmachine learning subfield enables machine learning models to abstain from\nmaking a prediction when likely to make a mistake.\n  This survey aims to provide an overview on machine learning with rejection.\nWe introduce the conditions leading to two types of rejection, ambiguity and\nnovelty rejection, which we carefully formalize. Moreover, we review and\ncategorize strategies to evaluate a model's predictive and rejective quality.\nAdditionally, we define the existing architectures for models with rejection\nand describe the standard techniques for learning such models. Finally, we\nprovide examples of relevant application domains and show how machine learning\nwith rejection relates to other machine learning research areas.",
    "authors": [
      "Kilian Hendrickx",
      "Lorenzo Perini",
      "Dries Van der Plas",
      "Wannes Meert",
      "Jesse Davis"
    ],
    "publication_date": "2021-07-23T14:43:56Z",
    "arxiv_id": "http://arxiv.org/abs/2107.11277v3",
    "download_url": "http://arxiv.org/abs/2107.11277v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "PSL is Dead. Long Live PSL",
    "abstract": "Property Specification Language (PSL) is a form of temporal logic that has\nbeen mainly used in discrete domains (e.g. formal hardware verification). In\nthis paper, we show that by merging machine learning techniques with PSL\nmonitors, we can extend PSL to work on continuous domains. We apply this\ntechnique in machine learning-based anomaly detection to analyze scenarios of\nreal-time streaming events from continuous variables in order to detect\nabnormal behaviors of a system. By using machine learning with formal models,\nwe leverage the strengths of both machine learning methods and formal semantics\nof time. On one hand, machine learning techniques can produce distributions on\ncontinuous variables, where abnormalities can be captured as deviations from\nthe distributions. On the other hand, formal methods can characterize discrete\ntemporal behaviors and relations that cannot be easily learned by machine\nlearning techniques. Interestingly, the anomalies detected by machine learning\nand the underlying time representation used are discrete events. We implemented\na temporal monitoring package (TEF) that operates in conjunction with normal\ndata science packages for anomaly detection machine learning systems, and we\nshow that TEF can be used to perform accurate interpretation of temporal\ncorrelation between events.",
    "authors": [
      "Kevin Smith",
      "Hai Lin",
      "Praveen Tiwari",
      "Marjorie Sayer",
      "Claudionor Coelho"
    ],
    "publication_date": "2022-05-27T17:55:54Z",
    "arxiv_id": "http://arxiv.org/abs/2205.14136v1",
    "download_url": "http://arxiv.org/abs/2205.14136v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Declarative Query Language for Scientific Machine Learning",
    "abstract": "The popularity of data science as a discipline and its importance in the\nemerging economy and industrial progress dictate that machine learning be\ndemocratized for the masses. This also means that the current practice of\nworkforce training using machine learning tools, which requires low-level\nstatistical and algorithmic details, is a barrier that needs to be addressed.\nSimilar to data management languages such as SQL, machine learning needs to be\npracticed at a conceptual level to help make it a staple tool for general\nusers. In particular, the technical sophistication demanded by existing machine\nlearning frameworks is prohibitive for many scientists who are not\ncomputationally savvy or well versed in machine learning techniques. The\nlearning curve to use the needed machine learning tools is also too high for\nthem to take advantage of these powerful platforms to rapidly advance science.\nIn this paper, we introduce a new declarative machine learning query language,\ncalled {\\em MQL}, for naive users. We discuss its merit and possible ways of\nimplementing it over a traditional relational database system. We discuss two\nmaterials science experiments implemented using MQL on a materials science\nworkflow system called MatFlow.",
    "authors": [
      "Hasan M Jamil"
    ],
    "publication_date": "2024-05-25T09:58:33Z",
    "arxiv_id": "http://arxiv.org/abs/2405.16159v1",
    "download_url": "http://arxiv.org/abs/2405.16159v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Transfer Learning for Voice Activity Detection: A Denoising Deep Neural\n  Network Perspective",
    "abstract": "Mismatching problem between the source and target noisy corpora severely\nhinder the practical use of the machine-learning-based voice activity detection\n(VAD). In this paper, we try to address this problem in the transfer learning\nprospective. Transfer learning tries to find a common learning machine or a\ncommon feature subspace that is shared by both the source corpus and the target\ncorpus. The denoising deep neural network is used as the learning machine.\nThree transfer techniques, which aim to learn common feature representations,\nare used for analysis. Experimental results demonstrate the effectiveness of\nthe transfer learning schemes on the mismatch problem.",
    "authors": [
      "Xiao-Lei Zhang",
      "Ji Wu"
    ],
    "publication_date": "2013-03-08T20:46:27Z",
    "arxiv_id": "http://arxiv.org/abs/1303.2104v1",
    "download_url": "http://arxiv.org/abs/1303.2104v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learnable: Theory vs Applications",
    "abstract": "Two different views on machine learning problem: Applied learning (machine\nlearning with business applications) and Agnostic PAC learning are formalized\nand compared here. I show that, under some conditions, the theory of PAC\nLearnable provides a way to solve the Applied learning problem. However, the\ntheory requires to have the training sets so large, that it would make the\nlearning practically useless. I suggest shedding some theoretical\nmisconceptions about learning to make the theory more aligned with the needs\nand experience of practitioners.",
    "authors": [
      "Marina Sapir"
    ],
    "publication_date": "2018-07-27T15:21:43Z",
    "arxiv_id": "http://arxiv.org/abs/1807.10681v1",
    "download_url": "http://arxiv.org/abs/1807.10681v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Minimal Achievable Sufficient Statistic Learning",
    "abstract": "We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a\ntraining method for machine learning models that attempts to produce minimal\nsufficient statistics with respect to a class of functions (e.g. deep networks)\nbeing optimized over. In deriving MASS Learning, we also introduce Conserved\nDifferential Information (CDI), an information-theoretic quantity that - unlike\nstandard mutual information - can be usefully applied to\ndeterministically-dependent continuous random variables like the input and\noutput of a deep network. In a series of experiments, we show that deep\nnetworks trained with MASS Learning achieve competitive performance on\nsupervised learning and uncertainty quantification benchmarks.",
    "authors": [
      "Milan Cvitkovic",
      "Günther Koliander"
    ],
    "publication_date": "2019-05-19T22:46:23Z",
    "arxiv_id": "http://arxiv.org/abs/1905.07822v2",
    "download_url": "http://arxiv.org/abs/1905.07822v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Sustainable Federated Learning",
    "abstract": "Potential environmental impact of machine learning by large-scale wireless\nnetworks is a major challenge for the sustainability of future smart\necosystems. In this paper, we introduce sustainable machine learning in\nfederated learning settings, using rechargeable devices that can collect energy\nfrom the ambient environment. We propose a practical federated learning\nframework that leverages intermittent energy arrivals for training, with\nprovable convergence guarantees. Our framework can be applied to a wide range\nof machine learning settings in networked environments, including distributed\nand federated learning in wireless and edge networks. Our experiments\ndemonstrate that the proposed framework can provide significant performance\nimprovement over the benchmark energy-agnostic federated learning settings.",
    "authors": [
      "Basak Guler",
      "Aylin Yener"
    ],
    "publication_date": "2021-02-22T18:58:47Z",
    "arxiv_id": "http://arxiv.org/abs/2102.11274v1",
    "download_url": "http://arxiv.org/abs/2102.11274v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "The ART of Transfer Learning: An Adaptive and Robust Pipeline",
    "abstract": "Transfer learning is an essential tool for improving the performance of\nprimary tasks by leveraging information from auxiliary data resources. In this\nwork, we propose Adaptive Robust Transfer Learning (ART), a flexible pipeline\nof performing transfer learning with generic machine learning algorithms. We\nestablish the non-asymptotic learning theory of ART, providing a provable\ntheoretical guarantee for achieving adaptive transfer while preventing negative\ntransfer. Additionally, we introduce an ART-integrated-aggregating machine that\nproduces a single final model when multiple candidate algorithms are\nconsidered. We demonstrate the promising performance of ART through extensive\nempirical studies on regression, classification, and sparse learning. We\nfurther present a real-data analysis for a mortality study.",
    "authors": [
      "Boxiang Wang",
      "Yunan Wu",
      "Chenglong Ye"
    ],
    "publication_date": "2023-04-30T16:36:57Z",
    "arxiv_id": "http://arxiv.org/abs/2305.00520v1",
    "download_url": "http://arxiv.org/abs/2305.00520v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Comparison of First-order Algorithms for Machine Learning",
    "abstract": "Using an optimization algorithm to solve a machine learning problem is one of\nmainstreams in the field of science. In this work, we demonstrate a\ncomprehensive comparison of some state-of-the-art first-order optimization\nalgorithms for convex optimization problems in machine learning. We concentrate\non several smooth and non-smooth machine learning problems with a loss function\nplus a regularizer. The overall experimental results show the superiority of\nprimal-dual algorithms in solving a machine learning problem from the\nperspectives of the ease to construct, running time and accuracy.",
    "authors": [
      "Yu Wei",
      "Pock Thomas"
    ],
    "publication_date": "2014-04-26T19:24:24Z",
    "arxiv_id": "http://arxiv.org/abs/1404.6674v1",
    "download_url": "http://arxiv.org/abs/1404.6674v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Meaningful Models: Utilizing Conceptual Structure to Improve Machine\n  Learning Interpretability",
    "abstract": "The last decade has seen huge progress in the development of advanced machine\nlearning models; however, those models are powerless unless human users can\ninterpret them. Here we show how the mind's construction of concepts and\nmeaning can be used to create more interpretable machine learning models. By\nproposing a novel method of classifying concepts, in terms of 'form' and\n'function', we elucidate the nature of meaning and offer proposals to improve\nmodel understandability. As machine learning begins to permeate daily life,\ninterpretable models may serve as a bridge between domain-expert authors and\nnon-expert users.",
    "authors": [
      "Nick Condry"
    ],
    "publication_date": "2016-07-01T15:07:52Z",
    "arxiv_id": "http://arxiv.org/abs/1607.00279v1",
    "download_url": "http://arxiv.org/abs/1607.00279v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Theoretical Impediments to Machine Learning With Seven Sparks from the\n  Causal Revolution",
    "abstract": "Current machine learning systems operate, almost exclusively, in a\nstatistical, or model-free mode, which entails severe theoretical limits on\ntheir power and performance. Such systems cannot reason about interventions and\nretrospection and, therefore, cannot serve as the basis for strong AI. To\nachieve human level intelligence, learning machines need the guidance of a\nmodel of reality, similar to the ones used in causal inference tasks. To\ndemonstrate the essential role of such models, I will present a summary of\nseven tasks which are beyond reach of current machine learning systems and\nwhich have been accomplished using the tools of causal modeling.",
    "authors": [
      "Judea Pearl"
    ],
    "publication_date": "2018-01-11T23:37:48Z",
    "arxiv_id": "http://arxiv.org/abs/1801.04016v1",
    "download_url": "http://arxiv.org/abs/1801.04016v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Optimizing for Generalization in Machine Learning with Cross-Validation\n  Gradients",
    "abstract": "Cross-validation is the workhorse of modern applied statistics and machine\nlearning, as it provides a principled framework for selecting the model that\nmaximizes generalization performance. In this paper, we show that the\ncross-validation risk is differentiable with respect to the hyperparameters and\ntraining data for many common machine learning algorithms, including logistic\nregression, elastic-net regression, and support vector machines. Leveraging\nthis property of differentiability, we propose a cross-validation gradient\nmethod (CVGM) for hyperparameter optimization. Our method enables efficient\noptimization in high-dimensional hyperparameter spaces of the cross-validation\nrisk, the best surrogate of the true generalization ability of our learning\nalgorithm.",
    "authors": [
      "Shane Barratt",
      "Rishi Sharma"
    ],
    "publication_date": "2018-05-18T07:04:37Z",
    "arxiv_id": "http://arxiv.org/abs/1805.07072v1",
    "download_url": "http://arxiv.org/abs/1805.07072v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Algebraic Expression of Subjective Spatial and Temporal Patterns",
    "abstract": "Universal learning machine is a theory trying to study machine learning from\nmathematical point of view. The outside world is reflected inside an universal\nlearning machine according to pattern of incoming data. This is subjective\npattern of learning machine. In [2,4], we discussed subjective spatial pattern,\nand established a powerful tool -- X-form, which is an algebraic expression for\nsubjective spatial pattern. However, as the initial stage of study, there we\nonly discussed spatial pattern. Here, we will discuss spatial and temporal\npatterns, and algebraic expression for them.",
    "authors": [
      "Chuyu Xiong"
    ],
    "publication_date": "2018-05-26T20:54:13Z",
    "arxiv_id": "http://arxiv.org/abs/1805.11959v2",
    "download_url": "http://arxiv.org/abs/1805.11959v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine Learning in Official Statistics",
    "abstract": "In the first half of 2018, the Federal Statistical Office of Germany\n(Destatis) carried out a \"Proof of Concept Machine Learning\" as part of its\nDigital Agenda. A major component of this was surveys on the use of machine\nlearning methods in official statistics, which were conducted at selected\nnational and international statistical institutions and among the divisions of\nDestatis. It was of particular interest to find out in which statistical areas\nand for which tasks machine learning is used and which methods are applied.\nThis paper is intended to make the results of the surveys publicly accessible.",
    "authors": [
      "Martin Beck",
      "Florian Dumpert",
      "Joerg Feuerhake"
    ],
    "publication_date": "2018-12-13T11:02:01Z",
    "arxiv_id": "http://arxiv.org/abs/1812.10422v1",
    "download_url": "http://arxiv.org/abs/1812.10422v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Seven Myths in Machine Learning Research",
    "abstract": "We present seven myths commonly believed to be true in machine learning\nresearch, circa Feb 2019. This is an archival copy of the blog post at\nhttps://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/\n  Myth 1: TensorFlow is a Tensor manipulation library\n  Myth 2: Image datasets are representative of real images found in the wild\n  Myth 3: Machine Learning researchers do not use the test set for validation\n  Myth 4: Every datapoint is used in training a neural network\n  Myth 5: We need (batch) normalization to train very deep residual networks\n  Myth 6: Attention $>$ Convolution\n  Myth 7: Saliency maps are robust ways to interpret neural networks",
    "authors": [
      "Oscar Chang",
      "Hod Lipson"
    ],
    "publication_date": "2019-02-18T20:38:14Z",
    "arxiv_id": "http://arxiv.org/abs/1902.06789v2",
    "download_url": "http://arxiv.org/abs/1902.06789v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Optimal Algorithms for Ski Rental with Soft Machine-Learned Predictions",
    "abstract": "We consider a variant of the classic Ski Rental online algorithm with\napplications to machine learning. In our variant, we allow the skier access to\na black-box machine-learning algorithm that provides an estimate of the\nprobability that there will be at most a threshold number of ski-days. We\nderive a class of optimal randomized algorithms to determine the strategy that\nminimizes the worst-case expected competitive ratio for the skier given a\nprediction from the machine learning algorithm,and analyze the performance and\nrobustness of these algorithms.",
    "authors": [
      "Rohan Kodialam"
    ],
    "publication_date": "2019-02-28T22:34:46Z",
    "arxiv_id": "http://arxiv.org/abs/1903.00092v2",
    "download_url": "http://arxiv.org/abs/1903.00092v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Towards Quantification of Bias in Machine Learning for Healthcare: A\n  Case Study of Renal Failure Prediction",
    "abstract": "As machine learning (ML) models, trained on real-world datasets, become\ncommon practice, it is critical to measure and quantify their potential biases.\nIn this paper, we focus on renal failure and compare a commonly used\ntraditional risk score, Tangri, with a more powerful machine learning model,\nwhich has access to a larger variable set and trained on 1.6 million patients'\nEHR data. We will compare and discuss the generalization and applicability of\nthese two models, in an attempt to quantify biases of status quo clinical\npractice, compared to ML-driven models.",
    "authors": [
      "Josie Williams",
      "Narges Razavian"
    ],
    "publication_date": "2019-11-18T15:04:31Z",
    "arxiv_id": "http://arxiv.org/abs/1911.07679v1",
    "download_url": "http://arxiv.org/abs/1911.07679v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "On the computation of counterfactual explanations -- A survey",
    "abstract": "Due to the increasing use of machine learning in practice it becomes more and\nmore important to be able to explain the prediction and behavior of machine\nlearning models. An instance of explanations are counterfactual explanations\nwhich provide an intuitive and useful explanations of machine learning models.\nIn this survey we review model-specific methods for efficiently computing\ncounterfactual explanations of many different machine learning models and\npropose methods for models that have not been considered in literature so far.",
    "authors": [
      "André Artelt",
      "Barbara Hammer"
    ],
    "publication_date": "2019-11-15T08:14:26Z",
    "arxiv_id": "http://arxiv.org/abs/1911.07749v1",
    "download_url": "http://arxiv.org/abs/1911.07749v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Computer Systems Have 99 Problems, Let's Not Make Machine Learning\n  Another One",
    "abstract": "Machine learning techniques are finding many applications in computer\nsystems, including many tasks that require decision making: network\noptimization, quality of service assurance, and security. We believe machine\nlearning systems are here to stay, and to materialize on their potential we\nadvocate a fresh look at various key issues that need further attention,\nincluding security as a requirement and system complexity, and how machine\nlearning systems affect them. We also discuss reproducibility as a key\nrequirement for sustainable machine learning systems, and leads to pursuing it.",
    "authors": [
      "David Mohaisen",
      "Songqing Chen"
    ],
    "publication_date": "2019-11-28T08:43:01Z",
    "arxiv_id": "http://arxiv.org/abs/1911.12593v1",
    "download_url": "http://arxiv.org/abs/1911.12593v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "When Machine Learning Meets Multiscale Modeling in Chemical Reactions",
    "abstract": "Due to the intrinsic complexity and nonlinearity of chemical reactions,\ndirect applications of traditional machine learning algorithms may face with\nmany difficulties. In this study, through two concrete examples with biological\nbackground, we illustrate how the key ideas of multiscale modeling can help to\nreduce the computational cost of machine learning a lot, as well as how machine\nlearning algorithms perform model reduction automatically in a time-scale\nseparated system. Our study highlights the necessity and effectiveness of an\nintegration of machine learning algorithms and multiscale modeling during the\nstudy of chemical reactions.",
    "authors": [
      "Wuyue Yang",
      "Liangrong Peng",
      "Yi Zhu",
      "Liu Hong"
    ],
    "publication_date": "2020-06-01T03:56:47Z",
    "arxiv_id": "http://arxiv.org/abs/2006.00700v1",
    "download_url": "http://arxiv.org/abs/2006.00700v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Distributed Double Machine Learning with a Serverless Architecture",
    "abstract": "This paper explores serverless cloud computing for double machine learning.\nBeing based on repeated cross-fitting, double machine learning is particularly\nwell suited to exploit the high level of parallelism achievable with serverless\ncomputing. It allows to get fast on-demand estimations without additional cloud\nmaintenance effort. We provide a prototype Python implementation\n\\texttt{DoubleML-Serverless} for the estimation of double machine learning\nmodels with the serverless computing platform AWS Lambda and demonstrate its\nutility with a case study analyzing estimation times and costs.",
    "authors": [
      "Malte S. Kurz"
    ],
    "publication_date": "2021-01-11T16:58:30Z",
    "arxiv_id": "http://arxiv.org/abs/2101.04025v2",
    "download_url": "http://arxiv.org/abs/2101.04025v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Categories of Differentiable Polynomial Circuits for Machine Learning",
    "abstract": "Reverse derivative categories (RDCs) have recently been shown to be a\nsuitable semantic framework for studying machine learning algorithms. Whereas\nemphasis has been put on training methodologies, less attention has been\ndevoted to particular \\emph{model classes}: the concrete categories whose\nmorphisms represent machine learning models. In this paper we study\npresentations by generators and equations of classes of RDCs. In particular, we\npropose \\emph{polynomial circuits} as a suitable machine learning model. We\ngive an axiomatisation for these circuits and prove a functional completeness\nresult. Finally, we discuss the use of polynomial circuits over specific\nsemirings to perform machine learning with discrete values.",
    "authors": [
      "Paul Wilson",
      "Fabio Zanasi"
    ],
    "publication_date": "2022-03-12T13:03:30Z",
    "arxiv_id": "http://arxiv.org/abs/2203.06430v2",
    "download_url": "http://arxiv.org/abs/2203.06430v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "When Physics Meets Machine Learning: A Survey of Physics-Informed\n  Machine Learning",
    "abstract": "Physics-informed machine learning (PIML), referring to the combination of\nprior knowledge of physics, which is the high level abstraction of natural\nphenomenons and human behaviours in the long history, with data-driven machine\nlearning models, has emerged as an effective way to mitigate the shortage of\ntraining data, to increase models' generalizability and to ensure the physical\nplausibility of results. In this paper, we survey an abundant number of recent\nworks in PIML and summarize them from three aspects: (1) motivations of PIML,\n(2) physics knowledge in PIML, (3) methods of physics knowledge integration in\nPIML. We also discuss current challenges and corresponding research\nopportunities in PIML.",
    "authors": [
      "Chuizheng Meng",
      "Sungyong Seo",
      "Defu Cao",
      "Sam Griesemer",
      "Yan Liu"
    ],
    "publication_date": "2022-03-31T04:58:27Z",
    "arxiv_id": "http://arxiv.org/abs/2203.16797v1",
    "download_url": "http://arxiv.org/abs/2203.16797v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "pystacked: Stacking generalization and machine learning in Stata",
    "abstract": "pystacked implements stacked generalization (Wolpert, 1992) for regression\nand binary classification via Python's scikit-learn. Stacking combines multiple\nsupervised machine learners -- the \"base\" or \"level-0\" learners -- into a\nsingle learner. The currently supported base learners include regularized\nregression, random forest, gradient boosted trees, support vector machines, and\nfeed-forward neural nets (multi-layer perceptron). pystacked can also be used\nwith as a `regular' machine learning program to fit a single base learner and,\nthus, provides an easy-to-use API for scikit-learn's machine learning\nalgorithms.",
    "authors": [
      "Achim Ahrens",
      "Christian B. Hansen",
      "Mark E. Schaffer"
    ],
    "publication_date": "2022-08-23T12:03:04Z",
    "arxiv_id": "http://arxiv.org/abs/2208.10896v2",
    "download_url": "http://arxiv.org/abs/2208.10896v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Efficient Private Machine Learning by Differentiable Random\n  Transformations",
    "abstract": "With the increasing demands for privacy protection, many privacy-preserving\nmachine learning systems were proposed in recent years. However, most of them\ncannot be put into production due to their slow training and inference speed\ncaused by the heavy cost of homomorphic encryption and secure multiparty\ncomputation(MPC) methods. To circumvent this, I proposed a privacy definition\nwhich is suitable for large amount of data in machine learning tasks. Based on\nthat, I showed that random transformations like linear transformation and\nrandom permutation can well protect privacy. Merging random transformations and\narithmetic sharing together, I designed a framework for private machine\nlearning with high efficiency and low computation cost.",
    "authors": [
      "Fei Zheng"
    ],
    "publication_date": "2020-08-18T06:17:31Z",
    "arxiv_id": "http://arxiv.org/abs/2008.07758v1",
    "download_url": "http://arxiv.org/abs/2008.07758v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Addressing Privacy Threats from Machine Learning",
    "abstract": "Every year at NeurIPS, machine learning researchers gather and discuss\nexciting applications of machine learning in areas such as public health,\ndisaster response, climate change, education, and more. However, many of these\nsame researchers are expressing growing concern about applications of machine\nlearning for surveillance (Nanayakkara et al., 2021). This paper presents a\nbrief overview of strategies for resisting these surveillance technologies and\ncalls for greater collaboration between machine learning and human-computer\ninteraction researchers to address the threats that these technologies pose.",
    "authors": [
      "Mary Anne Smart"
    ],
    "publication_date": "2021-10-25T03:40:25Z",
    "arxiv_id": "http://arxiv.org/abs/2111.04439v1",
    "download_url": "http://arxiv.org/abs/2111.04439v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Nine tips for ecologists using machine learning",
    "abstract": "Due to their high predictive performance and flexibility, machine learning\nmodels are an appropriate and efficient tool for ecologists. However,\nimplementing a machine learning model is not yet a trivial task and may seem\nintimidating to ecologists with no previous experience in this area. Here we\nprovide a series of tips to help ecologists in implementing machine learning\nmodels. We focus on classification problems as many ecological studies aim to\nassign data into predefined classes such as ecological states or biological\nentities. Each of the nine tips identifies a common error, trap or challenge in\ndeveloping machine learning models and provides recommendations to facilitate\ntheir use in ecological studies.",
    "authors": [
      "Marine Desprez",
      "Vincent Miele",
      "Olivier Gimenez"
    ],
    "publication_date": "2023-05-17T15:41:08Z",
    "arxiv_id": "http://arxiv.org/abs/2305.10472v2",
    "download_url": "http://arxiv.org/abs/2305.10472v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Comparison of Machine Learning Methods for Data with High-Cardinality\n  Categorical Variables",
    "abstract": "High-cardinality categorical variables are variables for which the number of\ndifferent levels is large relative to the sample size of a data set, or in\nother words, there are few data points per level. Machine learning methods can\nhave difficulties with high-cardinality variables. In this article, we\nempirically compare several versions of two of the most successful machine\nlearning methods, tree-boosting and deep neural networks, and linear mixed\neffects models using multiple tabular data sets with high-cardinality\ncategorical variables. We find that, first, machine learning models with random\neffects have higher prediction accuracy than their classical counterparts\nwithout random effects, and, second, tree-boosting with random effects\noutperforms deep neural networks with random effects.",
    "authors": [
      "Fabio Sigrist"
    ],
    "publication_date": "2023-07-05T07:26:27Z",
    "arxiv_id": "http://arxiv.org/abs/2307.02071v1",
    "download_url": "http://arxiv.org/abs/2307.02071v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine learning for accuracy in density functional approximations",
    "abstract": "Machine learning techniques have found their way into computational chemistry\nas indispensable tools to accelerate atomistic simulations and materials\ndesign. In addition, machine learning approaches hold the potential to boost\nthe predictive power of computationally efficient electronic structure methods,\nsuch as density functional theory, to chemical accuracy and to correct for\nfundamental errors in density functional approaches. Here, recent progress in\napplying machine learning to improve the accuracy of density functional and\nrelated approximations is reviewed. Promises and challenges in devising machine\nlearning models transferable between different chemistries and materials\nclasses are discussed with the help of examples applying promising models to\nsystems far outside their training sets.",
    "authors": [
      "Johannes Voss"
    ],
    "publication_date": "2023-11-01T00:02:09Z",
    "arxiv_id": "http://arxiv.org/abs/2311.00196v2",
    "download_url": "http://arxiv.org/abs/2311.00196v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  }
]