[
  {
    "title": "Implications of Computer Vision Driven Assistive Technologies Towards\n  Individuals with Visual Impairment",
    "abstract": "Computer vision based technology is becoming ubiquitous in society. One\napplication area that has seen an increase in computer vision is assistive\ntechnologies, specifically for those with visual impairment. Research has shown\nthe ability of computer vision models to achieve tasks such provide scene\ncaptions, detect objects and recognize faces. Although assisting individuals\nwith visual impairment with these tasks increases their independence and\nautonomy, concerns over bias, privacy and potential usefulness arise. This\npaper addresses the positive and negative implications computer vision based\nassistive technologies have on individuals with visual impairment, as well as\nconsiderations for computer vision researchers and developers in order to\nmitigate the amount of negative implications.",
    "authors": [
      "Linda Wang",
      "Alexander Wong"
    ],
    "publication_date": "2019-05-20T02:00:56Z",
    "arxiv_id": "http://arxiv.org/abs/1905.07844v1",
    "download_url": "http://arxiv.org/abs/1905.07844v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Second Croatian Computer Vision Workshop (CCVW 2013)",
    "abstract": "Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013,\nhttp://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb,\nCroatia. Workshop was organized by the Center of Excellence for Computer Vision\nof the University of Zagreb.",
    "authors": [
      "Sven Lončarić",
      "Siniša Šegvić"
    ],
    "publication_date": "2013-10-01T14:26:29Z",
    "arxiv_id": "http://arxiv.org/abs/1310.0319v3",
    "download_url": "http://arxiv.org/abs/1310.0319v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Multiband NFC for High-Throughput Wireless Computer Vision Sensor\n  Network",
    "abstract": "Vision sensors lie in the heart of computer vision. In many computer vision\napplications, such as AR/VR, non-contacting near-field communication (NFC) with\nhigh throughput is required to transfer information to algorithms. In this\nwork, we proposed a novel NFC system which utilizes multiple frequency bands to\nachieve high throughput.",
    "authors": [
      "F. Li",
      "J. Du"
    ],
    "publication_date": "2017-05-28T06:43:29Z",
    "arxiv_id": "http://arxiv.org/abs/1707.03720v1",
    "download_url": "http://arxiv.org/abs/1707.03720v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Deep Learning vs. Traditional Computer Vision",
    "abstract": "Deep Learning has pushed the limits of what was possible in the domain of\nDigital Image Processing. However, that is not to say that the traditional\ncomputer vision techniques which had been undergoing progressive development in\nyears prior to the rise of DL have become obsolete. This paper will analyse the\nbenefits and drawbacks of each approach. The aim of this paper is to promote a\ndiscussion on whether knowledge of classical computer vision techniques should\nbe maintained. The paper will also explore how the two sides of computer vision\ncan be combined. Several recent hybrid methodologies are reviewed which have\ndemonstrated the ability to improve computer vision performance and to tackle\nproblems not suited to Deep Learning. For example, combining traditional\ncomputer vision techniques with Deep Learning has been popular in emerging\ndomains such as Panoramic Vision and 3D vision for which Deep Learning models\nhave not yet been fully optimised",
    "authors": [
      "Niall O' Mahony",
      "Sean Campbell",
      "Anderson Carvalho",
      "Suman Harapanahalli",
      "Gustavo Velasco-Hernandez",
      "Lenka Krpalkova",
      "Daniel Riordan",
      "Joseph Walsh"
    ],
    "publication_date": "2019-10-30T12:25:10Z",
    "arxiv_id": "http://arxiv.org/abs/1910.13796v1",
    "download_url": "http://arxiv.org/abs/1910.13796v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Enhancing camera surveillance using computer vision: a research note",
    "abstract": "$\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has\nout-paced the ability of humans to monitor them effectively. Computer vision is\na possible solution. An ongoing research project on the application of computer\nvision within a municipal police department is described. The paper aims to\ndiscuss these issues.\n  $\\mathbf{Design/methodology/approach}$ - Following the demystification of\ncomputer vision technology, its potential for police agencies is developed\nwithin a focus on computer vision as a solution for two common surveillance\ncamera tasks (live monitoring of multiple surveillance cameras and summarizing\narchived video files). Three unaddressed research questions (can specialized\ncomputer vision applications for law enforcement be developed at this time, how\nwill computer vision be utilized within existing public safety camera\nmonitoring rooms, and what are the system-wide impacts of a computer vision\ncapability on local criminal justice systems) are considered.\n  $\\mathbf{Findings}$ - Despite computer vision becoming accessible to law\nenforcement agencies the impact of computer vision has not been discussed or\nadequately researched. There is little knowledge of computer vision or its\npotential in the field.\n  $\\mathbf{Originality/value}$ - This paper introduces and discusses computer\nvision from a law enforcement perspective and will be valuable to police\npersonnel tasked with monitoring large camera networks and considering computer\nvision as a system upgrade.",
    "authors": [
      "Haroon Idrees",
      "Mubarak Shah",
      "Ray Surette"
    ],
    "publication_date": "2018-08-12T20:01:37Z",
    "arxiv_id": "http://arxiv.org/abs/1808.03998v1",
    "download_url": "http://arxiv.org/abs/1808.03998v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Are object detection assessment criteria ready for maritime computer\n  vision?",
    "abstract": "Maritime vessels equipped with visible and infrared cameras can complement\nother conventional sensors for object detection. However, application of\ncomputer vision techniques in maritime domain received attention only recently.\nThe maritime environment offers its own unique requirements and challenges.\nAssessment of the quality of detections is a fundamental need in computer\nvision. However, the conventional assessment metrics suitable for usual object\ndetection are deficient in the maritime setting. Thus, a large body of related\nwork in computer vision appears inapplicable to the maritime setting at the\nfirst sight. We discuss the problem of defining assessment metrics suitable for\nmaritime computer vision. We consider new bottom edge proximity metrics as\nassessment metrics for maritime computer vision. These metrics indicate that\nexisting computer vision approaches are indeed promising for maritime computer\nvision and can play a foundational role in the emerging field of maritime\ncomputer vision.",
    "authors": [
      "Dilip K. Prasad",
      "Huixu Dong",
      "Deepu Rajan",
      "Chai Quek"
    ],
    "publication_date": "2018-09-12T20:18:04Z",
    "arxiv_id": "http://arxiv.org/abs/1809.04659v2",
    "download_url": "http://arxiv.org/abs/1809.04659v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "BMVC 2019: Workshop on Interpretable and Explainable Machine Vision",
    "abstract": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable\nMachine Vision, Cardiff, UK, September 12, 2019.",
    "authors": [
      "Alun Preece"
    ],
    "publication_date": "2019-09-16T14:44:19Z",
    "arxiv_id": "http://arxiv.org/abs/1909.07245v1",
    "download_url": "http://arxiv.org/abs/1909.07245v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Vision Transformers in Medical Computer Vision -- A Contemplative\n  Retrospection",
    "abstract": "Recent escalation in the field of computer vision underpins a huddle of\nalgorithms with the magnificent potential to unravel the information contained\nwithin images. These computer vision algorithms are being practised in medical\nimage analysis and are transfiguring the perception and interpretation of\nImaging data. Among these algorithms, Vision Transformers are evolved as one of\nthe most contemporary and dominant architectures that are being used in the\nfield of computer vision. These are immensely utilized by a plenty of\nresearchers to perform new as well as former experiments. Here, in this article\nwe investigate the intersection of Vision Transformers and Medical images and\nproffered an overview of various ViTs based frameworks that are being used by\ndifferent researchers in order to decipher the obstacles in Medical Computer\nVision. We surveyed the application of Vision transformers in different areas\nof medical computer vision such as image-based disease classification,\nanatomical structure segmentation, registration, region-based lesion Detection,\ncaptioning, report generation, reconstruction using multiple medical imaging\nmodalities that greatly assist in medical diagnosis and hence treatment\nprocess. Along with this, we also demystify several imaging modalities used in\nMedical Computer Vision. Moreover, to get more insight and deeper\nunderstanding, self-attention mechanism of transformers is also explained\nbriefly. Conclusively, we also put some light on available data sets, adopted\nmethodology, their performance measures, challenges and their solutions in form\nof discussion. We hope that this review article will open future directions for\nresearchers in medical computer vision.",
    "authors": [
      "Arshi Parvaiz",
      "Muhammad Anwaar Khalid",
      "Rukhsana Zafar",
      "Huma Ameer",
      "Muhammad Ali",
      "Muhammad Moazam Fraz"
    ],
    "publication_date": "2022-03-29T06:32:43Z",
    "arxiv_id": "http://arxiv.org/abs/2203.15269v1",
    "download_url": "http://arxiv.org/abs/2203.15269v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for\n  Large-scale Vision-Language Models",
    "abstract": "Large-scale Vision-Language Models (LVLMs) have significantly advanced with\ntext-aligned vision inputs. They have made remarkable progress in computer\nvision tasks by aligning text modality with vision inputs. There are also\nendeavors to incorporate multi-vision sensors beyond RGB, including thermal,\ndepth, and medical X-ray images. However, we observe that current LVLMs view\nimages taken from multi-vision sensors as if they were in the same RGB domain\nwithout considering the physical characteristics of multi-vision sensors. They\nfail to convey the fundamental multi-vision sensor information from the dataset\nand the corresponding contextual knowledge properly. Consequently, alignment\nbetween the information from the actual physical environment and the text is\nnot achieved correctly, making it difficult to answer complex sensor-related\nquestions that consider the physical environment. In this paper, we aim to\nestablish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK\nthat can reduce the fundamental multi-vision sensor information gap between\nimages and multi-vision sensors. We generated 6,248 vision-language test\nsamples to investigate multi-vision sensory perception and multi-vision sensory\nreasoning on physical sensor knowledge proficiency across different formats,\ncovering different types of sensor-related questions. We utilized these samples\nto assess ten leading LVLMs. The results showed that most models displayed\ndeficiencies in multi-vision sensory reasoning to varying extents. Codes and\ndata are available at https://github.com/top-yun/SPARK",
    "authors": [
      "Youngjoon Yu",
      "Sangyun Chung",
      "Byung-Kwan Lee",
      "Yong Man Ro"
    ],
    "publication_date": "2024-08-22T03:59:48Z",
    "arxiv_id": "http://arxiv.org/abs/2408.12114v3",
    "download_url": "http://arxiv.org/abs/2408.12114v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Adapting Computer Vision Algorithms for Omnidirectional Video",
    "abstract": "Omnidirectional (360{\\deg}) video has got quite popular because it provides a\nhighly immersive viewing experience. For computer vision algorithms, it poses\nseveral challenges, like the special (equirectangular) projection commonly\nemployed and the huge image size. In this work, we give a high-level overview\nof these challenges and outline strategies how to adapt computer vision\nalgorithm for the specifics of omnidirectional video.",
    "authors": [
      "Hannes Fassold"
    ],
    "publication_date": "2019-07-22T11:12:35Z",
    "arxiv_id": "http://arxiv.org/abs/1907.09233v1",
    "download_url": "http://arxiv.org/abs/1907.09233v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Fast Motion Estimation and Context-Aware Refinement for Efficient\n  Bayer-Domain Video Vision",
    "abstract": "The efficiency of video computer vision system remains a challenging task due\nto the high temporal redundancy inside a video. Existing works have been\nproposed for efficient vision computer vision. However, they do not fully\nreduce the temporal redundancy and neglect the front end computation overhead.\nIn this paper, we propose an efficient video computer vision system. First,\nimage signal processor is removed and Bayer-format data is directly fed into\nvideo computer vision models, thus saving the front end computation. Second,\ninstead of optical flow models and video codecs, a fast block matching-based\nmotion estimation algorithm is proposed specifically for efficient video\ncomputer vision, with a MV refinement module. To correct the error,\ncontext-aware block refinement network is introduced to refine regions with\nlarge error. To further balance the accuracy and efficiency, a frame selection\nstrategy is employed. Experiments on multiple video computer vision tasks\ndemonstrate that our method achieves significant acceleration with slight\nperformance loss.",
    "authors": [
      "Haichao Wang",
      "Xinyue Xi",
      "Jiangtao Wen",
      "Yuxing Han"
    ],
    "publication_date": "2025-08-08T03:55:19Z",
    "arxiv_id": "http://arxiv.org/abs/2508.05990v2",
    "download_url": "http://arxiv.org/abs/2508.05990v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Real-time Tracking Based on Neuromrophic Vision",
    "abstract": "Real-time tracking is an important problem in computer vision in which most\nmethods are based on the conventional cameras. Neuromorphic vision is a concept\ndefined by incorporating neuromorphic vision sensors such as silicon retinas in\nvision processing system. With the development of the silicon technology,\nasynchronous event-based silicon retinas that mimic neuro-biological\narchitectures has been developed in recent years. In this work, we combine the\nvision tracking algorithm of computer vision with the information encoding\nmechanism of event-based sensors which is inspired from the neural rate coding\nmechanism. The real-time tracking of single object with the advantage of high\nspeed of 100 time bins per second is successfully realized. Our method\ndemonstrates that the computer vision methods could be used for the\nneuromorphic vision processing and we can realize fast real-time tracking using\nneuromorphic vision sensors compare to the conventional camera.",
    "authors": [
      "Hongmin Li",
      "Pei Jing",
      "Guoqi Li"
    ],
    "publication_date": "2015-10-18T16:27:36Z",
    "arxiv_id": "http://arxiv.org/abs/1510.05275v1",
    "download_url": "http://arxiv.org/abs/1510.05275v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Vision Remember: Alleviating Visual Forgetting in Efficient MLLM with\n  Vision Feature Resample",
    "abstract": "In this work, we study the Efficient Multimodal Large Language Model.\nRedundant vision tokens consume a significant amount of computational memory\nand resources. Therefore, many previous works compress them in the Vision\nProjector to reduce the number of vision tokens. However, simply compressing in\nthe Vision Projector can lead to the loss of visual information, especially for\ntasks that rely on fine-grained spatial relationships, such as OCR and Chart \\&\nTable Understanding. To address this problem, we propose Vision Remember, which\nis inserted between the LLM decoder layers to allow vision tokens to\nre-memorize vision features. Specifically, we retain multi-level vision\nfeatures and resample them with the vision tokens that have interacted with the\ntext token. During the resampling process, each vision token only attends to a\nlocal region in vision features, which is referred to as saliency-enhancing\nlocal attention. Saliency-enhancing local attention not only improves\ncomputational efficiency but also captures more fine-grained contextual\ninformation and spatial relationships within the region. Comprehensive\nexperiments on multiple visual understanding benchmarks validate the\neffectiveness of our method when combined with various Efficient Vision\nProjectors, showing performance gains without sacrificing efficiency. Based on\nVision Remember, LLaVA-VR with only 2B parameters is also superior to previous\nrepresentative MLLMs such as Tokenpacker-HD-7B and DeepSeek-VL-7B.",
    "authors": [
      "Ze Feng",
      "Jiang-Jiang Liu",
      "Sen Yang",
      "Lingyu Xiao",
      "Xiaofan Li",
      "Wankou Yang",
      "Jingdong Wang"
    ],
    "publication_date": "2025-06-04T13:22:35Z",
    "arxiv_id": "http://arxiv.org/abs/2506.03928v1",
    "download_url": "http://arxiv.org/abs/2506.03928v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Reconfiguring the Imaging Pipeline for Computer Vision",
    "abstract": "Advancements in deep learning have ignited an explosion of research on\nefficient hardware for embedded computer vision. Hardware vision acceleration,\nhowever, does not address the cost of capturing and processing the image data\nthat feeds these algorithms. We examine the role of the image signal processing\n(ISP) pipeline in computer vision to identify opportunities to reduce\ncomputation and save energy. The key insight is that imaging pipelines should\nbe designed to be configurable: to switch between a traditional photography\nmode and a low-power vision mode that produces lower-quality image data\nsuitable only for computer vision. We use eight computer vision algorithms and\na reversible pipeline simulation tool to study the imaging system's impact on\nvision performance. For both CNN-based and classical vision algorithms, we\nobserve that only two ISP stages, demosaicing and gamma compression, are\ncritical for task performance. We propose a new image sensor design that can\ncompensate for skipping these stages. The sensor design features an adjustable\nresolution and tunable analog-to-digital converters (ADCs). Our proposed\nimaging system's vision mode disables the ISP entirely and configures the\nsensor to produce subsampled, lower-precision image data. This vision mode can\nsave ~75% of the average energy of a baseline photography mode while having\nonly a small impact on vision task accuracy.",
    "authors": [
      "Mark Buckler",
      "Suren Jayasuriya",
      "Adrian Sampson"
    ],
    "publication_date": "2017-05-11T18:57:01Z",
    "arxiv_id": "http://arxiv.org/abs/1705.04352v3",
    "download_url": "http://arxiv.org/abs/1705.04352v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Integration and Performance Analysis of Artificial Intelligence and\n  Computer Vision Based on Deep Learning Algorithms",
    "abstract": "This paper focuses on the analysis of the application effectiveness of the\nintegration of deep learning and computer vision technologies. Deep learning\nachieves a historic breakthrough by constructing hierarchical neural networks,\nenabling end-to-end feature learning and semantic understanding of images. The\nsuccessful experiences in the field of computer vision provide strong support\nfor training deep learning algorithms. The tight integration of these two\nfields has given rise to a new generation of advanced computer vision systems,\nsignificantly surpassing traditional methods in tasks such as machine vision\nimage classification and object detection. In this paper, typical image\nclassification cases are combined to analyze the superior performance of deep\nneural network models while also pointing out their limitations in\ngeneralization and interpretability, proposing directions for future\nimprovements. Overall, the efficient integration and development trend of deep\nlearning with massive visual data will continue to drive technological\nbreakthroughs and application expansion in the field of computer vision, making\nit possible to build truly intelligent machine vision systems. This deepening\nfusion paradigm will powerfully promote unprecedented tasks and functions in\ncomputer vision, providing stronger development momentum for related\ndisciplines and industries.",
    "authors": [
      "Bo Liu",
      "Liqiang Yu",
      "Chang Che",
      "Qunwei Lin",
      "Hao Hu",
      "Xinyu Zhao"
    ],
    "publication_date": "2023-12-20T09:37:06Z",
    "arxiv_id": "http://arxiv.org/abs/2312.12872v1",
    "download_url": "http://arxiv.org/abs/2312.12872v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Scaling Up Computer Vision Neural Networks Using Fast Fourier Transform",
    "abstract": "Deep Learning-based Computer Vision field has recently been trying to explore\nlarger kernels for convolution to effectively scale up Convolutional Neural\nNetworks. Simultaneously, new paradigm of models such as Vision Transformers\nfind it difficult to scale up to larger higher resolution images due to their\nquadratic complexity in terms of input sequence. In this report, Fast Fourier\nTransform is utilised in various ways to provide some solutions to these\nissues.",
    "authors": [
      "Siddharth Agrawal"
    ],
    "publication_date": "2023-02-02T19:19:10Z",
    "arxiv_id": "http://arxiv.org/abs/2302.12185v1",
    "download_url": "http://arxiv.org/abs/2302.12185v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Ethics and Creativity in Computer Vision",
    "abstract": "This paper offers a retrospective of what we learnt from organizing the\nworkshop *Ethical Considerations in Creative applications of Computer Vision*\nat CVPR 2021 conference and, prior to that, a series of workshops on *Computer\nVision for Fashion, Art and Design* at ECCV 2018, ICCV 2019, and CVPR 2020. We\nhope this reflection will bring artists and machine learning researchers into\nconversation around the ethical and social dimensions of creative applications\nof computer vision.",
    "authors": [
      "Negar Rostamzadeh",
      "Emily Denton",
      "Linda Petrini"
    ],
    "publication_date": "2021-12-06T15:23:08Z",
    "arxiv_id": "http://arxiv.org/abs/2112.03111v1",
    "download_url": "http://arxiv.org/abs/2112.03111v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models",
    "abstract": "Large multimodal models (LMMs) suffer significant computational challenges\ndue to the high cost of Large Language Models (LLMs) and the quadratic\ncomplexity of processing long vision token sequences. In this paper, we explore\nthe spatial redundancy among vision tokens and shorten the length of vision\ntoken sequences for inference acceleration. Specifically, we propose a Spatial\nToken Fusion (STF) method to learn compact vision tokens for short vision token\nsequence, where spatial-adjacent tokens are fused into one. Meanwhile,\nweight-frozen vision encoder can not well adapt to the demand of extensive\ndownstream vision-language tasks. To this end, we further introduce a\nMulti-Block Token Fusion (MBTF) module to supplement multi-granularity features\nfor the reduced token sequence. Overall, we combine STF and MBTF module to\nbalance token reduction and information preservation, thereby improving\ninference efficiency without sacrificing multimodal reasoning capabilities.\nExperimental results demonstrate that our method based on LLaVA-1.5 achieves\ncomparable or even superior performance to the baseline on 8 popular\nvision-language benchmarks with only $25\\%$ vision tokens of baseline. The\nsource code and trained weights are available at\nhttps://github.com/visresearch/LLaVA-STF.",
    "authors": [
      "Hao Tang",
      "Chengchao Shen"
    ],
    "publication_date": "2025-06-08T13:36:06Z",
    "arxiv_id": "http://arxiv.org/abs/2506.07138v1",
    "download_url": "http://arxiv.org/abs/2506.07138v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "I'm sorry to say, but your understanding of image processing\n  fundamentals is absolutely wrong",
    "abstract": "The ongoing discussion whether modern vision systems have to be viewed as\nvisually-enabled cognitive systems or cognitively-enabled vision systems is\ngroundless, because perceptual and cognitive faculties of vision are separate\ncomponents of human (and consequently, artificial) information processing\nsystem modeling.",
    "authors": [
      "Emanuel Diamant"
    ],
    "publication_date": "2008-08-01T04:45:17Z",
    "arxiv_id": "http://arxiv.org/abs/0808.0056v1",
    "download_url": "http://arxiv.org/abs/0808.0056v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Nomic Embed Vision: Expanding the Latent Space",
    "abstract": "This technical report describes the training of nomic-embed-vision, a highly\nperformant, open-code, open-weights image embedding model that shares the same\nlatent space as nomic-embed-text. Together, nomic-embed-vision and\nnomic-embed-text form the first unified latent space to achieve high\nperformance across vision, language, and multimodal tasks.",
    "authors": [
      "Zach Nussbaum",
      "Brandon Duderstadt",
      "Andriy Mulyar"
    ],
    "publication_date": "2024-06-06T21:02:51Z",
    "arxiv_id": "http://arxiv.org/abs/2406.18587v1",
    "download_url": "http://arxiv.org/abs/2406.18587v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time\n  Compute",
    "abstract": "Developing trustworthy intelligent vision systems for high-stakes domains,\n\\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness\nwithout costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a\ntraining-free, agentic reasoning framework that wraps off-the-shelf\nvision-language models \\emph{and} pure vision systems in a\n\\emph{Think--Critique--Act} loop. While VRA incurs significant additional\ntest-time computation, it achieves up to 40\\% absolute accuracy gains on\nchallenging visual reasoning benchmarks. Future work will optimize query\nrouting and early stopping to reduce inference overhead while preserving\nreliability in vision tasks.",
    "authors": [
      "Chung-En",
      "Yu",
      "Brian Jalaian",
      "Nathaniel D. Bastian"
    ],
    "publication_date": "2025-09-19T18:34:08Z",
    "arxiv_id": "http://arxiv.org/abs/2509.16343v1",
    "download_url": "http://arxiv.org/abs/2509.16343v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Computer Vision and Abnormal Patient Gait Assessment a Comparison of\n  Machine Learning Models",
    "abstract": "Abnormal gait, its associated falls and complications have high patient\nmorbidity, mortality. Computer vision detects, predicts patient gait\nabnormalities, assesses fall risk and serves as clinical decision support tool\nfor physicians. This paper performs a systematic review of how computer vision,\nmachine learning models perform an abnormal patient's gait assessment. Computer\nvision is beneficial in gait analysis, it helps capture the patient posture.\nSeveral literature suggests the use of different machine learning algorithms\nsuch as SVM, ANN, K-Star, Random Forest, KNN, among others to perform the\nclassification on the features extracted to study patient gait abnormalities.",
    "authors": [
      "Jasmin Hundall",
      "Benson A. Babu"
    ],
    "publication_date": "2020-03-22T02:00:15Z",
    "arxiv_id": "http://arxiv.org/abs/2004.02810v1",
    "download_url": "http://arxiv.org/abs/2004.02810v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Tuning computer vision models with task rewards",
    "abstract": "Misalignment between model predictions and intended usage can be detrimental\nfor the deployment of computer vision models. The issue is exacerbated when the\ntask involves complex structured outputs, as it becomes harder to design\nprocedures which address this misalignment. In natural language processing,\nthis is often addressed using reinforcement learning techniques that align\nmodels with a task reward. We adopt this approach and show its surprising\neffectiveness across multiple computer vision tasks, such as object detection,\npanoptic segmentation, colorization and image captioning. We believe this\napproach has the potential to be widely useful for better aligning models with\na diverse range of computer vision tasks.",
    "authors": [
      "André Susano Pinto",
      "Alexander Kolesnikov",
      "Yuge Shi",
      "Lucas Beyer",
      "Xiaohua Zhai"
    ],
    "publication_date": "2023-02-16T11:49:48Z",
    "arxiv_id": "http://arxiv.org/abs/2302.08242v1",
    "download_url": "http://arxiv.org/abs/2302.08242v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "The possibility of making \\$138,000 from shredded banknote pieces using\n  computer vision",
    "abstract": "Every country must dispose of old banknotes. At the Hong Kong Monetary\nAuthority visitor center, visitors can buy a paperweight souvenir full of\nshredded banknotes. Even though the shredded banknotes are small, by using\ncomputer vision, it is possible to reconstruct the whole banknote like a jigsaw\npuzzle. Each paperweight souvenir costs \\$100 HKD, and it is claimed to contain\nshredded banknotes equivalent to 138 complete \\$1000 HKD banknotes. In theory,\n\\$138,000 HKD can be recovered by using computer vision. This paper discusses\nthe technique of collecting shredded banknote pieces and applying a computer\nvision program.",
    "authors": [
      "Chung To Kong"
    ],
    "publication_date": "2023-11-17T02:25:31Z",
    "arxiv_id": "http://arxiv.org/abs/2401.06133v1",
    "download_url": "http://arxiv.org/abs/2401.06133v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Computer Stereo Vision for Autonomous Driving",
    "abstract": "As an important component of autonomous systems, autonomous car perception\nhas had a big leap with recent advances in parallel computing architectures.\nWith the use of tiny but full-feature embedded supercomputers, computer stereo\nvision has been prevalently applied in autonomous cars for depth perception.\nThe two key aspects of computer stereo vision are speed and accuracy. They are\nboth desirable but conflicting properties, as the algorithms with better\ndisparity accuracy usually have higher computational complexity. Therefore, the\nmain aim of developing a computer stereo vision algorithm for resource-limited\nhardware is to improve the trade-off between speed and accuracy. In this\nchapter, we introduce both the hardware and software aspects of computer stereo\nvision for autonomous car systems. Then, we discuss four autonomous car\nperception tasks, including 1) visual feature detection, description and\nmatching, 2) 3D information acquisition, 3) object detection/recognition and 4)\nsemantic image segmentation. The principles of computer stereo vision and\nparallel computing on multi-threading CPU and GPU architectures are then\ndetailed.",
    "authors": [
      "Rui Fan",
      "Li Wang",
      "Mohammud Junaid Bocus",
      "Ioannis Pitas"
    ],
    "publication_date": "2020-12-06T06:54:03Z",
    "arxiv_id": "http://arxiv.org/abs/2012.03194v2",
    "download_url": "http://arxiv.org/abs/2012.03194v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Computers Should Be Uniters Not Dividers: A Vision of Computer-Enhanced\n  Happy Future",
    "abstract": "This manifesto provides a vision of how computers can be used to bring people\ntogether, to enhance people's use of their natural creativity, and thus, make\nthem happier.",
    "authors": [
      "Alexander Titovets",
      "Philip Mills",
      "Vladik Kreinovich"
    ],
    "publication_date": "2014-08-30T19:55:55Z",
    "arxiv_id": "http://arxiv.org/abs/1409.0158v1",
    "download_url": "http://arxiv.org/abs/1409.0158v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "LM4LV: A Frozen Large Language Model for Low-level Vision Tasks",
    "abstract": "The success of large language models (LLMs) has fostered a new research trend\nof multi-modality large language models (MLLMs), which changes the paradigm of\nvarious fields in computer vision. Though MLLMs have shown promising results in\nnumerous high-level vision and vision-language tasks such as VQA and\ntext-to-image, no works have demonstrated how low-level vision tasks can\nbenefit from MLLMs. We find that most current MLLMs are blind to low-level\nfeatures due to their design of vision modules, thus are inherently incapable\nfor solving low-level vision tasks. In this work, we purpose $\\textbf{LM4LV}$,\na framework that enables a FROZEN LLM to solve a range of low-level vision\ntasks without any multi-modal data or prior. This showcases the LLM's strong\npotential in low-level vision and bridges the gap between MLLMs and low-level\nvision tasks. We hope this work can inspire new perspectives on LLMs and deeper\nunderstanding of their mechanisms. Code is available at\nhttps://github.com/bytetriper/LM4LV.",
    "authors": [
      "Boyang Zheng",
      "Jinjin Gu",
      "Shijun Li",
      "Chao Dong"
    ],
    "publication_date": "2024-05-24T17:25:00Z",
    "arxiv_id": "http://arxiv.org/abs/2405.15734v2",
    "download_url": "http://arxiv.org/abs/2405.15734v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Novel Framework for Automated Explain Vision Model Using\n  Vision-Language Models",
    "abstract": "The development of many vision models mainly focuses on improving their\nperformance using metrics such as accuracy, IoU, and mAP, with less attention\nto explainability due to the complexity of applying xAI methods to provide a\nmeaningful explanation of trained models. Although many existing xAI methods\naim to explain vision models sample-by-sample, methods explaining the general\nbehavior of vision models, which can only be captured after running on a large\ndataset, are still underexplored. Furthermore, understanding the behavior of\nvision models on general images can be very important to prevent biased\njudgments and help identify the model's trends and patterns. With the\napplication of Vision-Language Models, this paper proposes a pipeline to\nexplain vision models at both the sample and dataset levels. The proposed\npipeline can be used to discover failure cases and gain insights into vision\nmodels with minimal effort, thereby integrating vision model development with\nxAI analysis to advance image analysis.",
    "authors": [
      "Phu-Vinh Nguyen",
      "Tan-Hanh Pham",
      "Chris Ngo",
      "Truong Son Hy"
    ],
    "publication_date": "2025-08-27T19:16:40Z",
    "arxiv_id": "http://arxiv.org/abs/2508.20227v1",
    "download_url": "http://arxiv.org/abs/2508.20227v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Vision Mamba for Permeability Prediction of Porous Media",
    "abstract": "Vision Mamba has recently received attention as an alternative to Vision\nTransformers (ViTs) for image classification. The network size of Vision Mamba\nscales linearly with input image resolution, whereas ViTs scale quadratically,\na feature that improves computational and memory efficiency. Moreover, Vision\nMamba requires a significantly smaller number of trainable parameters than\ntraditional convolutional neural networks (CNNs), and thus, they can be more\nmemory efficient. Because of these features, we introduce, for the first time,\na neural network that uses Vision Mamba as its backbone for predicting the\npermeability of three-dimensional porous media. We compare the performance of\nVision Mamba with ViT and CNN models across multiple aspects of permeability\nprediction and perform an ablation study to assess the effects of its\ncomponents on accuracy. We demonstrate in practice the aforementioned\nadvantages of Vision Mamba over ViTs and CNNs in the permeability prediction of\nthree-dimensional porous media. We make the source code publicly available to\nfacilitate reproducibility and to enable other researchers to build on and\nextend this work. We believe the proposed framework has the potential to be\nintegrated into large vision models in which Vision Mamba is used instead of\nViTs.",
    "authors": [
      "Ali Kashefi",
      "Tapan Mukerji"
    ],
    "publication_date": "2025-10-16T10:02:33Z",
    "arxiv_id": "http://arxiv.org/abs/2510.14516v2",
    "download_url": "http://arxiv.org/abs/2510.14516v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A survey of the Vision Transformers and their CNN-Transformer based\n  Variants",
    "abstract": "Vision transformers have become popular as a possible substitute to\nconvolutional neural networks (CNNs) for a variety of computer vision\napplications. These transformers, with their ability to focus on global\nrelationships in images, offer large learning capacity. However, they may\nsuffer from limited generalization as they do not tend to model local\ncorrelation in images. Recently, in vision transformers hybridization of both\nthe convolution operation and self-attention mechanism has emerged, to exploit\nboth the local and global image representations. These hybrid vision\ntransformers, also referred to as CNN-Transformer architectures, have\ndemonstrated remarkable results in vision applications. Given the rapidly\ngrowing number of hybrid vision transformers, it has become necessary to\nprovide a taxonomy and explanation of these hybrid architectures. This survey\npresents a taxonomy of the recent vision transformer architectures and more\nspecifically that of the hybrid vision transformers. Additionally, the key\nfeatures of these architectures such as the attention mechanisms, positional\nembeddings, multi-scale processing, and convolution are also discussed. In\ncontrast to the previous survey papers that are primarily focused on individual\nvision transformer architectures or CNNs, this survey uniquely emphasizes the\nemerging trend of hybrid vision transformers. By showcasing the potential of\nhybrid vision transformers to deliver exceptional performance across a range of\ncomputer vision tasks, this survey sheds light on the future directions of this\nrapidly evolving architecture.",
    "authors": [
      "Asifullah Khan",
      "Zunaira Rauf",
      "Anabia Sohail",
      "Abdul Rehman",
      "Hifsa Asif",
      "Aqsa Asif",
      "Umair Farooq"
    ],
    "publication_date": "2023-05-17T01:27:27Z",
    "arxiv_id": "http://arxiv.org/abs/2305.09880v4",
    "download_url": "http://arxiv.org/abs/2305.09880v4",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Survey on Deep Learning Methods for Robot Vision",
    "abstract": "Deep learning has allowed a paradigm shift in pattern recognition, from using\nhand-crafted features together with statistical classifiers to using\ngeneral-purpose learning procedures for learning data-driven representations,\nfeatures, and classifiers together. The application of this new paradigm has\nbeen particularly successful in computer vision, in which the development of\ndeep learning methods for vision applications has become a hot research topic.\nGiven that deep learning has already attracted the attention of the robot\nvision community, the main purpose of this survey is to address the use of deep\nlearning in robot vision. To achieve this, a comprehensive overview of deep\nlearning and its usage in computer vision is given, that includes a description\nof the most frequently used neural models and their main application areas.\nThen, the standard methodology and tools used for designing deep-learning based\nvision systems are presented. Afterwards, a review of the principal work using\ndeep learning in robot vision is presented, as well as current and future\ntrends related to the use of deep learning in robotics. This survey is intended\nto be a guide for the developers of robot vision systems.",
    "authors": [
      "Javier Ruiz-del-Solar",
      "Patricio Loncomilla",
      "Naiomi Soto"
    ],
    "publication_date": "2018-03-28T21:37:14Z",
    "arxiv_id": "http://arxiv.org/abs/1803.10862v1",
    "download_url": "http://arxiv.org/abs/1803.10862v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "CloudCV: Large Scale Distributed Computer Vision as a Cloud Service",
    "abstract": "We are witnessing a proliferation of massive visual data. Unfortunately\nscaling existing computer vision algorithms to large datasets leaves\nresearchers repeatedly solving the same algorithmic, logistical, and\ninfrastructural problems. Our goal is to democratize computer vision; one\nshould not have to be a computer vision, big data and distributed computing\nexpert to have access to state-of-the-art distributed computer vision\nalgorithms. We present CloudCV, a comprehensive system to provide access to\nstate-of-the-art distributed computer vision algorithms as a cloud service\nthrough a Web Interface and APIs.",
    "authors": [
      "Harsh Agrawal",
      "Clint Solomon Mathialagan",
      "Yash Goyal",
      "Neelima Chavali",
      "Prakriti Banik",
      "Akrit Mohapatra",
      "Ahmed Osman",
      "Dhruv Batra"
    ],
    "publication_date": "2015-06-12T19:50:07Z",
    "arxiv_id": "http://arxiv.org/abs/1506.04130v3",
    "download_url": "http://arxiv.org/abs/1506.04130v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Teaching Computer Vision for Ecology",
    "abstract": "Computer vision can accelerate ecology research by automating the analysis of\nraw imagery from sensors like camera traps, drones, and satellites. However,\ncomputer vision is an emerging discipline that is rarely taught to ecologists.\nThis work discusses our experience teaching a diverse group of ecologists to\nprototype and evaluate computer vision systems in the context of an intensive\nhands-on summer workshop. We explain the workshop structure, discuss common\nchallenges, and propose best practices. This document is intended for computer\nscientists who teach computer vision across disciplines, but it may also be\nuseful to ecologists or other domain experts who are learning to use computer\nvision themselves.",
    "authors": [
      "Elijah Cole",
      "Suzanne Stathatos",
      "Björn Lütjens",
      "Tarun Sharma",
      "Justin Kay",
      "Jason Parham",
      "Benjamin Kellenberger",
      "Sara Beery"
    ],
    "publication_date": "2023-01-05T18:30:17Z",
    "arxiv_id": "http://arxiv.org/abs/2301.02211v1",
    "download_url": "http://arxiv.org/abs/2301.02211v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Snapshot of Algebraic Vision",
    "abstract": "In this survey article, we present interactions between algebraic geometry\nand computer vision, which have recently come under the header of algebraic\nvision. The subject has given new insights in multiple view geometry and its\napplication to 3D scene reconstruction and carried a host of novel problems and\nideas back into algebraic geometry.",
    "authors": [
      "Joe Kileel",
      "Kathlén Kohn"
    ],
    "publication_date": "2022-10-20T17:45:22Z",
    "arxiv_id": "http://arxiv.org/abs/2210.11443v2",
    "download_url": "http://arxiv.org/abs/2210.11443v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Negative Results in Computer Vision: A Perspective",
    "abstract": "A negative result is when the outcome of an experiment or a model is not what\nis expected or when a hypothesis does not hold. Despite being often overlooked\nin the scientific community, negative results are results and they carry value.\nWhile this topic has been extensively discussed in other fields such as social\nsciences and biosciences, less attention has been paid to it in the computer\nvision community. The unique characteristics of computer vision, particularly\nits experimental aspect, call for a special treatment of this matter. In this\npaper, I will address what makes negative results important, how they should be\ndisseminated and incentivized, and what lessons can be learned from cognitive\nvision research in this regard. Further, I will discuss issues such as computer\nvision and human vision interaction, experimental design and statistical\nhypothesis testing, explanatory versus predictive modeling, performance\nevaluation, model comparison, as well as computer vision research culture.",
    "authors": [
      "Ali Borji"
    ],
    "publication_date": "2017-05-11T23:39:18Z",
    "arxiv_id": "http://arxiv.org/abs/1705.04402v3",
    "download_url": "http://arxiv.org/abs/1705.04402v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Quantifying Visual Image Quality: A Bayesian View",
    "abstract": "Image quality assessment (IQA) models aim to establish a quantitative\nrelationship between visual images and their perceptual quality by human\nobservers. IQA modeling plays a special bridging role between vision science\nand engineering practice, both as a test-bed for vision theories and\ncomputational biovision models, and as a powerful tool that could potentially\nmake profound impact on a broad range of image processing, computer vision, and\ncomputer graphics applications, for design, optimization, and evaluation\npurposes. IQA research has enjoyed an accelerated growth in the past two\ndecades. Here we present an overview of IQA methods from a Bayesian\nperspective, with the goals of unifying a wide spectrum of IQA approaches under\na common framework and providing useful references to fundamental concepts\naccessible to vision scientists and image processing practitioners. We discuss\nthe implications of the successes and limitations of modern IQA methods for\nbiological vision and the prospect for vision science to inform the design of\nfuture artificial vision systems.",
    "authors": [
      "Zhengfang Duanmu",
      "Wentao Liu",
      "Zhongling Wang",
      "Zhou Wang"
    ],
    "publication_date": "2021-01-30T09:34:23Z",
    "arxiv_id": "http://arxiv.org/abs/2102.00195v2",
    "download_url": "http://arxiv.org/abs/2102.00195v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Vision Language Transformers: A Survey",
    "abstract": "Vision language tasks, such as answering questions about or generating\ncaptions that describe an image, are difficult tasks for computers to perform.\nA relatively recent body of research has adapted the pretrained transformer\narchitecture introduced in \\citet{vaswani2017attention} to vision language\nmodeling. Transformer models have greatly improved performance and versatility\nover previous vision language models. They do so by pretraining models on a\nlarge generic datasets and transferring their learning to new tasks with minor\nchanges in architecture and parameter values. This type of transfer learning\nhas become the standard modeling practice in both natural language processing\nand computer vision. Vision language transformers offer the promise of\nproducing similar advancements in tasks which require both vision and language.\nIn this paper, we provide a broad synthesis of the currently available research\non vision language transformer models and offer some analysis of their\nstrengths, limitations and some open questions that remain.",
    "authors": [
      "Clayton Fields",
      "Casey Kennington"
    ],
    "publication_date": "2023-07-06T19:08:56Z",
    "arxiv_id": "http://arxiv.org/abs/2307.03254v1",
    "download_url": "http://arxiv.org/abs/2307.03254v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "DAMamba: Vision State Space Model with Dynamic Adaptive Scan",
    "abstract": "State space models (SSMs) have recently garnered significant attention in\ncomputer vision. However, due to the unique characteristics of image data,\nadapting SSMs from natural language processing to computer vision has not\noutperformed the state-of-the-art convolutional neural networks (CNNs) and\nVision Transformers (ViTs). Existing vision SSMs primarily leverage manually\ndesigned scans to flatten image patches into sequences locally or globally.\nThis approach disrupts the original semantic spatial adjacency of the image and\nlacks flexibility, making it difficult to capture complex image structures. To\naddress this limitation, we propose Dynamic Adaptive Scan (DAS), a data-driven\nmethod that adaptively allocates scanning orders and regions. This enables more\nflexible modeling capabilities while maintaining linear computational\ncomplexity and global modeling capacity. Based on DAS, we further propose the\nvision backbone DAMamba, which significantly outperforms current\nstate-of-the-art vision Mamba models in vision tasks such as image\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Notably, it surpasses some of the latest state-of-the-art CNNs\nand ViTs. Code will be available at https://github.com/ltzovo/DAMamba.",
    "authors": [
      "Tanzhe Li",
      "Caoshuo Li",
      "Jiayi Lyu",
      "Hongjuan Pei",
      "Baochang Zhang",
      "Taisong Jin",
      "Rongrong Ji"
    ],
    "publication_date": "2025-02-18T08:12:47Z",
    "arxiv_id": "http://arxiv.org/abs/2502.12627v1",
    "download_url": "http://arxiv.org/abs/2502.12627v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "CompressAI-Vision: Open-source software to evaluate compression methods\n  for computer vision tasks",
    "abstract": "With the increasing use of neural network (NN)-based computer vision\napplications that process image and video data as input, interest has emerged\nin video compression technology optimized for computer vision tasks. In fact,\ngiven the variety of vision tasks, associated NN models and datasets, a\nconsolidated platform is needed as a common ground to implement and evaluate\ncompression methods optimized for downstream vision tasks. CompressAI-Vision is\nintroduced as a comprehensive evaluation platform where new coding tools\ncompete to efficiently compress the input of vision network while retaining\ntask accuracy in the context of two different inference scenarios: \"remote\" and\n\"split\" inferencing. Our study showcases various use cases of the evaluation\nplatform incorporated with standard codecs (under development) by examining the\ncompression gain on several datasets in terms of bit-rate versus task accuracy.\nThis evaluation platform has been developed as open-source software and is\nadopted by the Moving Pictures Experts Group (MPEG) for the development the\nFeature Coding for Machines (FCM) standard. The software is available publicly\nat https://github.com/InterDigitalInc/CompressAI-Vision.",
    "authors": [
      "Hyomin Choi",
      "Heeji Han",
      "Chris Rosewarne",
      "Fabien Racapé"
    ],
    "publication_date": "2025-09-25T06:01:55Z",
    "arxiv_id": "http://arxiv.org/abs/2509.20777v1",
    "download_url": "http://arxiv.org/abs/2509.20777v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data\n  Lottery Tickets",
    "abstract": "Vision transformers have revolutionized computer vision, but their\ncomputational demands present challenges for training and deployment. This\npaper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel\nmethod that leverages data lottery ticket selection and sparsity pruning to\naccelerate vision transformer training while maintaining accuracy. Our approach\nfocuses on identifying and utilizing the most informative data subsets and\neliminating redundant model parameters to optimize the training process.\nThrough extensive experiments, we demonstrate the effectiveness of LOTUS in\nachieving rapid convergence and high accuracy with significantly reduced\ncomputational requirements. This work highlights the potential of combining\ndata selection and sparsity techniques for efficient vision transformer\ntraining, opening doors for further research and development in this area.",
    "authors": [
      "Ojasw Upadhyay"
    ],
    "publication_date": "2024-05-01T23:30:12Z",
    "arxiv_id": "http://arxiv.org/abs/2405.00906v1",
    "download_url": "http://arxiv.org/abs/2405.00906v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Features extraction for image identification using computer vision",
    "abstract": "This study examines various feature extraction techniques in computer vision,\nthe primary focus of which is on Vision Transformers (ViTs) and other\napproaches such as Generative Adversarial Networks (GANs), deep feature models,\ntraditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive\nfeature models. Emphasizing ViTs, the report summarizes their architecture,\nincluding patch embedding, positional encoding, and multi-head self-attention\nmechanisms with which they overperform conventional convolutional neural\nnetworks (CNNs). Experimental results determine the merits and limitations of\nboth methods and their utilitarian applications in advancing computer vision.",
    "authors": [
      "Venant Niyonkuru",
      "Sylla Sekou",
      "Jimmy Jackson Sinzinkayo"
    ],
    "publication_date": "2025-07-22T10:43:52Z",
    "arxiv_id": "http://arxiv.org/abs/2507.18650v1",
    "download_url": "http://arxiv.org/abs/2507.18650v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Leveraging Vision Reconstruction Pipelines for Satellite Imagery",
    "abstract": "Reconstructing 3D geometry from satellite imagery is an important topic of\nresearch. However, disparities exist between how this 3D reconstruction problem\nis handled in the remote sensing context and how multi-view reconstruction\npipelines have been developed in the computer vision community. In this paper,\nwe explore whether state-of-the-art reconstruction pipelines from the vision\ncommunity can be applied to the satellite imagery. Along the way, we address\nseveral challenges adapting vision-based structure from motion and multi-view\nstereo methods. We show that vision pipelines can offer competitive speed and\naccuracy in the satellite context.",
    "authors": [
      "Kai Zhang",
      "Jin Sun",
      "Noah Snavely"
    ],
    "publication_date": "2019-10-07T18:14:28Z",
    "arxiv_id": "http://arxiv.org/abs/1910.02989v2",
    "download_url": "http://arxiv.org/abs/1910.02989v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Agriculture-Vision Challenge 2022 -- The Runner-Up Solution for\n  Agricultural Pattern Recognition via Transformer-based Models",
    "abstract": "The Agriculture-Vision Challenge in CVPR is one of the most famous and\ncompetitive challenges for global researchers to break the boundary between\ncomputer vision and agriculture sectors, aiming at agricultural pattern\nrecognition from aerial images. In this paper, we propose our solution to the\nthird Agriculture-Vision Challenge in CVPR 2022. We leverage a data\npre-processing scheme and several Transformer-based models as well as data\naugmentation techniques to achieve a mIoU of 0.582, accomplishing the 2nd place\nin this challenge.",
    "authors": [
      "Zhicheng Yang",
      "Jui-Hsin Lai",
      "Jun Zhou",
      "Hang Zhou",
      "Chen Du",
      "Zhongcheng Lai"
    ],
    "publication_date": "2022-06-23T18:02:12Z",
    "arxiv_id": "http://arxiv.org/abs/2206.11920v1",
    "download_url": "http://arxiv.org/abs/2206.11920v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models",
    "abstract": "Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).",
    "authors": [
      "Shicheng Xu",
      "Liang Pang",
      "Yunchang Zhu",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "publication_date": "2024-10-16T15:20:08Z",
    "arxiv_id": "http://arxiv.org/abs/2410.12662v2",
    "download_url": "http://arxiv.org/abs/2410.12662v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
    "abstract": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
    "authors": [
      "Shiwei Wu",
      "Joya Chen",
      "Kevin Qinghong Lin",
      "Qimeng Wang",
      "Yan Gao",
      "Qianli Xu",
      "Tong Xu",
      "Yao Hu",
      "Enhong Chen",
      "Mike Zheng Shou"
    ],
    "publication_date": "2024-08-29T17:21:58Z",
    "arxiv_id": "http://arxiv.org/abs/2408.16730v1",
    "download_url": "http://arxiv.org/abs/2408.16730v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for\n  Visual Emotion Analysis",
    "abstract": "Visual emotion analysis, which has gained considerable attention in the field\nof affective computing, aims to predict the dominant emotions conveyed by an\nimage. Despite advancements in visual emotion analysis with the emergence of\nvision-language models, we observed that instruction-tuned vision-language\nmodels and conventional vision models exhibit complementary strengths in visual\nemotion analysis, as vision-language models excel in certain cases, whereas\nvision models perform better in others. This finding highlights the need to\nintegrate these capabilities to enhance the performance of visual emotion\nanalysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned\nvision-language model augmented with a lightweight module distilled from\nconventional vision models. Instead of deploying both models simultaneously,\nwhich incurs high computational costs, we transfer the predictive patterns of a\nconventional vision model into the vision-language model using a knowledge\ndistillation framework. Our approach first fine-tunes a vision-language model\non emotion-specific instruction data and then attaches a distilled module to\nits visual encoder while keeping the vision-language model frozen. Predictions\nfrom the vision language model and the distillation module are effectively\nbalanced by a gate module, which subsequently generates the final outcome.\nExtensive experiments show that EmoVLM-KD achieves state-of-the-art performance\non multiple visual emotion analysis benchmark datasets, outperforming the\nexisting methods while maintaining computational efficiency. The code is\navailable in https://github.com/sange1104/EmoVLM-KD.",
    "authors": [
      "SangEun Lee",
      "Yubeen Lee",
      "Eunil Park"
    ],
    "publication_date": "2025-05-12T01:15:50Z",
    "arxiv_id": "http://arxiv.org/abs/2505.07164v1",
    "download_url": "http://arxiv.org/abs/2505.07164v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Towards Point Cloud Compression for Machine Perception: A Simple and\n  Strong Baseline by Learning the Octree Depth Level Predictor",
    "abstract": "Point cloud compression has garnered significant interest in computer vision.\nHowever, existing algorithms primarily cater to human vision, while most point\ncloud data is utilized for machine vision tasks. To address this, we propose a\npoint cloud compression framework that simultaneously handles both human and\nmachine vision tasks. Our framework learns a scalable bit-stream, using only\nsubsets for different machine vision tasks to save bit-rate, while employing\nthe entire bit-stream for human vision tasks. Building on mainstream\noctree-based frameworks like VoxelContext-Net, OctAttention, and G-PCC, we\nintroduce a new octree depth-level predictor. This predictor adaptively\ndetermines the optimal depth level for each octree constructed from a point\ncloud, controlling the bit-rate for machine vision tasks. For simpler tasks\n(\\textit{e.g.}, classification) or objects/scenarios, we use fewer depth levels\nwith fewer bits, saving bit-rate. Conversely, for more complex tasks\n(\\textit{e.g}., segmentation) or objects/scenarios, we use deeper depth levels\nwith more bits to enhance performance. Experimental results on various datasets\n(\\textit{e.g}., ModelNet10, ModelNet40, ShapeNet, ScanNet, and KITTI) show that\nour point cloud compression approach improves performance for machine vision\ntasks without compromising human vision quality.",
    "authors": [
      "Lei Liu",
      "Zhihao Hu",
      "Zhenghao Chen"
    ],
    "publication_date": "2024-06-02T16:13:57Z",
    "arxiv_id": "http://arxiv.org/abs/2406.00791v1",
    "download_url": "http://arxiv.org/abs/2406.00791v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Review of 3D Object Detection with Vision-Language Models",
    "abstract": "This review provides a systematic analysis of comprehensive survey of 3D\nobject detection with vision-language models(VLMs) , a rapidly advancing area\nat the intersection of 3D vision and multimodal AI. By examining over 100\nresearch papers, we provide the first systematic analysis dedicated to 3D\nobject detection with vision-language models. We begin by outlining the unique\nchallenges of 3D object detection with vision-language models, emphasizing\ndifferences from 2D detection in spatial reasoning and data complexity.\nTraditional approaches using point clouds and voxel grids are compared to\nmodern vision-language frameworks like CLIP and 3D LLMs, which enable\nopen-vocabulary detection and zero-shot generalization. We review key\narchitectures, pretraining strategies, and prompt engineering methods that\nalign textual and 3D features for effective 3D object detection with\nvision-language models. Visualization examples and evaluation benchmarks are\ndiscussed to illustrate performance and behavior. Finally, we highlight current\nchallenges, such as limited 3D-language datasets and computational demands, and\npropose future research directions to advance 3D object detection with\nvision-language models. >Object Detection, Vision-Language Models, Agents,\nVLMs, LLMs, AI",
    "authors": [
      "Ranjan Sapkota",
      "Konstantinos I Roumeliotis",
      "Rahul Harsha Cheppally",
      "Marco Flores Calero",
      "Manoj Karkee"
    ],
    "publication_date": "2025-04-25T23:27:26Z",
    "arxiv_id": "http://arxiv.org/abs/2504.18738v1",
    "download_url": "http://arxiv.org/abs/2504.18738v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Hypergraph Vision Transformers: Images are More than Nodes, More than\n  Edges",
    "abstract": "Recent advancements in computer vision have highlighted the scalability of\nVision Transformers (ViTs) across various tasks, yet challenges remain in\nbalancing adaptability, computational efficiency, and the ability to model\nhigher-order relationships. Vision Graph Neural Networks (ViGs) offer an\nalternative by leveraging graph-based methodologies but are hindered by the\ncomputational bottlenecks of clustering algorithms used for edge generation. To\naddress these issues, we propose the Hypergraph Vision Transformer (HgVT),\nwhich incorporates a hierarchical bipartite hypergraph structure into the\nvision transformer framework to capture higher-order semantic relationships\nwhile maintaining computational efficiency. HgVT leverages population and\ndiversity regularization for dynamic hypergraph construction without\nclustering, and expert edge pooling to enhance semantic extraction and\nfacilitate graph-based image retrieval. Empirical results demonstrate that HgVT\nachieves strong performance on image classification and retrieval, positioning\nit as an efficient framework for semantic-based vision tasks.",
    "authors": [
      "Joshua Fixelle"
    ],
    "publication_date": "2025-04-11T17:20:26Z",
    "arxiv_id": "http://arxiv.org/abs/2504.08710v1",
    "download_url": "http://arxiv.org/abs/2504.08710v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "From Structured to Unstructured:A Comparative Analysis of Computer\n  Vision and Graph Models in solving Mesh-based PDEs",
    "abstract": "This article investigates the application of computer vision and graph-based\nmodels in solving mesh-based partial differential equations within\nhigh-performance computing environments. Focusing on structured, graded\nstructured, and unstructured meshes, the study compares the performance and\ncomputational efficiency of three computer vision-based models against three\ngraph-based models across three data\\-sets. The research aims to identify the\nmost suitable models for different mesh topographies, particularly highlighting\nthe exploration of graded meshes, a less studied area. Results demonstrate that\ncomputer vision-based models, notably U-Net, outperform the graph models in\nprediction performance and efficiency in two (structured and graded) out of\nthree mesh topographies. The study also reveals the unexpected effectiveness of\ncomputer vision-based models in handling unstructured meshes, suggesting a\npotential shift in methodological approaches for data-driven partial\ndifferential equation learning. The article underscores deep learning as a\nviable and potentially sustainable way to enhance traditional high-performance\ncomputing methods, advocating for informed model selection based on the\ntopography of the mesh.",
    "authors": [
      "Jens Decke",
      "Olaf Wünsch",
      "Bernhard Sick",
      "Christian Gruhl"
    ],
    "publication_date": "2024-05-31T12:21:26Z",
    "arxiv_id": "http://arxiv.org/abs/2406.00081v1",
    "download_url": "http://arxiv.org/abs/2406.00081v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  }
]