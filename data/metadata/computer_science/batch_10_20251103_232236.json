[
  {
    "title": "NLP for The Greek Language: A Longer Survey",
    "abstract": "English language is in the spotlight of the Natural Language Processing (NLP)\ncommunity with other languages, like Greek, lagging behind in terms of offered\nmethods, tools and resources. Due to the increasing interest in NLP, in this\npaper we try to condense research efforts for the automatic processing of Greek\nlanguage covering the last three decades. In particular, we list and briefly\ndiscuss related works, resources and tools, categorized according to various\nprocessing layers and contexts. We are not restricted to the modern form of\nGreek language but also cover Ancient Greek and various Greek dialects. This\nsurvey can be useful for researchers and students interested in NLP tasks,\nInformation Retrieval and Knowledge Management for the Greek language.",
    "authors": [
      "Katerina Papantoniou",
      "Yannis Tzitzikas"
    ],
    "publication_date": "2024-08-20T15:57:18Z",
    "arxiv_id": "http://arxiv.org/abs/2408.10962v1",
    "download_url": "http://arxiv.org/abs/2408.10962v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Fuzzy Temporal Protoforms for the Quantitative Description of Processes\n  in Natural Language",
    "abstract": "In this paper, we propose a series of fuzzy temporal protoforms in the\nframework of the automatic generation of quantitative and qualitative natural\nlanguage descriptions of processes. The model includes temporal and causal\ninformation from processes and attributes, quantifies attributes in time during\nthe process life-span and recalls causal relations and temporal distances\nbetween events, among other features. Through integrating process mining\ntechniques and fuzzy sets within the usual Data-to-Text architecture, our\nframework is able to extract relevant quantitative temporal as well as\nstructural information from a process and describe it in natural language\ninvolving uncertain terms. A real use-case in the cardiology domain is\npresented, showing the potential of our model for providing natural language\nexplanations addressed to domain experts.",
    "authors": [
      "Yago Fontenla-Seco",
      "Alberto Bugarín-Diz",
      "Manuel Lama"
    ],
    "publication_date": "2023-05-16T14:59:38Z",
    "arxiv_id": "http://arxiv.org/abs/2305.09506v1",
    "download_url": "http://arxiv.org/abs/2305.09506v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "PyThaiNLP: Thai Natural Language Processing in Python",
    "abstract": "We present PyThaiNLP, a free and open-source natural language processing\n(NLP) library for Thai language implemented in Python. It provides a wide range\nof software, models, and datasets for Thai language. We first provide a brief\nhistorical context of tools for Thai language prior to the development of\nPyThaiNLP. We then outline the functionalities it provided as well as datasets\nand pre-trained language models. We later summarize its development milestones\nand discuss our experience during its development. We conclude by demonstrating\nhow industrial and research communities utilize PyThaiNLP in their work. The\nlibrary is freely available at https://github.com/pythainlp/pythainlp.",
    "authors": [
      "Wannaphong Phatthiyaphaibun",
      "Korakot Chaovavanich",
      "Charin Polpanumas",
      "Arthit Suriyawongkul",
      "Lalita Lowphansirikul",
      "Pattarawat Chormai",
      "Peerat Limkonchotiwat",
      "Thanathip Suntorntip",
      "Can Udomcharoenchaikit"
    ],
    "publication_date": "2023-12-07T19:19:43Z",
    "arxiv_id": "http://arxiv.org/abs/2312.04649v1",
    "download_url": "http://arxiv.org/abs/2312.04649v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Gap Analysis of Natural Language Processing Systems with respect to\n  Linguistic Modality",
    "abstract": "Modality is one of the important components of grammar in linguistics. It\nlets speaker to express attitude towards, or give assessment or potentiality of\nstate of affairs. It implies different senses and thus has different\nperceptions as per the context. This paper presents an account showing the gap\nin the functionality of the current state of art Natural Language Processing\n(NLP) systems. The contextual nature of linguistic modality is studied. In this\npaper, the works and logical approaches employed by Natural Language Processing\nsystems dealing with modality are reviewed. It sees human cognition and\nintelligence as multi-layered approach that can be implemented by intelligent\nsystems for learning. Lastly, current flow of research going on within this\nfield is talked providing futurology.",
    "authors": [
      "Vishal Shukla"
    ],
    "publication_date": "2015-04-18T13:28:59Z",
    "arxiv_id": "http://arxiv.org/abs/1504.04716v1",
    "download_url": "http://arxiv.org/abs/1504.04716v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Composition by Conversation",
    "abstract": "Most musical programming languages are developed purely for coding virtual\ninstruments or algorithmic compositions. Although there has been some work in\nthe domain of musical query languages for music information retrieval, there\nhas been little attempt to unify the principles of musical programming and\nquery languages with cognitive and natural language processing models that\nwould facilitate the activity of composition by conversation. We present a\nprototype framework, called MusECI, that merges these domains, permitting\nscore-level algorithmic composition in a text editor while also supporting\nconnectivity to existing natural language processing frameworks.",
    "authors": [
      "Donya Quick",
      "Clayton T. Morrison"
    ],
    "publication_date": "2017-09-07T05:39:00Z",
    "arxiv_id": "http://arxiv.org/abs/1709.02076v1",
    "download_url": "http://arxiv.org/abs/1709.02076v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Generalisation of language and knowledge models for corpus analysis",
    "abstract": "This paper takes new look on language and knowledge modelling for corpus\nlinguistics. Using ideas of Chaitin, a line of argument is made against\nlanguage/knowledge separation in Natural Language Processing. A simplistic\nmodel, that generalises approaches to language and knowledge, is proposed. One\nof hypothetical consequences of this model is Strong AI.",
    "authors": [
      "Anton Loss"
    ],
    "publication_date": "2012-03-14T22:06:42Z",
    "arxiv_id": "http://arxiv.org/abs/1203.3227v1",
    "download_url": "http://arxiv.org/abs/1203.3227v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Implementation of nlization framework for verbs, pronouns and\n  determiners with eugene",
    "abstract": "UNL system is designed and implemented by a nonprofit organization, UNDL\nFoundation at Geneva in 1999. UNL applications are application softwares that\nallow end users to accomplish natural language tasks, such as translating,\nsummarizing, retrieving or extracting information, etc. Two major web based\napplication softwares are Interactive ANalyzer (IAN), which is a natural\nlanguage analysis system. It represents natural language sentences as semantic\nnetworks in the UNL format. Other application software is dEep-to-sUrface\nGENErator (EUGENE), which is an open-source interactive NLizer. It generates\nnatural language sentences out of semantic networks represented in the UNL\nformat. In this paper, NLization framework with EUGENE is focused, while using\nUNL system for accomplishing the task of machine translation. In whole\nNLization process, EUGENE takes a UNL input and delivers an output in natural\nlanguage without any human intervention. It is language-independent and has to\nbe parametrized to the natural language input through a dictionary and a\ngrammar, provided as separate interpretable files. In this paper, it is\nexplained that how UNL input is syntactically and semantically analyzed with\nthe UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns\nand determiners for Punjabi natural language.",
    "authors": [
      "Harinder Singh",
      "Parteek Kumar"
    ],
    "publication_date": "2013-09-10T12:03:32Z",
    "arxiv_id": "http://arxiv.org/abs/1309.2471v1",
    "download_url": "http://arxiv.org/abs/1309.2471v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "NL-Debugging: Exploiting Natural Language as an Intermediate\n  Representation for Code Debugging",
    "abstract": "Debugging is a critical aspect of LLM's coding ability. Early debugging\nefforts primarily focused on code-level analysis, which often falls short when\naddressing complex programming errors that require a deeper understanding of\nalgorithmic logic. Recent advancements in large language models (LLMs) have\nshifted attention toward leveraging natural language reasoning to enhance\ncode-related tasks. However, two fundamental questions remain unanswered: What\ntype of natural language format is most effective for debugging tasks? And what\nspecific benefits does natural language reasoning bring to the debugging\nprocess? In this paper, we introduce NL-DEBUGGING, a novel framework that\nemploys natural language as an intermediate representation to improve code\ndebugging. By debugging at a natural language level, we demonstrate that\nNL-DEBUGGING outperforms traditional debugging methods and enables a broader\nmodification space through direct refinement guided by execution feedback. Our\nfindings highlight the potential of natural language reasoning to advance\nautomated code debugging and address complex programming challenges.",
    "authors": [
      "Weiming Zhang",
      "Qingyao Li",
      "Xinyi Dai",
      "Jizheng Chen",
      "Kounianhua Du",
      "Weiwen Liu",
      "Yasheng Wang",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "publication_date": "2025-05-21T10:38:50Z",
    "arxiv_id": "http://arxiv.org/abs/2505.15356v2",
    "download_url": "http://arxiv.org/abs/2505.15356v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset",
    "abstract": "JamPatoisNLI provides the first dataset for natural language inference in a\ncreole language, Jamaican Patois. Many of the most-spoken low-resource\nlanguages are creoles. These languages commonly have a lexicon derived from a\nmajor world language and a distinctive grammar reflecting the languages of the\noriginal speakers and the process of language birth by creolization. This gives\nthem a distinctive place in exploring the effectiveness of transfer from large\nmonolingual or multilingual pretrained models. While our work, along with\nprevious work, shows that transfer from these models to low-resource languages\nthat are unrelated to languages in their training set is not very effective, we\nwould expect stronger results from transfer to creoles. Indeed, our experiments\nshow considerably better results from few-shot learning of JamPatoisNLI than\nfor such unrelated languages, and help us begin to understand how the unique\nrelationship between creoles and their high-resource base languages affect\ncross-lingual transfer. JamPatoisNLI, which consists of naturally-occurring\npremises and expert-written hypotheses, is a step towards steering research\ninto a traditionally underserved language and a useful benchmark for\nunderstanding cross-lingual NLP.",
    "authors": [
      "Ruth-Ann Armstrong",
      "John Hewitt",
      "Christopher Manning"
    ],
    "publication_date": "2022-12-07T03:07:02Z",
    "arxiv_id": "http://arxiv.org/abs/2212.03419v1",
    "download_url": "http://arxiv.org/abs/2212.03419v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Evaluating Computational Language Models with Scaling Properties of\n  Natural Language",
    "abstract": "In this article, we evaluate computational models of natural language with\nrespect to the universal statistical behaviors of natural language. Statistical\nmechanical analyses have revealed that natural language text is characterized\nby scaling properties, which quantify the global structure in the vocabulary\npopulation and the long memory of a text. We study whether five scaling\nproperties (given by Zipf's law, Heaps' law, Ebeling's method, Taylor's law,\nand long-range correlation analysis) can serve for evaluation of computational\nmodels. Specifically, we test $n$-gram language models, a probabilistic\ncontext-free grammar (PCFG), language models based on Simon/Pitman-Yor\nprocesses, neural language models, and generative adversarial networks (GANs)\nfor text generation. Our analysis reveals that language models based on\nrecurrent neural networks (RNNs) with a gating mechanism (i.e., long short-term\nmemory, LSTM; a gated recurrent unit, GRU; and quasi-recurrent neural networks,\nQRNNs) are the only computational models that can reproduce the long memory\nbehavior of natural language. Furthermore, through comparison with recently\nproposed model-based evaluation methods, we find that the exponent of Taylor's\nlaw is a good indicator of model quality.",
    "authors": [
      "Shuntaro Takahashi",
      "Kumiko Tanaka-Ishii"
    ],
    "publication_date": "2019-06-22T03:24:32Z",
    "arxiv_id": "http://arxiv.org/abs/1906.09379v1",
    "download_url": "http://arxiv.org/abs/1906.09379v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Consolidating and Developing Benchmarking Datasets for the Nepali\n  Natural Language Understanding Tasks",
    "abstract": "The Nepali language has distinct linguistic features, especially its complex\nscript (Devanagari script), morphology, and various dialects,which pose a\nunique challenge for Natural Language Understanding (NLU) tasks. While the\nNepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a\nfoundation for evaluating models, it remains limited in scope, covering four\ntasks. This restricts their utility for comprehensive assessments of Natural\nLanguage Processing (NLP) models. To address this limitation, we introduce\ntwelve new datasets, creating a new benchmark, the Nepali /Language\nUnderstanding Evaluation (NLUE) benchmark for evaluating the performance of\nmodels across a diverse set of Natural Language Understanding (NLU) tasks. The\nadded tasks include Single-Sentence Classification, Similarity and Paraphrase\nTasks, Natural Language Inference (NLI), and General Masked Evaluation Task\n(GMET). Through extensive experiments, we demonstrate that existing top models\nstruggle with the added complexity of these tasks. We also find that the best\nmultilingual model outperforms the best monolingual models across most tasks,\nhighlighting the need for more robust solutions tailored to the Nepali\nlanguage. This expanded benchmark sets a new standard for evaluating,\ncomparing, and advancing models, contributing significantly to the broader goal\nof advancing NLP research for low-resource languages.",
    "authors": [
      "Jinu Nyachhyon",
      "Mridul Sharma",
      "Prajwal Thapa",
      "Bal Krishna Bal"
    ],
    "publication_date": "2024-11-28T16:32:02Z",
    "arxiv_id": "http://arxiv.org/abs/2411.19244v2",
    "download_url": "http://arxiv.org/abs/2411.19244v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Integrating Linguistic Theory and Neural Language Models",
    "abstract": "Transformer-based language models have recently achieved remarkable results\nin many natural language tasks. However, performance on leaderboards is\ngenerally achieved by leveraging massive amounts of training data, and rarely\nby encoding explicit linguistic knowledge into neural models. This has led many\nto question the relevance of linguistics for modern natural language\nprocessing. In this dissertation, I present several case studies to illustrate\nhow theoretical linguistics and neural language models are still relevant to\neach other. First, language models are useful to linguists by providing an\nobjective tool to measure semantic distance, which is difficult to do using\ntraditional methods. On the other hand, linguistic theory contributes to\nlanguage modelling research by providing frameworks and sources of data to\nprobe our language models for specific aspects of language understanding.\n  This thesis contributes three studies that explore different aspects of the\nsyntax-semantics interface in language models. In the first part of my thesis,\nI apply language models to the problem of word class flexibility. Using mBERT\nas a source of semantic distance measurements, I present evidence in favour of\nanalyzing word class flexibility as a directional process. In the second part\nof my thesis, I propose a method to measure surprisal at intermediate layers of\nlanguage models. My experiments show that sentences containing morphosyntactic\nanomalies trigger surprisals earlier in language models than semantic and\ncommonsense anomalies. Finally, in the third part of my thesis, I adapt several\npsycholinguistic studies to show that language models contain knowledge of\nargument structure constructions. In summary, my thesis develops new\nconnections between natural language processing, linguistic theory, and\npsycholinguistics to provide fresh perspectives for the interpretation of\nlanguage models.",
    "authors": [
      "Bai Li"
    ],
    "publication_date": "2022-07-20T04:20:46Z",
    "arxiv_id": "http://arxiv.org/abs/2207.09643v1",
    "download_url": "http://arxiv.org/abs/2207.09643v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Chart-driven Connectionist Categorial Parsing of Spoken Korean",
    "abstract": "While most of the speech and natural language systems which were developed\nfor English and other Indo-European languages neglect the morphological\nprocessing and integrate speech and natural language at the word level, for the\nagglutinative languages such as Korean and Japanese, the morphological\nprocessing plays a major role in the language processing since these languages\nhave very complex morphological phenomena and relatively simple syntactic\nfunctionality. Obviously degenerated morphological processing limits the usable\nvocabulary size for the system and word-level dictionary results in exponential\nexplosion in the number of dictionary entries. For the agglutinative languages,\nwe need sub-word level integration which leaves rooms for general morphological\nprocessing. In this paper, we developed a phoneme-level integration model of\nspeech and linguistic processings through general morphological analysis for\nagglutinative languages and a efficient parsing scheme for that integration.\nKorean is modeled lexically based on the categorial grammar formalism with\nunordered argument and suppressed category extensions, and chart-driven\nconnectionist parsing method is introduced.",
    "authors": [
      "WonIl Lee",
      "Geunbae Lee",
      "Jong-Hyeok Lee"
    ],
    "publication_date": "1995-11-29T05:19:16Z",
    "arxiv_id": "http://arxiv.org/abs/cmp-lg/9511005v1",
    "download_url": "http://arxiv.org/abs/cmp-lg/9511005v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Cedille: A large autoregressive French language model",
    "abstract": "Scaling up the size and training of autoregressive language models has\nenabled novel ways of solving Natural Language Processing tasks using zero-shot\nand few-shot learning. While extreme-scale language models such as GPT-3 offer\nmultilingual capabilities, zero-shot learning for languages other than English\nremain largely unexplored. Here, we introduce Cedille, a large open source\nauto-regressive language model, specifically trained for the French language.\nOur results show that Cedille outperforms existing French language models and\nis competitive with GPT-3 on a range of French zero-shot benchmarks.\nFurthermore, we provide an in-depth comparison of the toxicity exhibited by\nthese models, showing that Cedille marks an improvement in language model\nsafety thanks to dataset filtering.",
    "authors": [
      "Martin Müller",
      "Florian Laurent"
    ],
    "publication_date": "2022-02-07T17:40:43Z",
    "arxiv_id": "http://arxiv.org/abs/2202.03371v1",
    "download_url": "http://arxiv.org/abs/2202.03371v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "SKOPE: A connectionist/symbolic architecture of spoken Korean processing",
    "abstract": "Spoken language processing requires speech and natural language integration.\nMoreover, spoken Korean calls for unique processing methodology due to its\nlinguistic characteristics. This paper presents SKOPE, a connectionist/symbolic\nspoken Korean processing engine, which emphasizes that: 1) connectionist and\nsymbolic techniques must be selectively applied according to their relative\nstrength and weakness, and 2) the linguistic characteristics of Korean must be\nfully considered for phoneme recognition, speech and language integration, and\nmorphological/syntactic processing. The design and implementation of SKOPE\ndemonstrates how connectionist/symbolic hybrid architectures can be constructed\nfor spoken agglutinative language processing. Also SKOPE presents many novel\nideas for speech and language processing. The phoneme recognition,\nmorphological analysis, and syntactic analysis experiments show that SKOPE is a\nviable approach for the spoken Korean processing.",
    "authors": [
      "Geunbae Lee",
      "Jong-Hyeok Lee"
    ],
    "publication_date": "1995-04-07T14:39:09Z",
    "arxiv_id": "http://arxiv.org/abs/cmp-lg/9504008v2",
    "download_url": "http://arxiv.org/abs/cmp-lg/9504008v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Benchmarking Language Models for Code Syntax Understanding",
    "abstract": "Pre-trained language models have demonstrated impressive performance in both\nnatural language processing and program understanding, which represent the\ninput as a token sequence without explicitly modeling its structure. Some prior\nworks show that pre-trained language models can capture the syntactic rules of\nnatural languages without finetuning on syntax understanding tasks. However,\nthere is limited understanding of how well pre-trained models understand the\ncode structure so far. In this work, we perform the first thorough benchmarking\nof the state-of-the-art pre-trained models for identifying the syntactic\nstructures of programs. Specifically, we introduce CodeSyntax, a large-scale\ndataset of programs annotated with the syntactic relationships in their\ncorresponding abstract syntax trees. Our key observation is that existing\nlanguage models pretrained on code still lack the understanding of code syntax.\nIn fact, these pre-trained programming language models fail to match the\nperformance of simple baselines based on positional offsets and keywords. We\nalso present a natural language benchmark to highlight the differences between\nnatural languages and programming languages in terms of syntactic structure\nunderstanding. Our findings point out key limitations of existing pre-training\nmethods for programming languages, and suggest the importance of modeling code\nsyntactic structures.",
    "authors": [
      "Da Shen",
      "Xinyun Chen",
      "Chenguang Wang",
      "Koushik Sen",
      "Dawn Song"
    ],
    "publication_date": "2022-10-26T04:47:18Z",
    "arxiv_id": "http://arxiv.org/abs/2210.14473v1",
    "download_url": "http://arxiv.org/abs/2210.14473v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Architecture of an Ontology-Based Domain-Specific Natural Language\n  Question Answering System",
    "abstract": "Question answering (QA) system aims at retrieving precise information from a\nlarge collection of documents against a query. This paper describes the\narchitecture of a Natural Language Question Answering (NLQA) system for a\nspecific domain based on the ontological information, a step towards semantic\nweb question answering. The proposed architecture defines four basic modules\nsuitable for enhancing current QA capabilities with the ability of processing\ncomplex questions. The first module was the question processing, which analyses\nand classifies the question and also reformulates the user query. The second\nmodule allows the process of retrieving the relevant documents. The next module\nprocesses the retrieved documents, and the last module performs the extraction\nand generation of a response. Natural language processing techniques are used\nfor processing the question and documents and also for answer extraction.\nOntology and domain knowledge are used for reformulating queries and\nidentifying the relations. The aim of the system is to generate short and\nspecific answer to the question that is asked in the natural language in a\nspecific domain. We have achieved 94 % accuracy of natural language question\nanswering in our implementation.",
    "authors": [
      "Athira P. M.",
      "Sreeja M.",
      "P. C. Reghu Raj"
    ],
    "publication_date": "2013-11-13T15:36:12Z",
    "arxiv_id": "http://arxiv.org/abs/1311.3175v1",
    "download_url": "http://arxiv.org/abs/1311.3175v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "The Dissecting Power of Regular Languages",
    "abstract": "A recent study on structural properties of regular and context-free languages\nhas greatly promoted our basic understandings of the complex behaviors of those\nlanguages. We continue the study to examine how regular languages behave when\nthey need to cut numerous infinite languages. A particular interest rests on a\nsituation in which a regular language needs to \"dissect\" a given infinite\nlanguage into two subsets of infinite size. Every context-free language is\ndissected by carefully chosen regular languages (or it is REG-dissectible). In\na larger picture, we show that constantly-growing languages and semi-linear\nlanguages are REG-dissectible. Under certain natural conditions, complements\nand finite intersections of semi-linear languages also become REG-dissectible.\nRestricted to bounded languages, the intersections of finitely many\ncontext-free languages and, more surprisingly, the entire Boolean hierarchy\nover bounded context-free languages are REG-dissectible. As an immediate\napplication of the REG-dissectibility, we show another structural property, in\nwhich an appropriate bounded context-free language can \"separate with infinite\nmargins\" two given nested infinite bounded context-free languages.",
    "authors": [
      "Tomoyuki Yamakami",
      "Yuichi Kato"
    ],
    "publication_date": "2012-02-22T11:16:47Z",
    "arxiv_id": "http://arxiv.org/abs/1202.4883v3",
    "download_url": "http://arxiv.org/abs/1202.4883v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Natural Language Interfaces for Tabular Data Querying and Visualization:\n  A Survey",
    "abstract": "The emergence of natural language processing has revolutionized the way users\ninteract with tabular data, enabling a shift from traditional query languages\nand manual plotting to more intuitive, language-based interfaces. The rise of\nlarge language models (LLMs) such as ChatGPT and its successors has further\nadvanced this field, opening new avenues for natural language processing\ntechniques. This survey presents a comprehensive overview of natural language\ninterfaces for tabular data querying and visualization, which allow users to\ninteract with data using natural language queries. We introduce the fundamental\nconcepts and techniques underlying these interfaces with a particular emphasis\non semantic parsing, the key technology facilitating the translation from\nnatural language to SQL queries or data visualization commands. We then delve\ninto the recent advancements in Text-to-SQL and Text-to-Vis problems from the\nperspectives of datasets, methodologies, metrics, and system designs. This\nincludes a deep dive into the influence of LLMs, highlighting their strengths,\nlimitations, and potential for future improvements. Through this survey, we aim\nto provide a roadmap for researchers and practitioners interested in developing\nand applying natural language interfaces for data interaction in the era of\nlarge language models.",
    "authors": [
      "Weixu Zhang",
      "Yifei Wang",
      "Yuanfeng Song",
      "Victor Junqiu Wei",
      "Yuxing Tian",
      "Yiyan Qi",
      "Jonathan H. Chan",
      "Raymond Chi-Wing Wong",
      "Haiqin Yang"
    ],
    "publication_date": "2023-10-27T05:01:20Z",
    "arxiv_id": "http://arxiv.org/abs/2310.17894v3",
    "download_url": "http://arxiv.org/abs/2310.17894v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Spontaneous Emerging Preference in Two-tower Language Model",
    "abstract": "The ever-growing size of the foundation language model has brought\nsignificant performance gains in various types of downstream tasks. With the\nexistence of side-effects brought about by the large size of the foundation\nlanguage model such as deployment cost, availability issues, and environmental\ncost, there is some interest in exploring other possible directions, such as a\ndivide-and-conquer scheme. In this paper, we are asking a basic question: are\nlanguage processes naturally dividable? We study this problem with a simple\ntwo-tower language model setting, where two language models with identical\nconfigurations are trained side-by-side cooperatively. With this setting, we\ndiscover the spontaneous emerging preference phenomenon, where some of the\ntokens are consistently better predicted by one tower while others by another\ntower. This phenomenon is qualitatively stable, regardless of model\nconfiguration and type, suggesting this as an intrinsic property of natural\nlanguage. This study suggests that interesting properties of natural language\nare still waiting to be discovered, which may aid the future development of\nnatural language processing techniques.",
    "authors": [
      "Zhengqi He",
      "Taro Toyoizumi"
    ],
    "publication_date": "2022-10-13T13:55:19Z",
    "arxiv_id": "http://arxiv.org/abs/2210.07041v1",
    "download_url": "http://arxiv.org/abs/2210.07041v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Cheetah: Natural Language Generation for 517 African Languages",
    "abstract": "Low-resource African languages pose unique challenges for natural language\nprocessing (NLP) tasks, including natural language generation (NLG). In this\npaper, we develop Cheetah, a massively multilingual NLG language model for\nAfrican languages. Cheetah supports 517 African languages and language\nvarieties, allowing us to address the scarcity of NLG resources and provide a\nsolution to foster linguistic diversity. We demonstrate the effectiveness of\nCheetah through comprehensive evaluations across six generation downstream\ntasks. In five of the six tasks, Cheetah significantly outperforms other\nmodels, showcasing its remarkable performance for generating coherent and\ncontextually appropriate text in a wide range of African languages. We\nadditionally conduct a detailed human evaluation to delve deeper into the\nlinguistic capabilities of Cheetah. The introduction of Cheetah has\nfar-reaching benefits for linguistic diversity. By leveraging pretrained models\nand adapting them to specific languages, our approach facilitates the\ndevelopment of practical NLG applications for African communities. The findings\nof this study contribute to advancing NLP research in low-resource settings,\nenabling greater accessibility and inclusion for African languages in a rapidly\nexpanding digital landscape. We publicly release our models for research.",
    "authors": [
      "Ife Adebara",
      "AbdelRahim Elmadany",
      "Muhammad Abdul-Mageed"
    ],
    "publication_date": "2024-01-02T06:24:13Z",
    "arxiv_id": "http://arxiv.org/abs/2401.01053v3",
    "download_url": "http://arxiv.org/abs/2401.01053v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Language understanding as a step towards human level intelligence -\n  automatizing the construction of the initial dictionary from example\n  sentences",
    "abstract": "For a system to understand natural language, it needs to be able to take\nnatural language text and answer questions given in natural language with\nrespect to that text; it also needs to be able to follow instructions given in\nnatural language. To achieve this, a system must be able to process natural\nlanguage and be able to capture the knowledge within that text. Thus it needs\nto be able to translate natural language text into a formal language. We\ndiscuss our approach to do this, where the translation is achieved by composing\nthe meaning of words in a sentence. Our initial approach uses an inverse lambda\nmethod that we developed (and other methods) to learn meaning of words from\nmeaning of sentences and an initial lexicon. We then present an improved method\nwhere the initial lexicon is also learned by analyzing the training sentence\nand meaning pairs. We evaluate our methods and compare them with other existing\nmethods on a corpora of database querying and robot command and control.",
    "authors": [
      "Chitta Baral",
      "Juraj Dzifcak"
    ],
    "publication_date": "2011-08-18T20:12:50Z",
    "arxiv_id": "http://arxiv.org/abs/1108.3848v1",
    "download_url": "http://arxiv.org/abs/1108.3848v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Do Large Language Models Speak All Languages Equally? A Comparative\n  Study in Low-Resource Settings",
    "abstract": "Large language models (LLMs) have garnered significant interest in natural\nlanguage processing (NLP), particularly their remarkable performance in various\ndownstream tasks in resource-rich languages. Recent studies have highlighted\nthe limitations of LLMs in low-resource languages, primarily focusing on binary\nclassification tasks and giving minimal attention to South Asian languages.\nThese limitations are primarily attributed to constraints such as dataset\nscarcity, computational costs, and research gaps specific to low-resource\nlanguages. To address this gap, we present datasets for sentiment and hate\nspeech tasks by translating from English to Bangla, Hindi, and Urdu,\nfacilitating research in low-resource language processing. Further, we\ncomprehensively examine zero-shot learning using multiple LLMs in English and\nwidely spoken South Asian languages. Our findings indicate that GPT-4\nconsistently outperforms Llama 2 and Gemini, with English consistently\ndemonstrating superior performance across diverse tasks compared to\nlow-resource languages. Furthermore, our analysis reveals that natural language\ninference (NLI) exhibits the highest performance among the evaluated tasks,\nwith GPT-4 demonstrating superior capabilities.",
    "authors": [
      "Md. Arid Hasan",
      "Prerona Tarannum",
      "Krishno Dey",
      "Imran Razzak",
      "Usman Naseem"
    ],
    "publication_date": "2024-08-05T05:09:23Z",
    "arxiv_id": "http://arxiv.org/abs/2408.02237v1",
    "download_url": "http://arxiv.org/abs/2408.02237v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Constraint Logic Programming for Natural Language Processing",
    "abstract": "This paper proposes an evaluation of the adequacy of the constraint logic\nprogramming paradigm for natural language processing. Theoretical aspects of\nthis question have been discussed in several works. We adopt here a pragmatic\npoint of view and our argumentation relies on concrete solutions. Using actual\ncontraints (in the CLP sense) is neither easy nor direct. However, CLP can\nimprove parsing techniques in several aspects such as concision, control,\nefficiency or direct representation of linguistic formalism. This discussion is\nillustrated by several examples and the presentation of an HPSG parser.",
    "authors": [
      "Philippe Blache",
      "Nabil Hathout"
    ],
    "publication_date": "1995-04-05T14:16:33Z",
    "arxiv_id": "http://arxiv.org/abs/cmp-lg/9504005v1",
    "download_url": "http://arxiv.org/abs/cmp-lg/9504005v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Architecture of a Web-based Predictive Editor for Controlled Natural\n  Language Processing",
    "abstract": "In this paper, we describe the architecture of a web-based predictive text\neditor being developed for the controlled natural language PENG$^{ASP)$. This\ncontrolled language can be used to write non-monotonic specifications that have\nthe same expressive power as Answer Set Programs. In order to support the\nwriting process of these specifications, the predictive text editor\ncommunicates asynchronously with the controlled natural language processor that\ngenerates lookahead categories and additional auxiliary information for the\nauthor of a specification text. The text editor can display multiple sets of\nlookahead categories simultaneously for different possible sentence\ncompletions, anaphoric expressions, and supports the addition of new content\nwords to the lexicon.",
    "authors": [
      "Stephen Guy",
      "Rolf Schwitter"
    ],
    "publication_date": "2014-06-27T01:00:59Z",
    "arxiv_id": "http://arxiv.org/abs/1408.0016v1",
    "download_url": "http://arxiv.org/abs/1408.0016v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Natural Language Processing: State of The Art, Current Trends and\n  Challenges",
    "abstract": "Natural language processing (NLP) has recently gained much attention for\nrepresenting and analysing human language computationally. It has spread its\napplications in various fields such as machine translation, email spam\ndetection, information extraction, summarization, medical, and question\nanswering etc. The paper distinguishes four phases by discussing different\nlevels of NLP and components of Natural Language Generation (NLG) followed by\npresenting the history and evolution of NLP, state of the art presenting the\nvarious applications of NLP and current trends and challenges.",
    "authors": [
      "Diksha Khurana",
      "Aditya Koli",
      "Kiran Khatter",
      "Sukhdev Singh"
    ],
    "publication_date": "2017-08-17T06:42:03Z",
    "arxiv_id": "http://arxiv.org/abs/1708.05148v1",
    "download_url": "http://arxiv.org/abs/1708.05148v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Natural Language Semantics and Computability",
    "abstract": "This paper is a reflexion on the computability of natural language semantics.\nIt does not contain a new model or new results in the formal semantics of\nnatural language: it is rather a computational analysis of the logical models\nand algorithms currently used in natural language semantics, defined as the\nmapping of a statement to logical formulas - formulas, because a statement can\nbe ambiguous. We argue that as long as possible world semantics is left out,\none can compute the semantic representation(s) of a given statement, including\naspects of lexical meaning. We also discuss the algorithmic complexity of this\nprocess.",
    "authors": [
      "Richard Moot",
      "Christian Retoré"
    ],
    "publication_date": "2016-05-13T10:46:22Z",
    "arxiv_id": "http://arxiv.org/abs/1605.04122v1",
    "download_url": "http://arxiv.org/abs/1605.04122v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Logical Semantics, Dialogical Argumentation, and Textual Entailment",
    "abstract": "In this chapter, we introduce a new dialogical system for first order\nclassical logic which is close to natural language argumentation, and we prove\nits completeness with respect to usual classical validity. We combine our\ndialogical system with the Grail syntactic and semantic parser developed by the\nsecond author in order to address automated textual entailment, that is, we use\nit for deciding whether or not a sentence is a consequence of a short text.\nThis work-which connects natural language semantics and argumentation with\ndialogical logic-can be viewed as a step towards an inferentialist view of\nnatural language semantics.",
    "authors": [
      "Davide Catta",
      "Richard Moot",
      "Christian Retoré"
    ],
    "publication_date": "2020-08-17T08:04:11Z",
    "arxiv_id": "http://arxiv.org/abs/2008.07138v1",
    "download_url": "http://arxiv.org/abs/2008.07138v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Defining and Evaluating Fair Natural Language Generation",
    "abstract": "Our work focuses on the biases that emerge in the natural language generation\n(NLG) task of sentence completion. In this paper, we introduce a framework of\nfairness for NLG followed by an evaluation of gender biases in two\nstate-of-the-art language models. Our analysis provides a theoretical\nformulation for biases in NLG and empirical evidence that existing language\ngeneration models embed gender bias.",
    "authors": [
      "Catherine Yeo",
      "Alyssa Chen"
    ],
    "publication_date": "2020-07-28T04:11:10Z",
    "arxiv_id": "http://arxiv.org/abs/2008.01548v1",
    "download_url": "http://arxiv.org/abs/2008.01548v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Linking Emergent and Natural Languages via Corpus Transfer",
    "abstract": "The study of language emergence aims to understand how human languages are\nshaped by perceptual grounding and communicative intent. Computational\napproaches to emergent communication (EC) predominantly consider referential\ngames in limited domains and analyze the learned protocol within the game\nframework. As a result, it remains unclear how the emergent languages from\nthese settings connect to natural languages or provide benefits in real-world\nlanguage processing tasks, where statistical models trained on large text\ncorpora dominate. In this work, we propose a novel way to establish such a link\nby corpus transfer, i.e. pretraining on a corpus of emergent language for\ndownstream natural language tasks, which is in contrast to prior work that\ndirectly transfers speaker and listener parameters. Our approach showcases\nnon-trivial transfer benefits for two different tasks -- language modeling and\nimage captioning. For example, in a low-resource setup (modeling 2 million\nnatural language tokens), pre-training on an emergent language corpus with just\n2 million tokens reduces model perplexity by $24.6\\%$ on average across ten\nnatural languages. We also introduce a novel metric to predict the\ntransferability of an emergent language by translating emergent messages to\nnatural language captions grounded on the same images. We find that our\ntranslation-based metric highly correlates with the downstream performance on\nmodeling natural languages (for instance $\\rho=0.83$ on Hebrew), while\ntopographic similarity, a popular metric in previous work, shows surprisingly\nlow correlation ($\\rho=0.003$), hinting that simple properties like attribute\ndisentanglement from synthetic domains might not capture the full complexities\nof natural language. Our findings also indicate potential benefits of moving\nlanguage emergence forward with natural language resources and models.",
    "authors": [
      "Shunyu Yao",
      "Mo Yu",
      "Yang Zhang",
      "Karthik R Narasimhan",
      "Joshua B. Tenenbaum",
      "Chuang Gan"
    ],
    "publication_date": "2022-03-24T21:24:54Z",
    "arxiv_id": "http://arxiv.org/abs/2203.13344v1",
    "download_url": "http://arxiv.org/abs/2203.13344v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Large Language Models on the Chessboard: A Study on ChatGPT's Formal\n  Language Comprehension and Complex Reasoning Skills",
    "abstract": "While large language models have made strides in natural language processing,\ntheir proficiency in complex reasoning tasks requiring formal language\ncomprehension, such as chess, remains less investigated. This paper probes the\nperformance of ChatGPT, a sophisticated language model by OpenAI in tackling\nsuch complex reasoning tasks, using chess as a case study. Through robust\nmetrics examining both the legality and quality of moves, we assess ChatGPT's\nunderstanding of the chessboard, adherence to chess rules, and strategic\ndecision-making abilities. Our evaluation identifies limitations within\nChatGPT's attention mechanism that affect its formal language comprehension and\nuncovers the model's underdeveloped self-regulation abilities. Our study also\nreveals ChatGPT's propensity for a coherent strategy in its gameplay and a\nnoticeable uptick in decision-making assertiveness when the model is presented\nwith a greater volume of natural language or possesses a more lucid\nunderstanding of the state of the chessboard. These findings contribute to the\ngrowing exploration of language models' abilities beyond natural language\nprocessing, providing valuable information for future research towards models\ndemonstrating human-like cognitive abilities.",
    "authors": [
      "Mu-Tien Kuo",
      "Chih-Chung Hsueh",
      "Richard Tzong-Han Tsai"
    ],
    "publication_date": "2023-08-29T08:36:30Z",
    "arxiv_id": "http://arxiv.org/abs/2308.15118v1",
    "download_url": "http://arxiv.org/abs/2308.15118v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Persian Natural Language Inference: A Meta-learning approach",
    "abstract": "Incorporating information from other languages can improve the results of\ntasks in low-resource languages. A powerful method of building functional\nnatural language processing systems for low-resource languages is to combine\nmultilingual pre-trained representations with cross-lingual transfer learning.\nIn general, however, shared representations are learned separately, either\nacross tasks or across languages. This paper proposes a meta-learning approach\nfor inferring natural language in Persian. Alternately, meta-learning uses\ndifferent task information (such as QA in Persian) or other language\ninformation (such as natural language inference in English). Also, we\ninvestigate the role of task augmentation strategy for forming additional\nhigh-quality tasks. We evaluate the proposed method using four languages and an\nauxiliary task. Compared to the baseline approach, the proposed model\nconsistently outperforms it, improving accuracy by roughly six percent. We also\nexamine the effect of finding appropriate initial parameters using zero-shot\nevaluation and CCA similarity.",
    "authors": [
      "Heydar Soudani",
      "Mohammad Hassan Mojab",
      "Hamid Beigy"
    ],
    "publication_date": "2022-05-18T06:51:58Z",
    "arxiv_id": "http://arxiv.org/abs/2205.08755v1",
    "download_url": "http://arxiv.org/abs/2205.08755v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A State of the Art of Word Sense Induction: A Way Towards Word Sense\n  Disambiguation for Under-Resourced Languages",
    "abstract": "Word Sense Disambiguation (WSD), the process of automatically identifying the\nmeaning of a polysemous word in a sentence, is a fundamental task in Natural\nLanguage Processing (NLP). Progress in this approach to WSD opens up many\npromising developments in the field of NLP and its applications. Indeed,\nimprovement over current performance levels could allow us to take a first step\ntowards natural language understanding. Due to the lack of lexical resources it\nis sometimes difficult to perform WSD for under-resourced languages. This paper\nis an investigation on how to initiate research in WSD for under-resourced\nlanguages by applying Word Sense Induction (WSI) and suggests some interesting\ntopics to focus on.",
    "authors": [
      "Mohammad Nasiruddin"
    ],
    "publication_date": "2013-10-05T00:33:46Z",
    "arxiv_id": "http://arxiv.org/abs/1310.1425v1",
    "download_url": "http://arxiv.org/abs/1310.1425v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Teaching natural language to computers",
    "abstract": "\"Natural Language,\" whether spoken and attended to by humans, or processed\nand generated by computers, requires networked structures that reflect creative\nprocesses in semantic, syntactic, phonetic, linguistic, social, emotional, and\ncultural modules. Being able to produce novel and useful behavior following\nrepeated practice gets to the root of both artificial intelligence and human\nlanguage. This paper investigates the modalities involved in language-like\napplications that computers -- and programmers -- engage with, and aims to fine\ntune the questions we ask to better account for context, self-awareness, and\nembodiment.",
    "authors": [
      "Joseph Corneli",
      "Miriam Corneli"
    ],
    "publication_date": "2016-04-29T11:36:25Z",
    "arxiv_id": "http://arxiv.org/abs/1604.08781v2",
    "download_url": "http://arxiv.org/abs/1604.08781v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Automatic Code Generation using Pre-Trained Language Models",
    "abstract": "Recent advancements in natural language processing \\cite{gpt2} \\cite{BERT}\nhave led to near-human performance in multiple natural language tasks. In this\npaper, we seek to understand whether similar techniques can be applied to a\nhighly structured environment with strict syntax rules. Specifically, we\npropose an end-to-end machine learning model for code generation in the Python\nlanguage built on-top of pre-trained language models. We demonstrate that a\nfine-tuned model can perform well in code generation tasks, achieving a BLEU\nscore of 0.22, an improvement of 46\\% over a reasonable sequence-to-sequence\nbaseline. All results and related code used for training and data processing\nare available on GitHub.",
    "authors": [
      "Luis Perez",
      "Lizi Ottens",
      "Sudharshan Viswanathan"
    ],
    "publication_date": "2021-02-21T07:21:26Z",
    "arxiv_id": "http://arxiv.org/abs/2102.10535v1",
    "download_url": "http://arxiv.org/abs/2102.10535v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Racial Disparity in Natural Language Processing: A Case Study of Social\n  Media African-American English",
    "abstract": "We highlight an important frontier in algorithmic fairness: disparity in the\nquality of natural language processing algorithms when applied to language from\nauthors of different social groups. For example, current systems sometimes\nanalyze the language of females and minorities more poorly than they do of\nwhites and males. We conduct an empirical analysis of racial disparity in\nlanguage identification for tweets written in African-American English, and\ndiscuss implications of disparity in NLP.",
    "authors": [
      "Su Lin Blodgett",
      "Brendan O'Connor"
    ],
    "publication_date": "2017-06-30T22:57:50Z",
    "arxiv_id": "http://arxiv.org/abs/1707.00061v1",
    "download_url": "http://arxiv.org/abs/1707.00061v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "LangPro: Natural Language Theorem Prover",
    "abstract": "LangPro is an automated theorem prover for natural language\n(https://github.com/kovvalsky/LangPro). Given a set of premises and a\nhypothesis, it is able to prove semantic relations between them. The prover is\nbased on a version of analytic tableau method specially designed for natural\nlogic. The proof procedure operates on logical forms that preserve linguistic\nexpressions to a large extent. %This property makes the logical forms easily\nobtainable from syntactic trees. %, in particular, Combinatory Categorial\nGrammar derivation trees. The nature of proofs is deductive and transparent. On\nthe FraCaS and SICK textual entailment datasets, the prover achieves high\nresults comparable to state-of-the-art.",
    "authors": [
      "Lasha Abzianidze"
    ],
    "publication_date": "2017-08-30T18:22:28Z",
    "arxiv_id": "http://arxiv.org/abs/1708.09417v1",
    "download_url": "http://arxiv.org/abs/1708.09417v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Stopwords in Technical Language Processing",
    "abstract": "There are increasingly applications of natural language processing techniques\nfor information retrieval, indexing and topic modelling in the engineering\ncontexts. A standard component of such tasks is the removal of stopwords, which\nare uninformative components of the data. While researchers use readily\navailable stopword lists which are derived for general English language, the\ntechnical jargon of engineering fields contains their own highly frequent and\nuninformative words and there exists no standard stopword list for technical\nlanguage processing applications. Here we address this gap by rigorously\nidentifying generic, insignificant, uninformative stopwords in engineering\ntexts beyond the stopwords in general texts, based on the synthesis of\nalternative data-driven approaches, and curating a stopword list ready for\ntechnical language processing applications.",
    "authors": [
      "Serhad Sarica",
      "Jianxi Luo"
    ],
    "publication_date": "2020-06-04T03:52:59Z",
    "arxiv_id": "http://arxiv.org/abs/2006.02633v1",
    "download_url": "http://arxiv.org/abs/2006.02633v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Framework for Processing Textual Descriptions of Business Processes\n  using a Constrained Language -- Technical Report",
    "abstract": "This report explores how (potentially constrained) natural language can be\nused to enable non-experts to develop process models by simply describing\nscenarios in plain text. To this end, a framework, called BeePath, is proposed.\nIt allows users to write process descriptions in a constrained pattern-based\nlanguage, which can then be translated into formal models such as Petri nets\nand DECLARE. The framework also leverages large language models (LLMs) to help\nconvert unstructured descriptions into this constrained language.",
    "authors": [
      "Andrea Burattin",
      "Antonio Grama",
      "Ana-Maria Sima",
      "Andrey Rivkin",
      "Barbara Weber"
    ],
    "publication_date": "2025-08-13T15:08:42Z",
    "arxiv_id": "http://arxiv.org/abs/2508.15799v1",
    "download_url": "http://arxiv.org/abs/2508.15799v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Building low-resource African language corpora: A case study of\n  Kidawida, Kalenjin and Dholuo",
    "abstract": "Natural Language Processing is a crucial frontier in artificial intelligence,\nwith broad applications in many areas, including public health, agriculture,\neducation, and commerce. However, due to the lack of substantial linguistic\nresources, many African languages remain underrepresented in this digital\ntransformation. This paper presents a case study on the development of\nlinguistic corpora for three under-resourced Kenyan languages, Kidaw'ida,\nKalenjin, and Dholuo, with the aim of advancing natural language processing and\nlinguistic research in African communities. Our project, which lasted one year,\nemployed a selective crowd-sourcing methodology to collect text and speech data\nfrom native speakers of these languages. Data collection involved (1) recording\nconversations and translation of the resulting text into Kiswahili, thereby\ncreating parallel corpora, and (2) reading and recording written texts to\ngenerate speech corpora. We made these resources freely accessible via\nopen-research platforms, namely Zenodo for the parallel text corpora and\nMozilla Common Voice for the speech datasets, thus facilitating ongoing\ncontributions and access for developers to train models and develop Natural\nLanguage Processing applications. The project demonstrates how grassroots\nefforts in corpus building can support the inclusion of African languages in\nartificial intelligence innovations. In addition to filling resource gaps,\nthese corpora are vital in promoting linguistic diversity and empowering local\ncommunities by enabling Natural Language Processing applications tailored to\ntheir needs. As African countries like Kenya increasingly embrace digital\ntransformation, developing indigenous language resources becomes essential for\ninclusive growth. We encourage continued collaboration from native speakers and\ndevelopers to expand and utilize these corpora.",
    "authors": [
      "Audrey Mbogho",
      "Quin Awuor",
      "Andrew Kipkebut",
      "Lilian Wanzare",
      "Vivian Oloo"
    ],
    "publication_date": "2025-01-19T10:17:21Z",
    "arxiv_id": "http://arxiv.org/abs/2501.11003v1",
    "download_url": "http://arxiv.org/abs/2501.11003v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Indian Legal NLP Benchmarks : A Survey",
    "abstract": "Availability of challenging benchmarks is the key to advancement of AI in a\nspecific field.Since Legal Text is significantly different than normal English\ntext, there is a need to create separate Natural Language Processing benchmarks\nfor Indian Legal Text which are challenging and focus on tasks specific to\nLegal Systems. This will spur innovation in applications of Natural language\nProcessing for Indian Legal Text and will benefit AI community and Legal\nfraternity. We review the existing work in this area and propose ideas to\ncreate new benchmarks for Indian Legal Natural Language Processing.",
    "authors": [
      "Prathamesh Kalamkar",
      "Janani Venugopalan Ph. D.",
      "Vivek Raghavan Ph. D"
    ],
    "publication_date": "2021-07-13T13:10:10Z",
    "arxiv_id": "http://arxiv.org/abs/2107.06056v1",
    "download_url": "http://arxiv.org/abs/2107.06056v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Natural Language Processing for Financial Regulation",
    "abstract": "This article provides an understanding of Natural Language Processing\ntechniques in the framework of financial regulation, more specifically in order\nto perform semantic matching search between rules and policy when no dataset is\navailable for supervised learning. We outline how to outperform simple\npre-trained sentences-transformer models using freely available resources and\nexplain the mathematical concepts behind the key building blocks of Natural\nLanguage Processing.",
    "authors": [
      "Ixandra Achitouv",
      "Dragos Gorduza",
      "Antoine Jacquier"
    ],
    "publication_date": "2023-11-14T20:58:21Z",
    "arxiv_id": "http://arxiv.org/abs/2311.08533v1",
    "download_url": "http://arxiv.org/abs/2311.08533v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Understanding scholarly Natural Language Processing system diagrams\n  through application of the Richards-Engelhardt framework",
    "abstract": "We utilise Richards-Engelhardt framework as a tool for understanding Natural\nLanguage Processing systems diagrams. Through four examples from scholarly\nproceedings, we find that the application of the framework to this ecological\nand complex domain is effective for reflecting on these diagrams. We argue for\nvocabulary to describe multiple-codings, semiotic variability, and\ninconsistency or misuse of visual encoding principles in diagrams. Further, for\napplication to scholarly Natural Language Processing systems, and perhaps\nsystems diagrams more broadly, we propose the addition of \"Grouping by Object\"\nas a new visual encoding principle, and \"Emphasising\" as a new visual encoding\ntype.",
    "authors": [
      "Guy Clarke Marshall",
      "Caroline Jay",
      "André Freitas"
    ],
    "publication_date": "2020-08-26T20:06:30Z",
    "arxiv_id": "http://arxiv.org/abs/2008.11785v1",
    "download_url": "http://arxiv.org/abs/2008.11785v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Are Style Guides Controlled Languages? The Case of Koenig & Bauer AG",
    "abstract": "Controlled natural languages for industrial application are often regarded as\na response to the challenges of translation and multilingual communication.\nThis paper presents a quite different approach taken by Koenig & Bauer AG,\nwhere the main goal was the improvement of the authoring process for technical\ndocumentation. Most importantly, this paper explores the notion of a controlled\nlanguage and demonstrates how style guides can emerge from non-linguistic\nconsiderations. Moreover, it shows the transition from loose language\nrecommendations into precise and prescriptive rules and investigates whether\nsuch rules can be regarded as a full-fledged controlled language.",
    "authors": [
      "Karolina Suchowolec"
    ],
    "publication_date": "2014-06-13T09:23:53Z",
    "arxiv_id": "http://arxiv.org/abs/1406.3460v1",
    "download_url": "http://arxiv.org/abs/1406.3460v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Embedded Controlled Languages",
    "abstract": "Inspired by embedded programming languages, an embedded CNL (controlled\nnatural language) is a proper fragment of an entire natural language (its host\nlanguage), but it has a parser that recognizes the entire host language. This\nmakes it possible to process out-of-CNL input and give useful feedback to\nusers, instead of just reporting syntax errors. This extended abstract explains\nthe main concepts of embedded CNL implementation in GF (Grammatical Framework),\nwith examples from machine translation and some other ongoing work.",
    "authors": [
      "Aarne Ranta"
    ],
    "publication_date": "2014-06-16T16:11:32Z",
    "arxiv_id": "http://arxiv.org/abs/1406.4057v1",
    "download_url": "http://arxiv.org/abs/1406.4057v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Syntax Representation in Word Embeddings and Neural Networks -- A Survey",
    "abstract": "Neural networks trained on natural language processing tasks capture syntax\neven though it is not provided as a supervision signal. This indicates that\nsyntactic analysis is essential to the understating of language in artificial\nintelligence systems. This overview paper covers approaches of evaluating the\namount of syntactic information included in the representations of words for\ndifferent neural network architectures. We mainly summarize re-search on\nEnglish monolingual data on language modeling tasks and multilingual data for\nneural machine translation systems and multilingual language models. We\ndescribe which pre-trained models and representations of language are best\nsuited for transfer to syntactic tasks.",
    "authors": [
      "Tomasz Limisiewicz",
      "David Mareček"
    ],
    "publication_date": "2020-10-02T15:44:58Z",
    "arxiv_id": "http://arxiv.org/abs/2010.01063v1",
    "download_url": "http://arxiv.org/abs/2010.01063v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Exploring Chemical Space using Natural Language Processing Methodologies\n  for Drug Discovery",
    "abstract": "Text-based representations of chemicals and proteins can be thought of as\nunstructured languages codified by humans to describe domain-specific\nknowledge. Advances in natural language processing (NLP) methodologies in the\nprocessing of spoken languages accelerated the application of NLP to elucidate\nhidden knowledge in textual representations of these biochemical entities and\nthen use it to construct models to predict molecular properties or to design\nnovel molecules. This review outlines the impact made by these advances on drug\ndiscovery and aims to further the dialogue between medicinal chemists and\ncomputer scientists.",
    "authors": [
      "Hakime Öztürk",
      "Arzucan Özgür",
      "Philippe Schwaller",
      "Teodoro Laino",
      "Elif Ozkirimli"
    ],
    "publication_date": "2020-02-10T21:02:05Z",
    "arxiv_id": "http://arxiv.org/abs/2002.06053v1",
    "download_url": "http://arxiv.org/abs/2002.06053v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in\n  Logical Fallacy Detection",
    "abstract": "Translating natural language into formal language such as First-Order Logic\n(FOL) is a foundational challenge in NLP with wide-ranging applications in\nautomated reasoning, misinformation tracking, and knowledge validation. In this\npaper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework\nto autoformalize natural language to FOL step by step using Large Language\nModels (LLMs). Our approach addresses key challenges in this translation\nprocess, including the integration of implicit background knowledge. By\nleveraging structured representations generated by NL2FOL, we use\nSatisfiability Modulo Theory (SMT) solvers to reason about the logical validity\nof natural language statements. We present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach\nalso provides interpretable insights into the reasoning process and\ndemonstrates robustness without requiring model fine-tuning or labeled training\ndata. Our framework achieves strong performance on multiple datasets. On the\nLOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing\neffectively to the LOGICCLIMATE dataset with an F1-score of 80%.",
    "authors": [
      "Abhinav Lalwani",
      "Tasha Kim",
      "Lovish Chopra",
      "Christopher Hahn",
      "Zhijing Jin",
      "Mrinmaya Sachan"
    ],
    "publication_date": "2024-04-18T00:20:48Z",
    "arxiv_id": "http://arxiv.org/abs/2405.02318v3",
    "download_url": "http://arxiv.org/abs/2405.02318v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A perspective on the advancement of natural language processing tasks\n  via topological analysis of complex networks",
    "abstract": "Comment on \"Approaching human language with complex networks\" by Cong and Liu\n(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).",
    "authors": [
      "Diego R. Amancio"
    ],
    "publication_date": "2014-12-03T14:37:36Z",
    "arxiv_id": "http://arxiv.org/abs/1412.1342v1",
    "download_url": "http://arxiv.org/abs/1412.1342v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Special-Character Adversarial Attacks on Open-Source Language Model",
    "abstract": "Large language models (LLMs) have achieved remarkable performance across\ndiverse natural language processing tasks, yet their vulnerability to\ncharacter-level adversarial manipulations presents significant security\nchallenges for real-world deployments.",
    "authors": [
      "Ephraiem Sarabamoun"
    ],
    "publication_date": "2025-08-12T03:42:59Z",
    "arxiv_id": "http://arxiv.org/abs/2508.14070v1",
    "download_url": "http://arxiv.org/abs/2508.14070v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  }
]