[
  {
    "title": "MHfit: Mobile Health Data for Predicting Athletics Fitness Using Machine Learning",
    "abstract": "Mobile phones and other electronic gadgets or devices have aided in collecting data without the need for data entry. This paper will specifically focus on Mobile health data. Mobile health data use mobile devices to gather clinical health data and track patient vitals in real-time. Our study is aimed to give decisions for small or big sports teams on whether one athlete good fit or not for a particular game with the compare several machine learning algorithms to predict human behavior and health using the data collected from mobile devices and sensors placed on patients. In this study, we have obtained the dataset from a similar study done on mhealth. The dataset contains vital signs recordings of ten volunteers from different backgrounds. They had to perform several physical activities with a sensor placed on their bodies. Our study used 5 machine learning algorithms (XGBoost, Naive Bayes, Decision Tree, Random Forest, and Logistic Regression) to analyze and predict human health behavior. XGBoost performed better compared to the other machine learning algorithms and achieved 95.2% accuracy, 99.5% in sensitivity, 99.5% in specificity, and 99.66% in F1 score. Our research indicated a promising future in mhealth being used to predict human behavior and further research and exploration need to be done for it to be available for commercial use specifically in the sports industry.",
    "authors": [
      "Jonayet Miah",
      "Muntasir Mamun",
      "Md Minhazur Rahman",
      "Md Ishtyaq Mahmud",
      "Sabbir Ahmed",
      "Md Hasan Bin Nasir"
    ],
    "publication_date": "2023-04-10T19:53:03Z",
    "arxiv_id": "http://arxiv.org/abs/2304.04839v2",
    "download_url": "https://arxiv.org/abs/2304.04839v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Efficient Learning of Restricted Boltzmann Machines Using Covariance Estimates",
    "abstract": "Learning RBMs using standard algorithms such as CD(k) involves gradient descent on the negative log-likelihood. One of the terms in the gradient, which involves expectation w.r.t. the model distribution, is intractable and is obtained through an MCMC estimate. In this work we show that the Hessian of the log-likelihood can be written in terms of covariances of hidden and visible units and hence, all elements of the Hessian can also be estimated using the same MCMC samples with small extra computational costs. Since inverting the Hessian may be computationally expensive, we propose an algorithm that uses inverse of the diagonal approximation of the Hessian, instead. This essentially results in parameter-specific adaptive learning rates for the gradient descent process and improves the efficiency of learning RBMs compared to the standard methods. Specifically we show that using the inverse of diagonal approximation of Hessian in the stochastic DC (difference of convex functions) program approach results in very efficient learning of RBMs.",
    "authors": [
      "Vidyadhar Upadhya",
      "P. S. Sastry"
    ],
    "publication_date": "2018-10-25T08:51:19Z",
    "arxiv_id": "http://arxiv.org/abs/1810.10777v2",
    "download_url": "https://arxiv.org/abs/1810.10777v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective",
    "abstract": "As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of whether and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we formalize and study the disagreement problem in explainable machine learning. More specifically, we define the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how practitioners resolve these disagreements. We first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and six different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that (1) state-of-the-art explanation methods often disagree in terms of the explanations they output, and (2) machine learning practitioners often employ ad hoc heuristics when resolving such disagreements. These findings suggest that practitioners may be relying on misleading explanations when making consequential decisions. They also underscore the importance of developing principled frameworks for effectively evaluating and comparing explanations output by various explanation techniques.",
    "authors": [
      "Satyapriya Krishna",
      "Tessa Han",
      "Alex Gu",
      "Steven Wu",
      "Shahin Jabbari",
      "Himabindu Lakkaraju"
    ],
    "publication_date": "2022-02-03T14:19:23Z",
    "arxiv_id": "http://arxiv.org/abs/2202.01602v6",
    "download_url": "https://arxiv.org/abs/2202.01602v6",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Adaptive Second Order Coresets for Data-efficient Machine Learning",
    "abstract": "Training machine learning models on massive datasets incurs substantial computational costs. To alleviate such costs, there has been a sustained effort to develop data-efficient training methods that can carefully select subsets of the training examples that generalize on par with the full training data. However, existing methods are limited in providing theoretical guarantees for the quality of the models trained on the extracted subsets, and may perform poorly in practice. We propose AdaCore, a method that leverages the geometry of the data to extract subsets of the training examples for efficient machine learning. The key idea behind our method is to dynamically approximate the curvature of the loss function via an exponentially-averaged estimate of the Hessian to select weighted subsets (coresets) that provide a close approximation of the full gradient preconditioned with the Hessian. We prove rigorous guarantees for the convergence of various first and second-order methods applied to the subsets chosen by AdaCore. Our extensive experiments show that AdaCore extracts coresets with higher quality compared to baselines and speeds up training of convex and non-convex machine learning models, such as logistic regression and neural networks, by over 2.9x over the full data and 4.5x over random subsets.",
    "authors": [
      "Omead Pooladzandi",
      "David Davini",
      "Baharan Mirzasoleiman"
    ],
    "publication_date": "2022-07-28T05:43:09Z",
    "arxiv_id": "http://arxiv.org/abs/2207.13887v1",
    "download_url": "https://arxiv.org/abs/2207.13887v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning Insulin-Glucose Dynamics in the Wild",
    "abstract": "We develop a new model of insulin-glucose dynamics for forecasting blood glucose in type 1 diabetics. We augment an existing biomedical model by introducing time-varying dynamics driven by a machine learning sequence model. Our model maintains a physiologically plausible inductive bias and clinically interpretable parameters -- e.g., insulin sensitivity -- while inheriting the flexibility of modern pattern recognition algorithms. Critical to modeling success are the flexible, but structured representations of subject variability with a sequence model. In contrast, less constrained models like the LSTM fail to provide reliable or physiologically plausible forecasts. We conduct an extensive empirical study. We show that allowing biomedical model dynamics to vary in time improves forecasting at long time horizons, up to six hours, and produces forecasts consistent with the physiological effects of insulin and carbohydrates.",
    "authors": [
      "Andrew C. Miller",
      "Nicholas J. Foti",
      "Emily Fox"
    ],
    "publication_date": "2020-08-06T19:47:00Z",
    "arxiv_id": "http://arxiv.org/abs/2008.02852v1",
    "download_url": "https://arxiv.org/abs/2008.02852v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Outlier Robust Extreme Learning Machine for Multi-Target Regression",
    "abstract": "The popularity of algorithms based on Extreme Learning Machine (ELM), which can be used to train Single Layer Feedforward Neural Networks (SLFN), has increased in the past years. They have been successfully applied to a wide range of classification and regression tasks. The most commonly used methods are the ones based on minimizing the $\\ell_2$ norm of the error, which is not suitable to deal with outliers, essentially in regression tasks. The use of $\\ell_1$ norm was proposed in Outlier Robust ELM (OR-ELM), which is defined to one-dimensional outputs. In this paper, we generalize OR-ELM to deal with multi-target regression problems, using the error $\\ell_{2,1}$ norm and the Elastic Net theory, which can result in a more sparse network, resulting in our method, Generalized Outlier Robust ELM (GOR-ELM). We use Alternating Direction Method of Multipliers (ADMM) to solve the resulting optimization problem. An incremental version of GOR-ELM is also proposed. We chose 15 public real-world multi-target regression datasets to test our methods. Our conducted experiments show that they are statistically better than other ELM-based techniques, when considering data contaminated with outliers, and equivalent to them, otherwise.",
    "authors": [
      "Bruno Légora Souza da Silva",
      "Fernando Kentaro Inaba",
      "Evandro Ottoni Teatini Salles",
      "Patrick Marques Ciarelli"
    ],
    "publication_date": "2019-05-22T21:16:55Z",
    "arxiv_id": "http://arxiv.org/abs/1905.09368v2",
    "download_url": "https://arxiv.org/abs/1905.09368v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Prediction of the outcome of a Twenty-20 Cricket Match : A Machine Learning Approach",
    "abstract": "Twenty20 cricket, sometimes written Twenty-20, and often abbreviated to T20, is a short form of cricket. In a Twenty20 game the two teams of 11 players have a single innings each, which is restricted to a maximum of 20 overs. This version of cricket is especially unpredictable and is one of the reasons it has gained popularity over recent times. However, in this paper we try four different machine learning approaches for predicting the results of T20 Cricket Matches. Specifically we take in to account: previous performance statistics of the players involved in the competing teams, ratings of players obtained from reputed cricket statistics websites, clustering the players' with similar performance statistics and propose a novel method using an ELO based approach to rate players. We compare the performances of each of these feature engineering approaches by using different ML algorithms, including logistic regression, support vector machines, bayes network, decision tree, random forest.",
    "authors": [
      "Ashish V Shenoy",
      "Arjun Singhvi",
      "Shruthi Racha",
      "Srinivas Tunuguntla"
    ],
    "publication_date": "2022-09-13T23:43:36Z",
    "arxiv_id": "http://arxiv.org/abs/2209.06346v2",
    "download_url": "https://arxiv.org/abs/2209.06346v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Don't Waste Data: Transfer Learning to Leverage All Data for Machine-Learnt Climate Model Emulation",
    "abstract": "How can we learn from all available data when training machine-learnt climate models, without incurring any extra cost at simulation time? Typically, the training data comprises coarse-grained high-resolution data. But only keeping this coarse-grained data means the rest of the high-resolution data is thrown out. We use a transfer learning approach, which can be applied to a range of machine learning models, to leverage all the high-resolution data. We use three chaotic systems to show it stabilises training, gives improved generalisation performance and results in better forecasting skill. Our code is at https://github.com/raghul-parthipan/dont_waste_data",
    "authors": [
      "Raghul Parthipan",
      "Damon J. Wischik"
    ],
    "publication_date": "2022-10-08T11:51:12Z",
    "arxiv_id": "http://arxiv.org/abs/2210.04001v2",
    "download_url": "https://arxiv.org/abs/2210.04001v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Elements of effective machine learning datasets in astronomy",
    "abstract": "In this work, we identify elements of effective machine learning datasets in astronomy and present suggestions for their design and creation. Machine learning has become an increasingly important tool for analyzing and understanding the large-scale flood of data in astronomy. To take advantage of these tools, datasets are required for training and testing. However, building machine learning datasets for astronomy can be challenging. Astronomical data is collected from instruments built to explore science questions in a traditional fashion rather than to conduct machine learning. Thus, it is often the case that raw data, or even downstream processed data is not in a form amenable to machine learning. We explore the construction of machine learning datasets and we ask: what elements define effective machine learning datasets? We define effective machine learning datasets in astronomy to be formed with well-defined data points, structure, and metadata. We discuss why these elements are important for astronomical applications and ways to put them in practice. We posit that these qualities not only make the data suitable for machine learning, they also help to foster usable, reusable, and replicable science practices.",
    "authors": [
      "Bernie Boscoe",
      "Tuan Do",
      "Evan Jones",
      "Yunqi Li",
      "Kevin Alfaro",
      "Christy Ma"
    ],
    "publication_date": "2022-11-25T23:37:24Z",
    "arxiv_id": "http://arxiv.org/abs/2211.14401v2",
    "download_url": "https://arxiv.org/abs/2211.14401v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine Learning-Based Cloud Computing Compliance Process Automation",
    "abstract": "Cloud computing adoption across industries has revolutionized enterprise operations while introducing significant challenges in compliance management. Organizations must continuously meet evolving regulatory requirements such as GDPR and ISO 27001, yet traditional manual review processes have become increasingly inadequate for modern business scales. This paper presents a novel machine learning-based framework for automating cloud computing compliance processes, addressing critical challenges including resource-intensive manual reviews, extended compliance cycles, and delayed risk identification. Our proposed framework integrates multiple machine learning technologies, including BERT-based document processing (94.5% accuracy), One-Class SVM for anomaly detection (88.7% accuracy), and an improved CNN-LSTM architecture for sequential compliance data analysis (90.2% accuracy). Implementation results demonstrate significant improvements: reducing compliance process duration from 7 days to 1.5 days, improving accuracy from 78% to 93%, and decreasing manual effort by 73.3%. A real-world deployment at a major securities firm validated these results, processing 800,000 daily transactions with 94.2% accuracy in risk identification.",
    "authors": [
      "Yuqing Wang",
      "Xiao Yang"
    ],
    "publication_date": "2025-02-22T20:18:21Z",
    "arxiv_id": "http://arxiv.org/abs/2502.16344v1",
    "download_url": "https://arxiv.org/abs/2502.16344v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning Decision Policies with Instrumental Variables through Double Machine Learning",
    "abstract": "A common issue in learning decision-making policies in data-rich settings is spurious correlations in the offline dataset, which can be caused by hidden confounders. Instrumental variable (IV) regression, which utilises a key unconfounded variable known as the instrument, is a standard technique for learning causal relationships between confounded action, outcome, and context variables. Most recent IV regression algorithms use a two-stage approach, where a deep neural network (DNN) estimator learnt in the first stage is directly plugged into the second stage, in which another DNN is used to estimate the causal effect. Naively plugging the estimator can cause heavy bias in the second stage, especially when regularisation bias is present in the first stage estimator. We propose DML-IV, a non-linear IV regression method that reduces the bias in two-stage IV regressions and effectively learns high-performing policies. We derive a novel learning objective to reduce bias and design the DML-IV algorithm following the double/debiased machine learning (DML) framework. The learnt DML-IV estimator has strong convergence rate and $O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is unconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV regression benchmarks and learns high-performing policies in the presence of instruments.",
    "authors": [
      "Daqian Shao",
      "Ashkan Soleymani",
      "Francesco Quinzan",
      "Marta Kwiatkowska"
    ],
    "publication_date": "2024-05-14T10:55:04Z",
    "arxiv_id": "http://arxiv.org/abs/2405.08498v3",
    "download_url": "https://arxiv.org/abs/2405.08498v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Debugging Machine Learning Pipelines",
    "abstract": "Machine learning tasks entail the use of complex computational pipelines to reach quantitative and qualitative conclusions. If some of the activities in a pipeline produce erroneous or uninformative outputs, the pipeline may fail or produce incorrect results. Inferring the root cause of failures and unexpected behavior is challenging, usually requiring much human thought, and is both time-consuming and error-prone. We propose a new approach that makes use of iteration and provenance to automatically infer the root causes and derive succinct explanations of failures. Through a detailed experimental evaluation, we assess the cost, precision, and recall of our approach compared to the state of the art. Our source code and experimental data will be available for reproducibility and enhancement.",
    "authors": [
      "Raoni Lourenço",
      "Juliana Freire",
      "Dennis Shasha"
    ],
    "publication_date": "2020-02-11T19:13:12Z",
    "arxiv_id": "http://arxiv.org/abs/2002.04640v1",
    "download_url": "https://arxiv.org/abs/2002.04640v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine Learning Global Simulation of Nonlocal Gravity Wave Propagation",
    "abstract": "Global climate models typically operate at a grid resolution of hundreds of kilometers and fail to resolve atmospheric mesoscale processes, e.g., clouds, precipitation, and gravity waves (GWs). Model representation of these processes and their sources is essential to the global circulation and planetary energy budget, but subgrid scale contributions from these processes are often only approximately represented in models using parameterizations. These parameterizations are subject to approximations and idealizations, which limit their capability and accuracy. The most drastic of these approximations is the \"single-column approximation\" which completely neglects the horizontal evolution of these processes, resulting in key biases in current climate models. With a focus on atmospheric GWs, we present the first-ever global simulation of atmospheric GW fluxes using machine learning (ML) models trained on the WINDSET dataset to emulate global GW emulation in the atmosphere, as an alternative to traditional single-column parameterizations. Using an Attention U-Net-based architecture trained on globally resolved GW momentum fluxes, we illustrate the importance and effectiveness of global nonlocality, when simulating GWs using data-driven schemes.",
    "authors": [
      "Aman Gupta",
      "Aditi Sheshadri",
      "Sujit Roy",
      "Vishal Gaur",
      "Manil Maskey",
      "Rahul Ramachandran"
    ],
    "publication_date": "2024-06-20T22:57:38Z",
    "arxiv_id": "http://arxiv.org/abs/2406.14775v2",
    "download_url": "https://arxiv.org/abs/2406.14775v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "High throughput screening with machine learning",
    "abstract": "This study assesses the efficiency of several popular machine learning approaches in the prediction of molecular binding affinity: CatBoost, Graph Attention Neural Network, and Bidirectional Encoder Representations from Transformers. The models were trained to predict binding affinities in terms of inhibition constants $K_i$ for pairs of proteins and small organic molecules. First two approaches use thoroughly selected physico-chemical features, while the third one is based on textual molecular representations - it is one of the first attempts to apply Transformer-based predictors for the binding affinity. We also discuss the visualization of attention layers within the Transformer approach in order to highlight the molecular sites responsible for interactions. All approaches are free from atomic spatial coordinates thus avoiding bias from known structures and being able to generalize for compounds with unknown conformations. The achieved accuracy for all suggested approaches prove their potential in high throughput screening.",
    "authors": [
      "Oleksandr Gurbych",
      "Maksym Druchok",
      "Dzvenymyra Yarish",
      "Sofiya Garkot"
    ],
    "publication_date": "2020-12-15T13:19:03Z",
    "arxiv_id": "http://arxiv.org/abs/2012.08275v1",
    "download_url": "https://arxiv.org/abs/2012.08275v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Relevance Vector Machines for harmonization of MRI brain volumes using image descriptors",
    "abstract": "With the increased need for multi-center magnetic resonance imaging studies, problems arise related to differences in hardware and software between centers. Namely, current algorithms for brain volume quantification are unreliable for the longitudinal assessment of volume changes in this type of setting. Currently most methods attempt to decrease this issue by regressing the scanner- and/or center-effects from the original data. In this work, we explore a novel approach to harmonize brain volume measurements by using only image descriptors. First, we explore the relationships between volumes and image descriptors. Then, we train a Relevance Vector Machine (RVM) model over a large multi-site dataset of healthy subjects to perform volume harmonization. Finally, we validate the method over two different datasets: i) a subset of unseen healthy controls; and ii) a test-retest dataset of multiple sclerosis (MS) patients. The method decreases scanner and center variability while preserving measurements that did not require correction in MS patient data. We show that image descriptors can be used as input to a machine learning algorithm to improve the reliability of longitudinal volumetric studies.",
    "authors": [
      "Maria Ines Meyer",
      "Ezequiel de la Rosa",
      "Koen Van Leemput",
      "Diana M. Sima"
    ],
    "publication_date": "2019-11-08T14:37:14Z",
    "arxiv_id": "http://arxiv.org/abs/1911.04289v1",
    "download_url": "https://arxiv.org/abs/1911.04289v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Privacy and Transparency in Graph Machine Learning: A Unified Perspective",
    "abstract": "Graph Machine Learning (GraphML), whereby classical machine learning is generalized to irregular graph domains, has enjoyed a recent renaissance, leading to a dizzying array of models and their applications in several domains. With its growing applicability to sensitive domains and regulations by governmental agencies for trustworthy AI systems, researchers have started looking into the issues of transparency and privacy of graph learning.\n  However, these topics have been mainly investigated independently. In this position paper, we provide a unified perspective on the interplay of privacy and transparency in GraphML. In particular, we describe the challenges and possible research directions for a formal investigation of privacy-transparency tradeoffs in GraphML.",
    "authors": [
      "Megha Khosla"
    ],
    "publication_date": "2022-07-22T06:18:33Z",
    "arxiv_id": "http://arxiv.org/abs/2207.10896v2",
    "download_url": "https://arxiv.org/abs/2207.10896v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Evolutionary Dynamic Optimization and Machine Learning",
    "abstract": "Evolutionary Computation (EC) has emerged as a powerful field of Artificial Intelligence, inspired by nature's mechanisms of gradual development. However, EC approaches often face challenges such as stagnation, diversity loss, computational complexity, population initialization, and premature convergence. To overcome these limitations, researchers have integrated learning algorithms with evolutionary techniques. This integration harnesses the valuable data generated by EC algorithms during iterative searches, providing insights into the search space and population dynamics. Similarly, the relationship between evolutionary algorithms and Machine Learning (ML) is reciprocal, as EC methods offer exceptional opportunities for optimizing complex ML tasks characterized by noisy, inaccurate, and dynamic objective functions. These hybrid techniques, known as Evolutionary Machine Learning (EML), have been applied at various stages of the ML process. EC techniques play a vital role in tasks such as data balancing, feature selection, and model training optimization. Moreover, ML tasks often require dynamic optimization, for which Evolutionary Dynamic Optimization (EDO) is valuable. This paper presents the first comprehensive exploration of reciprocal integration between EDO and ML. The study aims to stimulate interest in the evolutionary learning community and inspire innovative contributions in this domain.",
    "authors": [
      "Abdennour Boulesnane"
    ],
    "publication_date": "2023-10-12T22:28:53Z",
    "arxiv_id": "http://arxiv.org/abs/2310.08748v3",
    "download_url": "https://arxiv.org/abs/2310.08748v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Quantifying the Carbon Emissions of Machine Learning",
    "abstract": "From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.",
    "authors": [
      "Alexandre Lacoste",
      "Alexandra Luccioni",
      "Victor Schmidt",
      "Thomas Dandres"
    ],
    "publication_date": "2019-10-21T23:57:32Z",
    "arxiv_id": "http://arxiv.org/abs/1910.09700v2",
    "download_url": "https://arxiv.org/abs/1910.09700v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "FeDXL: Provable Federated Learning for Deep X-Risk Optimization",
    "abstract": "In this paper, we tackle a novel federated learning (FL) problem for optimizing a family of X-risks, to which no existing FL algorithms are applicable. In particular, the objective has the form of $\\mathbb E_{z\\sim S_1} f(\\mathbb E_{z'\\sim S_2} \\ell(w; z, z'))$, where two sets of data $S_1, S_2$ are distributed over multiple machines, $\\ell(\\cdot)$ is a pairwise loss that only depends on the prediction outputs of the input data pairs $(z, z')$, and $f(\\cdot)$ is possibly a non-linear non-convex function. This problem has important applications in machine learning, e.g., AUROC maximization with a pairwise loss, and partial AUROC maximization with a compositional loss. The challenges for designing an FL algorithm for X-risks lie in the non-decomposability of the objective over multiple machines and the interdependency between different machines. To this end, we propose an active-passive decomposition framework that decouples the gradient's components with two types, namely active parts and passive parts, where the active parts depend on local data that are computed with the local model and the passive parts depend on other machines that are communicated/computed based on historical models and samples. Under this framework, we develop two provable FL algorithms (FeDXL) for handling linear and nonlinear $f$, respectively, based on federated averaging and merging. We develop a novel theoretical analysis to combat the latency of the passive parts and the interdependency between the local model parameters and the involved data for computing local gradient estimators. We establish both iteration and communication complexities and show that using the historical samples and models for computing the passive parts do not degrade the complexities. We conduct empirical studies of FeDXL for deep AUROC and partial AUROC maximization, and demonstrate their performance compared with several baselines.",
    "authors": [
      "Zhishuai Guo",
      "Rong Jin",
      "Jiebo Luo",
      "Tianbao Yang"
    ],
    "publication_date": "2022-10-26T00:23:36Z",
    "arxiv_id": "http://arxiv.org/abs/2210.14396v4",
    "download_url": "https://arxiv.org/abs/2210.14396v4",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Online Learning to Rank in Stochastic Click Models",
    "abstract": "Online learning to rank is a core problem in information retrieval and machine learning. Many provably efficient algorithms have been recently proposed for this problem in specific click models. The click model is a model of how the user interacts with a list of documents. Though these results are significant, their impact on practice is limited, because all proposed algorithms are designed for specific click models and lack convergence guarantees in other models. In this work, we propose BatchRank, the first online learning to rank algorithm for a broad class of click models. The class encompasses two most fundamental click models, the cascade and position-based models. We derive a gap-dependent upper bound on the $T$-step regret of BatchRank and evaluate it on a range of web search queries. We observe that BatchRank outperforms ranked bandits and is more robust than CascadeKL-UCB, an existing algorithm for the cascade model.",
    "authors": [
      "Masrour Zoghi",
      "Tomas Tunys",
      "Mohammad Ghavamzadeh",
      "Branislav Kveton",
      "Csaba Szepesvari",
      "Zheng Wen"
    ],
    "publication_date": "2017-03-07T18:53:58Z",
    "arxiv_id": "http://arxiv.org/abs/1703.02527v2",
    "download_url": "https://arxiv.org/abs/1703.02527v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Quick Introduction to Quantum Machine Learning for Non-Practitioners",
    "abstract": "This paper provides an introduction to quantum machine learning, exploring the potential benefits of using quantum computing principles and algorithms that may improve upon classical machine learning approaches. Quantum computing utilizes particles governed by quantum mechanics for computational purposes, leveraging properties like superposition and entanglement for information representation and manipulation. Quantum machine learning applies these principles to enhance classical machine learning models, potentially reducing network size and training time on quantum hardware. The paper covers basic quantum mechanics principles, including superposition, phase space, and entanglement, and introduces the concept of quantum gates that exploit these properties. It also reviews classical deep learning concepts, such as artificial neural networks, gradient descent, and backpropagation, before delving into trainable quantum circuits as neural networks. An example problem demonstrates the potential advantages of quantum neural networks, and the appendices provide detailed derivations. The paper aims to help researchers new to quantum mechanics and machine learning develop their expertise more efficiently.",
    "authors": [
      "Ethan N. Evans",
      "Dominic Byrne",
      "Matthew G. Cook"
    ],
    "publication_date": "2024-02-22T16:48:17Z",
    "arxiv_id": "http://arxiv.org/abs/2402.14694v1",
    "download_url": "https://arxiv.org/abs/2402.14694v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Beyond the technical challenges for deploying Machine Learning solutions in a software company",
    "abstract": "Recently software development companies started to embrace Machine Learning (ML) techniques for introducing a series of advanced functionality in their products such as personalisation of the user experience, improved search, content recommendation and automation. The technical challenges for tackling these problems are heavily researched in literature. A less studied area is a pragmatic approach to the role of humans in a complex modern industrial environment where ML based systems are developed. Key stakeholders affect the system from inception and up to operation and maintenance. Product managers want to embed \"smart\" experiences for their users and drive the decisions on what should be built next; software engineers are challenged to build or utilise ML software tools that require skills that are well outside of their comfort zone; legal and risk departments may influence design choices and data access; operations teams are requested to maintain ML systems which are non-stationary in their nature and change behaviour over time; and finally ML practitioners should communicate with all these stakeholders to successfully build a reliable system. This paper discusses some of the challenges we faced in Atlassian as we started investing more in the ML space.",
    "authors": [
      "Ilias Flaounas"
    ],
    "publication_date": "2017-08-08T03:59:09Z",
    "arxiv_id": "http://arxiv.org/abs/1708.02363v1",
    "download_url": "https://arxiv.org/abs/1708.02363v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Explicit and data-Efficient Encoding via Gradient Flow",
    "abstract": "The autoencoder model typically uses an encoder to map data to a lower dimensional latent space and a decoder to reconstruct it. However, relying on an encoder for inversion can lead to suboptimal representations, particularly limiting in physical sciences where precision is key. We introduce a decoder-only method using gradient flow to directly encode data into the latent space, defined by ordinary differential equations (ODEs). This approach eliminates the need for approximate encoder inversion. We train the decoder via the adjoint method and show that costly integrals can be avoided with minimal accuracy loss. Additionally, we propose a $2^{nd}$ order ODE variant, approximating Nesterov's accelerated gradient descent for faster convergence. To handle stiff ODEs, we use an adaptive solver that prioritizes loss minimization, improving robustness. Compared to traditional autoencoders, our method demonstrates explicit encoding and superior data efficiency, which is crucial for data-scarce scenarios in the physical sciences. Furthermore, this work paves the way for integrating machine learning into scientific workflows, where precise and efficient encoding is critical. \\footnote{The code for this work is available at \\url{https://github.com/k-flouris/gfe}.}",
    "authors": [
      "Kyriakos Flouris",
      "Anna Volokitin",
      "Gustav Bredell",
      "Ender Konukoglu"
    ],
    "publication_date": "2024-12-01T15:54:50Z",
    "arxiv_id": "http://arxiv.org/abs/2412.00864v2",
    "download_url": "https://arxiv.org/abs/2412.00864v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Designing for the Long Tail of Machine Learning",
    "abstract": "Recent technical advances has made machine learning (ML) a promising component to include in end user facing systems. However, user experience (UX) practitioners face challenges in relating ML to existing user-centered design processes and how to navigate the possibilities and constraints of this design space. Drawing on our own experience, we characterize designing within this space as navigating trade-offs between data gathering, model development and designing valuable interactions for a given model performance. We suggest that the theoretical description of how machine learning performance scales with training data can guide designers in these trade-offs as well as having implications for prototyping. We exemplify the learning curve's usage by arguing that a useful pattern is to design an initial system in a bootstrap phase that aims to exploit the training effect of data collected at increasing orders of magnitude.",
    "authors": [
      "Martin Lindvall",
      "Jesper Molin"
    ],
    "publication_date": "2020-01-21T11:53:28Z",
    "arxiv_id": "http://arxiv.org/abs/2001.07455v1",
    "download_url": "https://arxiv.org/abs/2001.07455v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions",
    "abstract": "Deep learning has been effectively applied to many discrete optimization problems. However, learning-based scheduling on unrelated parallel machines remains particularly difficult to design. Not only do the numbers of jobs and machines vary, but each job-machine pair has a unique processing time, dynamically altering feature dimensions. We propose a novel approach with a neural network tailored for offline deterministic scheduling of arbitrary sizes on unrelated machines. The goal is to minimize a complex objective function that includes the makespan and the weighted tardiness of jobs and machines. Unlike existing online approaches, which process jobs sequentially, our method generates a complete schedule considering the entire input at once. The key contribution of this work lies in the sophisticated architecture of our model. By leveraging various NLP-inspired architectures, it effectively processes any number of jobs and machines with varying feature dimensions imposed by unrelated processing times. Our approach enables supervised training on small problem instances while demonstrating strong generalization to much larger scheduling environments. Trained and tested on instances with 8 jobs and 4 machines, costs were only 2.51% above optimal. Across all tested configurations of up to 100 jobs and 10 machines, our network consistently outperformed an advanced dispatching rule, which incurred 22.22% higher costs on average. As our method allows fast retraining with simulated data and adaptation to various scheduling conditions, we believe it has the potential to become a standard approach for learning-based scheduling on unrelated machines and similar problem environments.",
    "authors": [
      "Diego Hitzges",
      "Guillaume Sagnol"
    ],
    "publication_date": "2025-12-22T16:18:29Z",
    "arxiv_id": "http://arxiv.org/abs/2512.19527v1",
    "download_url": "https://arxiv.org/abs/2512.19527v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Detecting Moving Objects With Machine Learning",
    "abstract": "The scientific study of the Solar System's minor bodies ultimately starts with a search for those bodies. This chapter presents a review of the use of machine learning techniques to find moving objects, both natural and artificial, in astronomical imagery. After a short review of the classical non-machine learning techniques that are historically used, I review the relatively nascent machine learning literature, which can broadly be summarized into three categories: streak detection, detection of moving point sources in image sequences, and detection of moving sources in shift and stack searches. In most cases, convolutional neural networks are utilized, which is the obvious choice given the imagery nature of the inputs. In this chapter I present two example networks: a Residual Network I designed which is in use in various shift and stack searches, and a convolutional neural network that was designed for prediction of source brightnesses and their uncertainties in those same shift-stacks. In discussion of the literature and example networks, I discuss various pitfalls with the use of machine learning techniques, including a discussion on the important issue of overfitting. I discuss various pitfall associated with the use of machine learning techniques, and what I consider best practices to follow in the application of machine learning to a new problem, including methods for the creation of robust training sets, validation, and training to avoid overfitting.",
    "authors": [
      "Wesley C. Fraser"
    ],
    "publication_date": "2024-05-10T00:13:39Z",
    "arxiv_id": "http://arxiv.org/abs/2405.06148v1",
    "download_url": "https://arxiv.org/abs/2405.06148v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Nonlinear Least Squares for Large-Scale Machine Learning using Stochastic Jacobian Estimates",
    "abstract": "For large nonlinear least squares loss functions in machine learning we exploit the property that the number of model parameters typically exceeds the data in one batch. This implies a low-rank structure in the Hessian of the loss, which enables effective means to compute search directions. Using this property, we develop two algorithms that estimate Jacobian matrices and perform well when compared to state-of-the-art methods.",
    "authors": [
      "Johannes J. Brust"
    ],
    "publication_date": "2021-07-12T17:29:08Z",
    "arxiv_id": "http://arxiv.org/abs/2107.05598v1",
    "download_url": "https://arxiv.org/abs/2107.05598v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Challenges, Methods, Data -- a Survey of Machine Learning in Water Distribution Networks",
    "abstract": "Research on methods for planning and controlling water distribution networks gains increasing relevance as the availability of drinking water will decrease as a consequence of climate change. So far, the majority of approaches is based on hydraulics and engineering expertise. However, with the increasing availability of sensors, machine learning techniques constitute a promising tool. This work presents the main tasks in water distribution networks, discusses how they relate to machine learning and analyses how the particularities of the domain pose challenges to and can be leveraged by machine learning approaches. Besides, it provides a technical toolkit by presenting evaluation benchmarks and a structured survey of the exemplary task of leakage detection and localization.",
    "authors": [
      "Valerie Vaquet",
      "Fabian Hinder",
      "André Artelt",
      "Inaam Ashraf",
      "Janine Strotherm",
      "Jonas Vaquet",
      "Johannes Brinkrolf",
      "Barbara Hammer"
    ],
    "publication_date": "2024-10-16T11:21:07Z",
    "arxiv_id": "http://arxiv.org/abs/2410.12461v1",
    "download_url": "https://arxiv.org/abs/2410.12461v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Graphs in machine learning: an introduction",
    "abstract": "Graphs are commonly used to characterise interactions between objects of interest. Because they are based on a straightforward formalism, they are used in many scientific fields from computer science to historical sciences. In this paper, we give an introduction to some methods relying on graphs for learning. This includes both unsupervised and supervised methods. Unsupervised learning algorithms usually aim at visualising graphs in latent spaces and/or clustering the nodes. Both focus on extracting knowledge from graph topologies. While most existing techniques are only applicable to static graphs, where edges do not evolve through time, recent developments have shown that they could be extended to deal with evolving networks. In a supervised context, one generally aims at inferring labels or numerical values attached to nodes using both the graph and, when they are available, node characteristics. Balancing the two sources of information can be challenging, especially as they can disagree locally or globally. In both contexts, supervised and un-supervised, data can be relational (augmented with one or several global graphs) as described above, or graph valued. In this latter case, each object of interest is given as a full graph (possibly completed by other characteristics). In this context, natural tasks include graph clustering (as in producing clusters of graphs rather than clusters of nodes in a single graph), graph classification, etc. 1 Real networks One of the first practical studies on graphs can be dated back to the original work of Moreno [51] in the 30s. Since then, there has been a growing interest in graph analysis associated with strong developments in the modelling and the processing of these data. Graphs are now used in many scientific fields. In Biology [54, 2, 7], for instance, metabolic networks can describe pathways of biochemical reactions [41], while in social sciences networks are used to represent relation ties between actors [66, 56, 36, 34]. Other examples include powergrids [71] and the web [75]. Recently, networks have also been considered in other areas such as geography [22] and history [59, 39]. In machine learning, networks are seen as powerful tools to model problems in order to extract information from data and for prediction purposes. This is the object of this paper. For more complete surveys, we refer to [28, 62, 49, 45]. In this section, we introduce notations and highlight properties shared by most real networks. In Section 2, we then consider methods aiming at extracting information from a unique network. We will particularly focus on clustering methods where the goal is to find clusters of vertices. Finally, in Section 3, techniques that take a series of networks into account, where each network is",
    "authors": [
      "Pierre Latouche",
      "Fabrice Rossi"
    ],
    "publication_date": "2015-06-23T12:12:45Z",
    "arxiv_id": "http://arxiv.org/abs/1506.06962v1",
    "download_url": "https://arxiv.org/abs/1506.06962v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Quantum Machine Learning for Radio Astronomy",
    "abstract": "In this work we introduce a novel approach to the pulsar classification problem in time-domain radio astronomy using a Born machine, often referred to as a quantum neural network. Using a single-qubit architecture, we show that the pulsar classification problem maps well to the Bloch sphere and that comparable accuracies to more classical machine learning approaches are achievable. We introduce a novel single-qubit encoding for the pulsar data used in this work and show that this performs comparably to a multi-qubit QAOA encoding.",
    "authors": [
      "Mohammad Kordzanganeh",
      "Aydin Utting",
      "Anna Scaife"
    ],
    "publication_date": "2021-12-05T19:05:08Z",
    "arxiv_id": "http://arxiv.org/abs/2112.02655v2",
    "download_url": "https://arxiv.org/abs/2112.02655v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations",
    "abstract": "Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the $O(N \\log N)$ Cooley-Tukey FFT algorithm to machine precision, for dimensions $N$ up to $1024$. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points -- the first time a structured approach has done so -- with 4X faster inference speed and 40X fewer parameters.",
    "authors": [
      "Tri Dao",
      "Albert Gu",
      "Matthew Eichhorn",
      "Atri Rudra",
      "Christopher Ré"
    ],
    "publication_date": "2019-03-14T10:20:38Z",
    "arxiv_id": "http://arxiv.org/abs/1903.05895v2",
    "download_url": "https://arxiv.org/abs/1903.05895v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning Invariant Representations with Local Transformations",
    "abstract": "Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformation-invariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-the-art performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.",
    "authors": [
      "Kihyuk Sohn",
      "Honglak Lee"
    ],
    "publication_date": "2012-06-27T19:59:59Z",
    "arxiv_id": "http://arxiv.org/abs/1206.6418v1",
    "download_url": "https://arxiv.org/abs/1206.6418v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning Explainable Treatment Policies with Clinician-Informed Representations: A Practical Approach",
    "abstract": "Digital health interventions (DHIs) and remote patient monitoring (RPM) have shown great potential in improving chronic disease management through personalized care. However, barriers like limited efficacy and workload concerns hinder adoption of existing DHIs; while limited sample sizes and lack of interpretability limit the effectiveness and adoption of purely black-box algorithmic DHIs. In this paper, we address these challenges by developing a pipeline for learning explainable treatment policies for RPM-enabled DHIs. We apply our approach in the real-world setting of RPM using a DHI to improve glycemic control of youth with type 1 diabetes. Our main contribution is to reveal the importance of clinical domain knowledge in developing state and action representations for effective, efficient, and interpretable targeting policies. We observe that policies learned from clinician-informed representations are significantly more efficacious and efficient than policies learned from black-box representations. This work emphasizes the importance of collaboration between ML researchers and clinicians for developing effective DHIs in the real world.",
    "authors": [
      "Johannes O. Ferstad",
      "Emily B. Fox",
      "David Scheinker",
      "Ramesh Johari"
    ],
    "publication_date": "2024-11-26T16:32:08Z",
    "arxiv_id": "http://arxiv.org/abs/2411.17570v1",
    "download_url": "https://arxiv.org/abs/2411.17570v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Bregman-Hausdorff divergence: strengthening the connections between computational geometry and machine learning",
    "abstract": "The purpose of this paper is twofold. On a technical side, we propose an extension of the Hausdorff distance from metric spaces to spaces equipped with asymmetric distance measures. Specifically, we focus on the family of Bregman divergences, which includes the popular Kullback--Leibler divergence (also known as relative entropy).\n  As a proof of concept, we use the resulting Bregman--Hausdorff divergence to compare two collections of probabilistic predictions produced by different machine learning models trained using the relative entropy loss. The algorithms we propose are surprisingly efficient even for large inputs with hundreds of dimensions.\n  In addition to the introduction of this technical concept, we provide a survey. It outlines the basics of Bregman geometry, as well as computational geometry algorithms. We focus on algorithms that are compatible with this geometry and are relevant for machine learning.",
    "authors": [
      "Tuyen Pham",
      "Hana Dal Poz Kouřimská",
      "Hubert Wagner"
    ],
    "publication_date": "2025-04-09T22:42:29Z",
    "arxiv_id": "http://arxiv.org/abs/2504.07322v1",
    "download_url": "https://arxiv.org/abs/2504.07322v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A hybrid deep learning approach for medical relation extraction",
    "abstract": "Mining relationships between treatment(s) and medical problem(s) is vital in the biomedical domain. This helps in various applications, such as decision support system, safety surveillance, and new treatment discovery. We propose a deep learning approach that utilizes both word level and sentence-level representations to extract the relationships between treatment and problem. While deep learning techniques demand a large amount of data for training, we make use of a rule-based system particularly for relationship classes with fewer samples. Our final relations are derived by jointly combining the results from deep learning and rule-based models. Our system achieved a promising performance on the relationship classes of I2b2 2010 relation extraction task.",
    "authors": [
      "Veera Raghavendra Chikka",
      "Kamalakar Karlapalem"
    ],
    "publication_date": "2018-06-26T06:38:01Z",
    "arxiv_id": "http://arxiv.org/abs/1806.11189v1",
    "download_url": "https://arxiv.org/abs/1806.11189v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Comment on \"robustness and regularization of support vector machines\" by H. Xu, et al., (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009, arXiv:0803.3490)",
    "abstract": "This paper comments on the published work dealing with robustness and regularization of support vector machines (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. They proposed a theorem to show that it is possible to relate robustness in the feature space and robustness in the sample space directly. In this paper, we propose a counter example that rejects their theorem.",
    "authors": [
      "Yahya Forghani",
      "Hadi Sadoghi Yazdi"
    ],
    "publication_date": "2013-08-17T03:56:03Z",
    "arxiv_id": "http://arxiv.org/abs/1308.3750v1",
    "download_url": "https://arxiv.org/abs/1308.3750v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Coordinated Multi-Agent Imitation Learning",
    "abstract": "We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable. We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for fine-grained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy. We show that having a coordination model to infer the roles of players yields substantially improved imitation loss compared to conventional baselines.",
    "authors": [
      "Hoang M. Le",
      "Yisong Yue",
      "Peter Carr",
      "Patrick Lucey"
    ],
    "publication_date": "2017-03-09T03:45:42Z",
    "arxiv_id": "http://arxiv.org/abs/1703.03121v2",
    "download_url": "https://arxiv.org/abs/1703.03121v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A review and experimental evaluation of deep learning methods for MRI reconstruction",
    "abstract": "Following the success of deep learning in a wide range of applications, neural network-based machine-learning techniques have received significant interest for accelerating magnetic resonance imaging (MRI) acquisition and reconstruction strategies. A number of ideas inspired by deep learning techniques for computer vision and image processing have been successfully applied to nonlinear image reconstruction in the spirit of compressed sensing for accelerated MRI. Given the rapidly growing nature of the field, it is imperative to consolidate and summarize the large number of deep learning methods that have been reported in the literature, to obtain a better understanding of the field in general. This article provides an overview of the recent developments in neural-network based approaches that have been proposed specifically for improving parallel imaging. A general background and introduction to parallel MRI is also given from a classical view of k-space based reconstruction methods. Image domain based techniques that introduce improved regularizers are covered along with k-space based methods which focus on better interpolation strategies using neural networks. While the field is rapidly evolving with plenty of papers published each year, in this review, we attempt to cover broad categories of methods that have shown good performance on publicly available data sets. Limitations and open problems are also discussed and recent efforts for producing open data sets and benchmarks for the community are examined.",
    "authors": [
      "Arghya Pal",
      "Yogesh Rathi"
    ],
    "publication_date": "2021-09-17T15:50:51Z",
    "arxiv_id": "http://arxiv.org/abs/2109.08618v3",
    "download_url": "https://arxiv.org/abs/2109.08618v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Multiple Instance Learning for ECG Risk Stratification",
    "abstract": "Patients who suffer an acute coronary syndrome are at elevated risk for adverse cardiovascular events such as myocardial infarction and cardiovascular death. Accurate assessment of this risk is crucial to their course of care. We focus on estimating a patient's risk of cardiovascular death after an acute coronary syndrome based on a patient's raw electrocardiogram (ECG) signal. Learning from this signal is challenging for two reasons: 1) positive examples signifying a downstream cardiovascular event are scarce, causing drastic class imbalance, and 2) each patient's ECG signal consists of thousands of heartbeats, accompanied by a single label for the downstream outcome. Machine learning has been previously applied to this task, but most approaches rely on hand-crafted features and domain knowledge. We propose a method that learns a representation from the raw ECG signal by using a multiple instance learning framework. We present a learned risk score for cardiovascular death that outperforms existing risk metrics in predicting cardiovascular death within 30, 60, 90, and 365 days on a dataset of 5000 patients.",
    "authors": [
      "Divya Shanmugam",
      "Davis Blalock",
      "John Guttag"
    ],
    "publication_date": "2018-12-02T21:55:53Z",
    "arxiv_id": "http://arxiv.org/abs/1812.00475v4",
    "download_url": "https://arxiv.org/abs/1812.00475v4",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Robust Offline Reinforcement Learning with Linearly Structured f-Divergence Regularization",
    "abstract": "The Robust Regularized Markov Decision Process (RRMDP) is proposed to learn policies robust to dynamics shifts by adding regularization to the transition dynamics in the value function. Existing methods mostly use unstructured regularization, potentially leading to conservative policies under unrealistic transitions. To address this limitation, we propose a novel framework, the $d$-rectangular linear RRMDP ($d$-RRMDP), which introduces latent structures into both transition kernels and regularization. We focus on offline reinforcement learning, where an agent learns policies from a precollected dataset in the nominal environment. We develop the Robust Regularized Pessimistic Value Iteration (R2PVI) algorithm that employs linear function approximation for robust policy learning in $d$-RRMDPs with $f$-divergence based regularization terms on transition kernels. We provide instance-dependent upper bounds on the suboptimality gap of R2PVI policies, demonstrating that these bounds are influenced by how well the dataset covers state-action spaces visited by the optimal robust policy under robustly admissible transitions. We establish information-theoretic lower bounds to verify that our algorithm is near-optimal. Finally, numerical experiments validate that R2PVI learns robust policies and exhibits superior computational efficiency compared to baseline methods.",
    "authors": [
      "Cheng Tang",
      "Zhishuai Liu",
      "Pan Xu"
    ],
    "publication_date": "2024-11-27T18:57:03Z",
    "arxiv_id": "http://arxiv.org/abs/2411.18612v2",
    "download_url": "https://arxiv.org/abs/2411.18612v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Widening Access to Applied Machine Learning with TinyML",
    "abstract": "Broadening access to both computational and educational resources is critical to diffusing machine-learning (ML) innovation. However, today, most ML resources and experts are siloed in a few countries and organizations. In this paper, we describe our pedagogical approach to increasing access to applied ML through a massive open online course (MOOC) on Tiny Machine Learning (TinyML). We suggest that TinyML, ML on resource-constrained embedded devices, is an attractive means to widen access because TinyML both leverages low-cost and globally accessible hardware, and encourages the development of complete, self-contained applications, from data collection to deployment. To this end, a collaboration between academia (Harvard University) and industry (Google) produced a four-part MOOC that provides application-oriented instruction on how to develop solutions using TinyML. The series is openly available on the edX MOOC platform, has no prerequisites beyond basic programming, and is designed for learners from a global variety of backgrounds. It introduces pupils to real-world applications, ML algorithms, data-set engineering, and the ethical considerations of these technologies via hands-on programming and deployment of TinyML applications in both the cloud and their own microcontrollers. To facilitate continued learning, community building, and collaboration beyond the courses, we launched a standalone website, a forum, a chat, and an optional course-project competition. We also released the course materials publicly, hoping they will inspire the next generation of ML practitioners and educators and further broaden access to cutting-edge ML technologies.",
    "authors": [
      "Vijay Janapa Reddi",
      "Brian Plancher",
      "Susan Kennedy",
      "Laurence Moroney",
      "Pete Warden",
      "Anant Agarwal",
      "Colby Banbury",
      "Massimo Banzi",
      "Matthew Bennett",
      "Benjamin Brown",
      "Sharad Chitlangia",
      "Radhika Ghosal",
      "Sarah Grafman",
      "Rupert Jaeger",
      "Srivatsan Krishnan",
      "Maximilian Lam",
      "Daniel Leiker",
      "Cara Mann",
      "Mark Mazumder",
      "Dominic Pajak",
      "Dhilan Ramaprasad",
      "J. Evan Smith",
      "Matthew Stewart",
      "Dustin Tingley"
    ],
    "publication_date": "2021-06-07T23:31:47Z",
    "arxiv_id": "http://arxiv.org/abs/2106.04008v2",
    "download_url": "https://arxiv.org/abs/2106.04008v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Defending Against Adversarial Machine Learning",
    "abstract": "An Adversarial System to attack and an Authorship Attribution System (AAS) to defend itself against the attacks are analyzed. Defending a system against attacks from an adversarial machine learner can be done by randomly switching between models for the system, by detecting and reacting to changes in the distribution of normal inputs, or by using other methods. Adversarial machine learning is used to identify a system that is being used to map system inputs to outputs. Three types of machine learners are using for the model that is being attacked. The machine learners that are used to model the system being attacked are a Radial Basis Function Support Vector Machine, a Linear Support Vector Machine, and a Feedforward Neural Network. The feature masks are evolved using accuracy as the fitness measure. The system defends itself against adversarial machine learning attacks by identifying inputs that do not match the probability distribution of normal inputs. The system also defends itself against adversarial attacks by randomly switching between the feature masks being used to map system inputs to outputs.",
    "authors": [
      "Alison Jenkins"
    ],
    "publication_date": "2019-11-26T18:28:47Z",
    "arxiv_id": "http://arxiv.org/abs/1911.11746v1",
    "download_url": "https://arxiv.org/abs/1911.11746v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning at the Speed of Physics: Equilibrium Propagation on Oscillator Ising Machines",
    "abstract": "Physical systems that naturally perform energy descent offer a direct route to accelerating machine learning. Oscillator Ising Machines (OIMs) exemplify this idea: their GHz-frequency dynamics mirror both the optimization of energy-based models (EBMs) and gradient descent on loss landscapes, while intrinsic noise corresponds to Langevin dynamics - supporting sampling as well as optimization. Equilibrium Propagation (EP) unifies these processes into descent on a single total energy landscape, enabling local learning rules without global backpropagation. We show that EP on OIMs achieves competitive accuracy ($\\sim 97.2 \\pm 0.1 \\%$ on MNIST, $\\sim 88.0 \\pm 0.1 \\%$ on Fashion-MNIST), while maintaining robustness under realistic hardware constraints such as parameter quantization and phase noise. These results establish OIMs as a fast, energy-efficient substrate for neuromorphic learning, and suggest that EBMs - often bottlenecked by conventional processors - may find practical realization on physical hardware whose dynamics directly perform their optimization.",
    "authors": [
      "Alex Gower"
    ],
    "publication_date": "2025-10-14T19:15:49Z",
    "arxiv_id": "http://arxiv.org/abs/2510.12934v2",
    "download_url": "https://arxiv.org/abs/2510.12934v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Data Heterogeneity Modeling for Trustworthy Machine Learning",
    "abstract": "Data heterogeneity plays a pivotal role in determining the performance of machine learning (ML) systems. Traditional algorithms, which are typically designed to optimize average performance, often overlook the intrinsic diversity within datasets. This oversight can lead to a myriad of issues, including unreliable decision-making, inadequate generalization across different domains, unfair outcomes, and false scientific inferences. Hence, a nuanced approach to modeling data heterogeneity is essential for the development of dependable, data-driven systems. In this survey paper, we present a thorough exploration of heterogeneity-aware machine learning, a paradigm that systematically integrates considerations of data heterogeneity throughout the entire ML pipeline -- from data collection and model training to model evaluation and deployment. By applying this approach to a variety of critical fields, including healthcare, agriculture, finance, and recommendation systems, we demonstrate the substantial benefits and potential of heterogeneity-aware ML. These applications underscore how a deeper understanding of data diversity can enhance model robustness, fairness, and reliability and help model diagnosis and improvements. Moreover, we delve into future directions and provide research opportunities for the whole data mining community, aiming to promote the development of heterogeneity-aware ML.",
    "authors": [
      "Jiashuo Liu",
      "Peng Cui"
    ],
    "publication_date": "2025-06-01T11:36:56Z",
    "arxiv_id": "http://arxiv.org/abs/2506.00969v1",
    "download_url": "https://arxiv.org/abs/2506.00969v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Non-convex Optimization for Machine Learning",
    "abstract": "A vast majority of machine learning algorithms train their models and perform inference by solving optimization problems. In order to capture the learning and prediction problems accurately, structural constraints such as sparsity or low rank are frequently imposed or else the objective itself is designed to be a non-convex function. This is especially true of algorithms that operate in high-dimensional spaces or that train non-linear models such as tensor models and deep networks.\n  The freedom to express the learning problem as a non-convex optimization problem gives immense modeling power to the algorithm designer, but often such problems are NP-hard to solve. A popular workaround to this has been to relax non-convex problems to convex ones and use traditional methods to solve the (convex) relaxed optimization problems. However this approach may be lossy and nevertheless presents significant challenges for large scale optimization.\n  On the other hand, direct approaches to non-convex optimization have met with resounding success in several domains and remain the methods of choice for the practitioner, as they frequently outperform relaxation-based techniques - popular heuristics include projected gradient descent and alternating minimization. However, these are often poorly understood in terms of their convergence and other properties.\n  This monograph presents a selection of recent advances that bridge a long-standing gap in our understanding of these heuristics. The monograph will lead the reader through several widely used non-convex optimization techniques, as well as applications thereof. The goal of this monograph is to both, introduce the rich literature in this area, as well as equip the reader with the tools and techniques needed to analyze these simple procedures for non-convex problems.",
    "authors": [
      "Prateek Jain",
      "Purushottam Kar"
    ],
    "publication_date": "2017-12-21T12:05:40Z",
    "arxiv_id": "http://arxiv.org/abs/1712.07897v1",
    "download_url": "https://arxiv.org/abs/1712.07897v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Active Learning for Machine Learning Driven Molecular Dynamics",
    "abstract": "Machine-learned coarse-grained (CG) potentials are fast, but degrade over time when simulations reach under-sampled bio-molecular conformations, and generating widespread all-atom (AA) data to combat this is computationally infeasible. We propose a novel active learning (AL) framework for CG neural network potentials in molecular dynamics (MD). Building on the CGSchNet model, our method employs root mean squared deviation (RMSD)-based frame selection from MD simulations in order to generate data on-the-fly by querying an oracle during the training of a neural network potential. This framework preserves CG-level efficiency while correcting the model at precise, RMSD-identified coverage gaps. By training CGSchNet, a coarse-grained neural network potential, we empirically show that our framework explores previously unseen configurations and trains the model on unexplored regions of conformational space. Our active learning framework enables a CGSchNet model trained on the Chignolin protein to achieve a 33.05\\% improvement in the Wasserstein-1 (W1) metric in Time-lagged Independent Component Analysis (TICA) space on an in-house benchmark suite.",
    "authors": [
      "Kevin Bachelor",
      "Sanya Murdeshwar",
      "Daniel Sabo",
      "Razvan Marinescu"
    ],
    "publication_date": "2025-09-21T19:26:32Z",
    "arxiv_id": "http://arxiv.org/abs/2509.17208v2",
    "download_url": "https://arxiv.org/abs/2509.17208v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Hyperbolic Entailment Cones for Learning Hierarchical Embeddings",
    "abstract": "Learning graph representations via low-dimensional embeddings that preserve relevant network properties is an important class of problems in machine learning. We here present a novel method to embed directed acyclic graphs. Following prior work, we first advocate for using hyperbolic spaces which provably model tree-like structures better than Euclidean geometry. Second, we view hierarchical relations as partial orders defined using a family of nested geodesically convex cones. We prove that these entailment cones admit an optimal shape with a closed form expression both in the Euclidean and hyperbolic spaces, and they canonically define the embedding learning process. Experiments show significant improvements of our method over strong recent baselines both in terms of representational capacity and generalization.",
    "authors": [
      "Octavian-Eugen Ganea",
      "Gary Bécigneul",
      "Thomas Hofmann"
    ],
    "publication_date": "2018-04-03T19:25:10Z",
    "arxiv_id": "http://arxiv.org/abs/1804.01882v3",
    "download_url": "https://arxiv.org/abs/1804.01882v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine Learning for Microcontroller-Class Hardware: A Review",
    "abstract": "The advancements in machine learning opened a new opportunity to bring intelligence to the low-end Internet-of-Things nodes such as microcontrollers. Conventional machine learning deployment has high memory and compute footprint hindering their direct deployment on ultra resource-constrained microcontrollers. This paper highlights the unique requirements of enabling onboard machine learning for microcontroller class devices. Researchers use a specialized model development workflow for resource-limited applications to ensure the compute and latency budget is within the device limits while still maintaining the desired performance. We characterize a closed-loop widely applicable workflow of machine learning model development for microcontroller class devices and show that several classes of applications adopt a specific instance of it. We present both qualitative and numerical insights into different stages of model development by showcasing several use cases. Finally, we identify the open research challenges and unsolved questions demanding careful considerations moving forward.",
    "authors": [
      "Swapnil Sayan Saha",
      "Sandeep Singh Sandha",
      "Mani Srivastava"
    ],
    "publication_date": "2022-05-29T00:59:38Z",
    "arxiv_id": "http://arxiv.org/abs/2205.14550v5",
    "download_url": "https://arxiv.org/abs/2205.14550v5",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in Materials Science",
    "abstract": "We present the Open MatSci ML Toolkit: a flexible, self-contained, and scalable Python-based framework to apply deep learning models and methods on scientific data with a specific focus on materials science and the OpenCatalyst Dataset. Our toolkit provides: 1. A scalable machine learning workflow for materials science leveraging PyTorch Lightning, which enables seamless scaling across different computation capabilities (laptop, server, cluster) and hardware platforms (CPU, GPU, XPU). 2. Deep Graph Library (DGL) support for rapid graph neural network prototyping and development. By publishing and sharing this toolkit with the research community via open-source release, we hope to: 1. Lower the entry barrier for new machine learning researchers and practitioners that want to get started with the OpenCatalyst dataset, which presently comprises the largest computational materials science dataset. 2. Enable the scientific community to apply advanced machine learning tools to high-impact scientific challenges, such as modeling of materials behavior for clean energy applications. We demonstrate the capabilities of our framework by enabling three new equivariant neural network models for multiple OpenCatalyst tasks and arrive at promising results for compute scaling and model performance.",
    "authors": [
      "Santiago Miret",
      "Kin Long Kelvin Lee",
      "Carmelo Gonzales",
      "Marcel Nassar",
      "Matthew Spellings"
    ],
    "publication_date": "2022-10-31T17:11:36Z",
    "arxiv_id": "http://arxiv.org/abs/2210.17484v1",
    "download_url": "https://arxiv.org/abs/2210.17484v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Scientific intuition inspired by machine learning generated hypotheses",
    "abstract": "Machine learning with application to questions in the physical sciences has become a widely used tool, successfully applied to classification, regression and optimization tasks in many areas. Research focus mostly lies in improving the accuracy of the machine learning models in numerical predictions, while scientific understanding is still almost exclusively generated by human researchers analysing numerical results and drawing conclusions. In this work, we shift the focus on the insights and the knowledge obtained by the machine learning models themselves. In particular, we study how it can be extracted and used to inspire human scientists to increase their intuitions and understanding of natural systems. We apply gradient boosting in decision trees to extract human interpretable insights from big data sets from chemistry and physics. In chemistry, we not only rediscover widely know rules of thumb but also find new interesting motifs that tell us how to control solubility and energy levels of organic molecules. At the same time, in quantum physics, we gain new understanding on experiments for quantum entanglement. The ability to go beyond numerics and to enter the realm of scientific insight and hypothesis generation opens the door to use machine learning to accelerate the discovery of conceptual understanding in some of the most challenging domains of science.",
    "authors": [
      "Pascal Friederich",
      "Mario Krenn",
      "Isaac Tamblyn",
      "Alan Aspuru-Guzik"
    ],
    "publication_date": "2020-10-27T12:12:12Z",
    "arxiv_id": "http://arxiv.org/abs/2010.14236v2",
    "download_url": "https://arxiv.org/abs/2010.14236v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Towards Efficient Data-Centric Robust Machine Learning with Noise-based Augmentation",
    "abstract": "The data-centric machine learning aims to find effective ways to build appropriate datasets which can improve the performance of AI models. In this paper, we mainly focus on designing an efficient data-centric scheme to improve robustness for models towards unforeseen malicious inputs in the black-box test settings. Specifically, we introduce a noised-based data augmentation method which is composed of Gaussian Noise, Salt-and-Pepper noise, and the PGD adversarial perturbations. The proposed method is built on lightweight algorithms and proved highly effective based on comprehensive evaluations, showing good efficiency on computation cost and robustness enhancement. In addition, we share our insights about the data-centric robust machine learning gained from our experiments.",
    "authors": [
      "Xiaogeng Liu",
      "Haoyu Wang",
      "Yechao Zhang",
      "Fangzhou Wu",
      "Shengshan Hu"
    ],
    "publication_date": "2022-03-08T02:05:40Z",
    "arxiv_id": "http://arxiv.org/abs/2203.03810v1",
    "download_url": "https://arxiv.org/abs/2203.03810v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Ethnography and Machine Learning: Synergies and New Directions",
    "abstract": "Ethnography (social scientific methods that illuminate how people understand, navigate and shape the real world contexts in which they live their lives) and machine learning (computational techniques that use big data and statistical learning models to perform quantifiable tasks) are each core to contemporary social science. Yet these tools have remained largely separate in practice. This chapter draws on a growing body of scholarship that argues that ethnography and machine learning can be usefully combined, particularly for large comparative studies. Specifically, this paper (a) explains the value (and challenges) of using machine learning alongside qualitative field research for certain types of projects, (b) discusses recent methodological trends to this effect, (c) provides examples that illustrate workflow drawn from several large projects, and (d) concludes with a roadmap for enabling productive coevolution of field methods and machine learning.",
    "authors": [
      "Zhuofan Li",
      "Corey M. Abramson"
    ],
    "publication_date": "2024-12-08T22:28:05Z",
    "arxiv_id": "http://arxiv.org/abs/2412.06087v1",
    "download_url": "https://arxiv.org/abs/2412.06087v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction",
    "abstract": "Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.",
    "authors": [
      "Yiting He",
      "Zhishuai Liu",
      "Weixin Wang",
      "Pan Xu"
    ],
    "publication_date": "2025-11-07T16:24:22Z",
    "arxiv_id": "http://arxiv.org/abs/2511.05396v1",
    "download_url": "https://arxiv.org/abs/2511.05396v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Evaluating the Economic Implications of Using Machine Learning in Clinical Psychiatry",
    "abstract": "With the growing interest in using AI and machine learning (ML) in medicine, there is an increasing number of literature covering the application and ethics of using AI and ML in areas of medicine such as clinical psychiatry. The problem is that there is little literature covering the economic aspects associated with using ML in clinical psychiatry. This study addresses this gap by specifically studying the economic implications of using ML in clinical psychiatry. In this paper, we evaluate the economic implications of using ML in clinical psychiatry through using three problem-oriented case studies, literature on economics, socioeconomic and medical AI, and two types of health economic evaluations. In addition, we provide details on fairness, legal, ethics and other considerations for ML in clinical psychiatry.",
    "authors": [
      "Soaad Hossain",
      "James Rasalingam",
      "Arhum Waheed",
      "Fatah Awil",
      "Rachel Kandiah",
      "Syed Ishtiaque Ahmed"
    ],
    "publication_date": "2024-11-07T01:57:06Z",
    "arxiv_id": "http://arxiv.org/abs/2411.05856v1",
    "download_url": "https://arxiv.org/abs/2411.05856v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Edge Machine Learning for Cluster Counting in Next-Generation Drift Chambers",
    "abstract": "Drift chambers have long been central to collider tracking, but future machines like a Higgs factory motivate higher granularity and cluster counting for particle ID, posing new data processing challenges. Machine learning (ML) at the \"edge\", or in cell-level readout, can dramatically reduce the off-detector data rate for high-granularity drift chambers by performing cluster counting at-source. We present machine learning algorithms for cluster counting in real-time readout of future drift chambers. These algorithms outperform traditional derivative-based techniques based on achievable pion-kaon separation. When synthesized to FPGA resources, they can achieve latencies consistent with real-time operation in a future Higgs factory scenario, thus advancing both R&D for future collider detectors as well as hardware-based ML for edge applications in high energy physics.",
    "authors": [
      "Deniz Yilmaz",
      "Liangyu Wu",
      "Julia Gonski",
      "Dylan Rankin",
      "Christian Herwig"
    ],
    "publication_date": "2025-11-13T17:39:22Z",
    "arxiv_id": "http://arxiv.org/abs/2511.10540v2",
    "download_url": "https://arxiv.org/abs/2511.10540v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Interlacing Personal and Reference Genomes for Machine Learning Disease-Variant Detection",
    "abstract": "DNA sequencing to identify genetic variants is becoming increasingly valuable in clinical settings. Assessment of variants in such sequencing data is commonly implemented through Bayesian heuristic algorithms. Machine learning has shown great promise in improving on these variant calls, but the input for these is still a standardized \"pile-up\" image, which is not always best suited. In this paper, we present a novel method for generating images from DNA sequencing data, which interlaces the human reference genome with personalized sequencing output, to maximize usage of sequencing reads and improve machine learning algorithm performance. We demonstrate the success of this in improving standard germline variant calling. We also furthered this approach to include somatic variant calling across tumor/normal data with Siamese networks. These approaches can be used in machine learning applications on sequencing data with the hope of improving clinical outcomes, and are freely available for noncommercial use at www.ccg.ai.",
    "authors": [
      "Luke R Harries",
      "Suyi Zhang",
      "Geoffroy Dubourg-Felonneau",
      "James H R Farmery",
      "Jonathan Sinai",
      "Belle Taylor",
      "Nirmesh Patel",
      "John W Cassidy",
      "John Shawe-Taylor",
      "Harry W Clifford"
    ],
    "publication_date": "2018-11-26T15:38:29Z",
    "arxiv_id": "http://arxiv.org/abs/1811.11674v1",
    "download_url": "https://arxiv.org/abs/1811.11674v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Generalization in anti-causal learning",
    "abstract": "The ability to learn and act in novel situations is still a prerogative of animate intelligence, as current machine learning methods mostly fail when moving beyond the standard i.i.d. setting. What is the reason for this discrepancy? Most machine learning tasks are anti-causal, i.e., we infer causes (labels) from effects (observations). Typically, in supervised learning we build systems that try to directly invert causal mechanisms. Instead, in this paper we argue that strong generalization capabilities crucially hinge on searching and validating meaningful hypotheses, requiring access to a causal model. In such a framework, we want to find a cause that leads to the observed effect. Anti-causal models are used to drive this search, but a causal model is required for validation. We investigate the fundamental differences between causal and anti-causal tasks, discuss implications for topics ranging from adversarial attacks to disentangling factors of variation, and provide extensive evidence from the literature to substantiate our view. We advocate for incorporating causal models in supervised learning to shift the paradigm from inference only, to search and validation.",
    "authors": [
      "Niki Kilbertus",
      "Giambattista Parascandolo",
      "Bernhard Schölkopf"
    ],
    "publication_date": "2018-12-03T02:26:35Z",
    "arxiv_id": "http://arxiv.org/abs/1812.00524v1",
    "download_url": "https://arxiv.org/abs/1812.00524v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Sparse Projection Oblique Randomer Forests",
    "abstract": "Decision forests, including Random Forests and Gradient Boosting Trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. Decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. In contrast, many recent extensions to decision forests are based on axis-oblique splits. Unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. We introduce yet another decision forest, called \"Sparse Projection Oblique Randomer Forests\" (SPORF). SPORF uses very sparse random projections, i.e., linear combinations of a small subset of features. SPORF significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with >100 problems of varying dimension, sample size, and number of classes. To illustrate how SPORF addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. SPORF typically yields improved performance over existing decision forests, while mitigating computational efficiency and scalability and maintaining interpretability. SPORF can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains.",
    "authors": [
      "Tyler M. Tomita",
      "James Browne",
      "Cencheng Shen",
      "Jaewon Chung",
      "Jesse L. Patsolic",
      "Benjamin Falk",
      "Jason Yim",
      "Carey E. Priebe",
      "Randal Burns",
      "Mauro Maggioni",
      "Joshua T. Vogelstein"
    ],
    "publication_date": "2015-06-10T17:55:51Z",
    "arxiv_id": "http://arxiv.org/abs/1506.03410v6",
    "download_url": "https://arxiv.org/abs/1506.03410v6",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning",
    "abstract": "Applications of machine learning (ML) are growing by the day for many unique and challenging scientific applications. However, a crucial challenge facing these applications is their need for ultra low-latency and on-detector ML capabilities. Given the slowdown in Moore's law and Dennard scaling, coupled with the rapid advances in scientific instrumentation that is resulting in growing data rates, there is a need for ultra-fast ML at the extreme edge. Fast ML at the edge is essential for reducing and filtering scientific data in real-time to accelerate science experimentation and enable more profound insights. To accelerate real-time scientific edge ML hardware and software solutions, we need well-constrained benchmark tasks with enough specifications to be generically applicable and accessible. These benchmarks can guide the design of future edge ML hardware for scientific applications capable of meeting the nanosecond and microsecond level latency requirements. To this end, we present an initial set of scientific ML benchmarks, covering a variety of ML and embedded system techniques.",
    "authors": [
      "Javier Duarte",
      "Nhan Tran",
      "Ben Hawks",
      "Christian Herwig",
      "Jules Muhizi",
      "Shvetank Prakash",
      "Vijay Janapa Reddi"
    ],
    "publication_date": "2022-07-16T14:30:15Z",
    "arxiv_id": "http://arxiv.org/abs/2207.07958v1",
    "download_url": "https://arxiv.org/abs/2207.07958v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Airbnb Price Prediction Using Machine Learning and Sentiment Analysis",
    "abstract": "Pricing a rental property on Airbnb is a challenging task for the owner as it determines the number of customers for the place. On the other hand, customers have to evaluate an offered price with minimal knowledge of an optimal value for the property. This paper aims to develop a reliable price prediction model using machine learning, deep learning, and natural language processing techniques to aid both the property owners and the customers with price evaluation given minimal available information about the property. Features of the rentals, owner characteristics, and the customer reviews will comprise the predictors, and a range of methods from linear regression to tree-based models, support-vector regression (SVR), K-means Clustering (KMC), and neural networks (NNs) will be used for creating the prediction model.",
    "authors": [
      "Pouya Rezazadeh Kalehbasti",
      "Liubov Nikolenko",
      "Hoormazd Rezaei"
    ],
    "publication_date": "2019-07-29T21:45:42Z",
    "arxiv_id": "http://arxiv.org/abs/1907.12665v1",
    "download_url": "https://arxiv.org/abs/1907.12665v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Towards Wide Learning: Experiments in Healthcare",
    "abstract": "In this paper, a Wide Learning architecture is proposed that attempts to automate the feature engineering portion of the machine learning (ML) pipeline. Feature engineering is widely considered as the most time consuming and expert knowledge demanding portion of any ML task. The proposed feature recommendation approach is tested on 3 healthcare datasets: a) PhysioNet Challenge 2016 dataset of phonocardiogram (PCG) signals, b) MIMIC II blood pressure classification dataset of photoplethysmogram (PPG) signals and c) an emotion classification dataset of PPG signals. While the proposed method beats the state of the art techniques for 2nd and 3rd dataset, it reaches 94.38% of the accuracy level of the winner of PhysioNet Challenge 2016. In all cases, the effort to reach a satisfactory performance was drastically less (a few days) than manual feature engineering.",
    "authors": [
      "Snehasis Banerjee",
      "Tanushyam Chattopadhyay",
      "Swagata Biswas",
      "Rohan Banerjee",
      "Anirban Dutta Choudhury",
      "Arpan Pal",
      "Utpal Garain"
    ],
    "publication_date": "2016-12-17T11:00:49Z",
    "arxiv_id": "http://arxiv.org/abs/1612.05730v2",
    "download_url": "https://arxiv.org/abs/1612.05730v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Peering Partner Recommendation for ISPs using Machine Learning",
    "abstract": "Internet service providers (ISPs) need to connect with other ISPs to provide global connectivity services to their users. To ensure global connectivity, ISPs can either use transit service(s) or establish direct peering relationships between themselves via Internet exchange points (IXPs). Peering offers more room for ISP-specific optimizations and is preferred, but it often involves a lengthy and complex process. Automating peering partner selection can enhance efficiency in the global Internet ecosystem. We explore the use of publicly available data on ISPs to develop a machine learning (ML) model that can predict whether an ISP pair should peer or not. At first, we explore public databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we evaluate the performance of three broad types of ML models for predicting peering relationships: tree-based, neural network-based, and transformer-based. Among these, we observe that tree-based models achieve the highest accuracy and efficiency in our experiments. The XGBoost model trained with publicly available data showed promising performance, with a 98% accuracy rate in predicting peering partners. In addition, the model demonstrated great resilience to variations in time, space, and missing data. We envision that ISPs can adopt our method to fully automate the peering partner selection process, thus transitioning to a more efficient and optimized Internet ecosystem.",
    "authors": [
      "Md Ibrahim Ibne Alam",
      "Ankur Senapati",
      "Anindo Mahmood",
      "Murat Yuksel",
      "Koushik Kar"
    ],
    "publication_date": "2025-09-11T04:43:31Z",
    "arxiv_id": "http://arxiv.org/abs/2509.09146v1",
    "download_url": "https://arxiv.org/abs/2509.09146v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Open Problems in Engineering and Quality Assurance of Safety Critical Machine Learning Systems",
    "abstract": "Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems using machine-learning and deep-learning models, such as automated-driving vehicles. Quality assurance frameworks are required for such machine learning systems, but there are no widely accepted and established quality-assurance concepts and techniques. At the same time, open problems and the relevant technical fields are not organized. To establish standard quality assurance frameworks, it is necessary to visualize and organize these open problems in an interdisciplinary way, so that the experts from many different technical fields may discuss these problems in depth and develop solutions. In the present study, we identify, classify, and explore the open problems in quality assurance of safety-critical machine-learning systems, and their relevant corresponding industry and technological trends, using automated-driving vehicles as an example. Our results show that addressing these open problems requires incorporating knowledge from several different technological and industrial fields, including the automobile industry, statistics, software engineering, and machine learning.",
    "authors": [
      "Hiroshi Kuwajima",
      "Hirotoshi Yasuoka",
      "Toshihiro Nakae"
    ],
    "publication_date": "2018-12-07T15:02:40Z",
    "arxiv_id": "http://arxiv.org/abs/1812.03057v1",
    "download_url": "https://arxiv.org/abs/1812.03057v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Meta-Learning Probabilistic Inference For Prediction",
    "abstract": "This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.",
    "authors": [
      "Jonathan Gordon",
      "John Bronskill",
      "Matthias Bauer",
      "Sebastian Nowozin",
      "Richard E. Turner"
    ],
    "publication_date": "2018-05-24T22:08:27Z",
    "arxiv_id": "http://arxiv.org/abs/1805.09921v4",
    "download_url": "https://arxiv.org/abs/1805.09921v4",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Mislabeled examples detection viewed as probing machine learning models: concepts, survey and extensive benchmark",
    "abstract": "Mislabeled examples are ubiquitous in real-world machine learning datasets, advocating the development of techniques for automatic detection. We show that most mislabeled detection methods can be viewed as probing trained machine learning models using a few core principles. We formalize a modular framework that encompasses these methods, parameterized by only 4 building blocks, as well as a Python library that demonstrates that these principles can actually be implemented. The focus is on classifier-agnostic concepts, with an emphasis on adapting methods developed for deep learning models to non-deep classifiers for tabular data. We benchmark existing methods on (artificial) Completely At Random (NCAR) as well as (realistic) Not At Random (NNAR) labeling noise from a variety of tasks with imperfect labeling rules. This benchmark provides new insights as well as limitations of existing methods in this setup.",
    "authors": [
      "Thomas George",
      "Pierre Nodet",
      "Alexis Bondu",
      "Vincent Lemaire"
    ],
    "publication_date": "2024-10-21T08:32:02Z",
    "arxiv_id": "http://arxiv.org/abs/2410.15772v1",
    "download_url": "https://arxiv.org/abs/2410.15772v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Framework and Benchmark for Deep Batch Active Learning for Regression",
    "abstract": "The acquisition of labels for supervised learning can be expensive. To improve the sample efficiency of neural network regression, we study active learning methods that adaptively select batches of unlabeled data for labeling. We present a framework for constructing such methods out of (network-dependent) base kernels, kernel transformations, and selection methods. Our framework encompasses many existing Bayesian methods based on Gaussian process approximations of neural networks as well as non-Bayesian methods. Additionally, we propose to replace the commonly used last-layer features with sketched finite-width neural tangent kernels and to combine them with a novel clustering method. To evaluate different methods, we introduce an open-source benchmark consisting of 15 large tabular regression data sets. Our proposed method outperforms the state-of-the-art on our benchmark, scales to large data sets, and works out-of-the-box without adjusting the network architecture or training code. We provide open-source code that includes efficient implementations of all kernels, kernel transformations, and selection methods, and can be used for reproducing our results.",
    "authors": [
      "David Holzmüller",
      "Viktor Zaverkin",
      "Johannes Kästner",
      "Ingo Steinwart"
    ],
    "publication_date": "2022-03-17T16:11:36Z",
    "arxiv_id": "http://arxiv.org/abs/2203.09410v4",
    "download_url": "https://arxiv.org/abs/2203.09410v4",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Distributed and Secure Kernel-Based Quantum Machine Learning",
    "abstract": "Quantum computing promises to revolutionize machine learning, offering significant efficiency gains in tasks such as clustering and distance estimation. Additionally, it provides enhanced security through fundamental principles like the measurement postulate and the no-cloning theorem, enabling secure protocols such as quantum teleportation and quantum key distribution. While advancements in secure quantum machine learning are notable, the development of secure and distributed quantum analogues of kernel-based machine learning techniques remains underexplored.\n  In this work, we present a novel approach for securely computing common kernels, including polynomial, radial basis function (RBF), and Laplacian kernels, when data is distributed, using quantum feature maps. Our methodology introduces a robust framework that leverages quantum teleportation to ensure secure and distributed kernel learning. The proposed architecture is validated using IBM's Qiskit Aer Simulator on various public datasets.",
    "authors": [
      "Arjhun Swaminathan",
      "Mete Akgün"
    ],
    "publication_date": "2024-08-16T06:31:45Z",
    "arxiv_id": "http://arxiv.org/abs/2408.10265v3",
    "download_url": "https://arxiv.org/abs/2408.10265v3",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine Learning Methods Applied to Cortico-Cortical Evoked Potentials Aid in Localizing Seizure Onset Zones",
    "abstract": "Epilepsy affects millions of people, reducing quality of life and increasing risk of premature death. One-third of epilepsy cases are drug-resistant and require surgery for treatment, which necessitates localizing the seizure onset zone (SOZ) in the brain. Attempts have been made to use cortico-cortical evoked potentials (CCEPs) to improve SOZ localization but none have been successful enough for clinical adoption. Here, we compare the performance of ten machine learning classifiers in localizing SOZ from CCEP data. This preliminary study validates a novel application of machine learning, and the results establish our approach as a promising line of research that warrants further investigation. This work also serves to facilitate discussion and collaboration with fellow machine learning and/or epilepsy researchers.",
    "authors": [
      "Ian G. Malone",
      "Kaleb E. Smith",
      "Morgan E. Urdaneta",
      "Tyler S. Davis",
      "Daria Nesterovich Anderson",
      "Brian J. Phillip",
      "John D. Rolston",
      "Christopher R. Butson"
    ],
    "publication_date": "2022-11-15T03:18:37Z",
    "arxiv_id": "http://arxiv.org/abs/2211.07867v1",
    "download_url": "https://arxiv.org/abs/2211.07867v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Deep Reinforcement Learning",
    "abstract": "We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.",
    "authors": [
      "Yuxi Li"
    ],
    "publication_date": "2018-10-15T13:20:56Z",
    "arxiv_id": "http://arxiv.org/abs/1810.06339v1",
    "download_url": "https://arxiv.org/abs/1810.06339v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Quantum Geometric Machine Learning for Quantum Circuits and Control",
    "abstract": "The application of machine learning techniques to solve problems in quantum control together with established geometric methods for solving optimisation problems leads naturally to an exploration of how machine learning approaches can be used to enhance geometric approaches to solving problems in quantum information processing. In this work, we review and extend the application of deep learning to quantum geometric control problems. Specifically, we demonstrate enhancements in time-optimal control in the context of quantum circuit synthesis problems by applying novel deep learning algorithms in order to approximate geodesics (and thus minimal circuits) along Lie group manifolds relevant to low-dimensional multi-qubit systems, such as SU(2), SU(4) and SU(8). We demonstrate the superior performance of greybox models, which combine traditional blackbox algorithms with prior domain knowledge of quantum mechanics, as means of learning underlying quantum circuit distributions of interest. Our results demonstrate how geometric control techniques can be used to both (a) verify the extent to which geometrically synthesised quantum circuits lie along geodesic, and thus time-optimal, routes and (b) synthesise those circuits. Our results are of interest to researchers in quantum control and quantum information theory seeking to combine machine learning and geometric techniques for time-optimal control problems.",
    "authors": [
      "Elija Perrier",
      "Christopher Ferrie",
      "Dacheng Tao"
    ],
    "publication_date": "2020-06-19T19:12:14Z",
    "arxiv_id": "http://arxiv.org/abs/2006.11332v2",
    "download_url": "https://arxiv.org/abs/2006.11332v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Fair Machine Learning under Limited Demographically Labeled Data",
    "abstract": "Research has shown that, machine learning models might inherit and propagate undesired social biases encoded in the data. To address this problem, fair training algorithms are developed. However, most algorithms assume we know demographic/sensitive data features such as gender and race. This assumption falls short in scenarios where collecting demographic information is not feasible due to privacy concerns, and data protection policies. A recent line of work develops fair training methods that can function without any demographic feature on the data, that are collectively referred as Rawlsian methods. Yet, we show in experiments that, Rawlsian methods tend to exhibit relatively high bias. Given this, we look at the middle ground between the previous approaches, and consider a setting where we know the demographic attributes for only a small subset of our data. In such a setting, we design fair training algorithms which exhibit both good utility, and low bias. In particular, we show that our techniques can train models to significantly outperform Rawlsian approaches even when 0.1% of demographic attributes are available in the training data. Furthermore, our main algorithm can accommodate multiple training objectives easily. We expand our main algorithm to achieve robustness to label noise in addition to fairness in the limited demographics setting to highlight that property as well.",
    "authors": [
      "Mustafa Safa Ozdayi",
      "Murat Kantarcioglu",
      "Rishabh Iyer"
    ],
    "publication_date": "2021-06-03T22:36:17Z",
    "arxiv_id": "http://arxiv.org/abs/2106.04757v2",
    "download_url": "https://arxiv.org/abs/2106.04757v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems",
    "abstract": "MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters.\n  This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.",
    "authors": [
      "Tianqi Chen",
      "Mu Li",
      "Yutian Li",
      "Min Lin",
      "Naiyan Wang",
      "Minjie Wang",
      "Tianjun Xiao",
      "Bing Xu",
      "Chiyuan Zhang",
      "Zheng Zhang"
    ],
    "publication_date": "2015-12-03T22:49:21Z",
    "arxiv_id": "http://arxiv.org/abs/1512.01274v1",
    "download_url": "https://arxiv.org/abs/1512.01274v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A Scoping Review of Earth Observation and Machine Learning for Causal Inference: Implications for the Geography of Poverty",
    "abstract": "Earth observation (EO) data such as satellite imagery can have far-reaching impacts on our understanding of the geography of poverty, especially when coupled with machine learning (ML) and computer vision. Early research used computer vision to predict living conditions in areas with limited data, but recent studies increasingly focus on causal analysis. Despite this shift, the use of EO-ML methods for causal inference lacks thorough documentation, and best practices are still developing. Through a comprehensive scoping review, we catalog the current literature on EO-ML methods in causal analysis. We synthesize five principal approaches to incorporating EO data in causal workflows: (1) outcome imputation for downstream causal analysis, (2) EO image deconfounding, (3) EO-based treatment effect heterogeneity, (4) EO-based transportability analysis, and (5) image-informed causal discovery. Building on these findings, we provide a detailed protocol guiding researchers in integrating EO data into causal analysis -- covering data requirements, computer vision model selection, and evaluation metrics. While our focus centers on health and living conditions outcomes, our protocol is adaptable to other sustainable development domains utilizing EO data.",
    "authors": [
      "Kazuki Sakamoto",
      "Connor T. Jerzak",
      "Adel Daoud"
    ],
    "publication_date": "2024-05-30T20:48:10Z",
    "arxiv_id": "http://arxiv.org/abs/2406.02584v4",
    "download_url": "https://arxiv.org/abs/2406.02584v4",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine learning for sports betting: should model selection be based on accuracy or calibration?",
    "abstract": "Sports betting's recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to reliably predict the probability of an outcome, they can recognise when the bookmaker's odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of predictive models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. We show that using calibration, rather than accuracy, as the basis for model selection leads to greater returns, on average (return on investment of $+34.69\\%$ versus $-35.17\\%$) and in the best case ($+36.93\\%$ versus $+5.56\\%$). These findings suggest that for sports betting (or any probabilistic decision-making problem), calibration is a more important metric than accuracy. Sports bettors who wish to increase profits should therefore select their predictive model based on calibration, rather than accuracy.",
    "authors": [
      "Conor Walsh",
      "Alok Joshi"
    ],
    "publication_date": "2023-03-10T16:22:38Z",
    "arxiv_id": "http://arxiv.org/abs/2303.06021v4",
    "download_url": "https://arxiv.org/abs/2303.06021v4",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Physics Encoded Blocks in Residual Neural Network Architectures for Digital Twin Models",
    "abstract": "Physics Informed Machine Learning has emerged as a popular approach for modeling and simulation in digital twins, enabling the generation of accurate models of processes and behaviors in real-world systems. However, existing methods either rely on simple loss regularizations that offer limited physics integration or employ highly specialized architectures that are difficult to generalize across diverse physical systems. This paper presents a generic approach based on a novel physics-encoded residual neural network (PERNN) architecture that seamlessly combines data-driven and physics-based analytical models to overcome these limitations. Our method integrates differentiable physics blocks-implementing mathematical operators from physics-based models with feed-forward learning blocks, while intermediate residual blocks ensure stable gradient flow during training. Consequently, the model naturally adheres to the underlying physical principles even when prior physics knowledge is incomplete, thereby improving generalizability with low data requirements and reduced model complexity. We investigate our approach in two application domains. The first is a steering model for autonomous vehicles in a simulation environment, and the second is a digital twin for climate modeling using an ordinary differential equation (ODE)-based model of Net Ecosystem Exchange (NEE) to enable gap-filling in flux tower data. In both cases, our method outperforms conventional neural network approaches as well as state-of-the-art Physics Informed Machine Learning methods.",
    "authors": [
      "Muhammad Saad Zia",
      "Ashiq Anjum",
      "Lu Liu",
      "Anthony Conway",
      "Anasol Pena Rios"
    ],
    "publication_date": "2024-11-18T11:58:20Z",
    "arxiv_id": "http://arxiv.org/abs/2411.11497v2",
    "download_url": "https://arxiv.org/abs/2411.11497v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine Learning on the COVID-19 Pandemic, Human Mobility and Air Quality: A Review",
    "abstract": "The ongoing COVID-19 global pandemic is affecting every facet of human lives (e.g., public health, education, economy, transportation, and the environment). This novel pandemic and citywide implemented lockdown measures are affecting virus transmission, people's travel patterns, and air quality. Many studies have been conducted to predict the COVID-19 diffusion, assess the impacts of the pandemic on human mobility and air quality, and assess the impacts of lockdown measures on viral spread with a range of Machine Learning (ML) techniques. This review study aims to analyze results from past research to understand the interactions among the COVID-19 pandemic, lockdown measures, human mobility, and air quality. The critical review of prior studies indicates that urban form, people's socioeconomic and physical conditions, social cohesion, and social distancing measures significantly affect human mobility and COVID-19 transmission. during the COVID-19 pandemic, many people are inclined to use private transportation for necessary travel purposes to mitigate coronavirus-related health problems. This review study also noticed that COVID-19 related lockdown measures significantly improve air quality by reducing the concentration of air pollutants, which in turn improves the COVID-19 situation by reducing respiratory-related sickness and deaths of people. It is argued that ML is a powerful, effective, and robust analytic paradigm to handle complex and wicked problems such as a global pandemic. This study also discusses policy implications, which will be helpful for policymakers to take prompt actions to moderate the severity of the pandemic and improve urban environments by adopting data-driven analytic methods.",
    "authors": [
      "Md. Mokhlesur Rahman",
      "Kamal Chandra Paul",
      "Md. Amjad Hossain",
      "G. G. Md. NawazAli",
      "Md. Shahinoor Rahman",
      "Jean-Claude Thill"
    ],
    "publication_date": "2021-03-13T10:08:24Z",
    "arxiv_id": "http://arxiv.org/abs/2104.04059v1",
    "download_url": "https://arxiv.org/abs/2104.04059v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Similarity Learning for Provably Accurate Sparse Linear Classification",
    "abstract": "In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to state-of-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.",
    "authors": [
      "Aurelien Bellet",
      "Amaury Habrard",
      "Marc Sebban"
    ],
    "publication_date": "2012-06-27T19:59:59Z",
    "arxiv_id": "http://arxiv.org/abs/1206.6476v1",
    "download_url": "https://arxiv.org/abs/1206.6476v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Progressive Sampling-Based Bayesian Optimization for Efficient and Automatic Machine Learning Model Selection",
    "abstract": "Purpose: Machine learning is broadly used for clinical data analysis. Before training a model, a machine learning algorithm must be selected. Also, the values of one or more model parameters termed hyper-parameters must be set. Selecting algorithms and hyper-parameter values requires advanced machine learning knowledge and many labor-intensive manual iterations. To lower the bar to machine learning, miscellaneous automatic selection methods for algorithms and/or hyper-parameter values have been proposed. Existing automatic selection methods are inefficient on large data sets. This poses a challenge for using machine learning in the clinical big data era. Methods: To address the challenge, this paper presents progressive sampling-based Bayesian optimization, an efficient and automatic selection method for both algorithms and hyper-parameter values. Results: We report an implementation of the method. We show that compared to a state of the art automatic selection method, our method can significantly reduce search time, classification error rate, and standard deviation of error rate due to randomization. Conclusions: This is major progress towards enabling fast turnaround in identifying high-quality solutions required by many machine learning-based clinical data analysis tasks.",
    "authors": [
      "Xueqiang Zeng",
      "Gang Luo"
    ],
    "publication_date": "2018-12-06T23:46:15Z",
    "arxiv_id": "http://arxiv.org/abs/1812.02855v1",
    "download_url": "https://arxiv.org/abs/1812.02855v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "A comparative analysis of Graph Neural Networks and commonly used machine learning algorithms on fake news detection",
    "abstract": "Fake news on social media is increasingly regarded as one of the most concerning issues. Low cost, simple accessibility via social platforms, and a plethora of low-budget online news sources are some of the factors that contribute to the spread of false news. Most of the existing fake news detection algorithms are solely focused on the news content only but engaged users prior posts or social activities provide a wealth of information about their views on news and have significant ability to improve fake news identification. Graph Neural Networks are a form of deep learning approach that conducts prediction on graph-described data. Social media platforms are followed graph structure in their representation, Graph Neural Network are special types of neural networks that could be usually applied to graphs, making it much easier to execute edge, node, and graph-level prediction. Therefore, in this paper, we present a comparative analysis among some commonly used machine learning algorithms and Graph Neural Networks for detecting the spread of false news on social media platforms. In this study, we take the UPFD dataset and implement several existing machine learning algorithms on text data only. Besides this, we create different GNN layers for fusing graph-structured news propagation data and the text data as the node feature in our GNN models. GNNs provide the best solutions to the dilemma of identifying false news in our research.",
    "authors": [
      "Fahim Belal Mahmud",
      "Mahi Md. Sadek Rayhan",
      "Mahdi Hasan Shuvo",
      "Islam Sadia",
      "Md. Kishor Morol"
    ],
    "publication_date": "2022-03-26T18:40:03Z",
    "arxiv_id": "http://arxiv.org/abs/2203.14132v1",
    "download_url": "https://arxiv.org/abs/2203.14132v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Improve State-Level Wheat Yield Forecasts in Kazakhstan on GEOGLAM's EO Data by Leveraging A Simple Spatial-Aware Technique",
    "abstract": "Accurate yield forecasting is essential for making informed policies and long-term decisions for food security. Earth Observation (EO) data and machine learning algorithms play a key role in providing a comprehensive and timely view of crop conditions from field to national scales. However, machine learning algorithms' prediction accuracy is often harmed by spatial heterogeneity caused by exogenous factors not reflected in remote sensing data, such as differences in crop management strategies. In this paper, we propose and investigate a simple technique called state-wise additive bias to explicitly address the cross-region yield heterogeneity in Kazakhstan. Compared to baseline machine learning models (Random Forest, CatBoost, XGBoost), our method reduces the overall RMSE by 8.9\\% and the highest state-wise RMSE by 28.37\\%. The effectiveness of state-wise additive bias indicates machine learning's performance can be significantly improved by explicitly addressing the spatial heterogeneity, motivating future work on spatial-aware machine learning algorithms for yield forecasts as well as for general geospatial forecasting problems.",
    "authors": [
      "Anh Nhat Nhu",
      "Ritvik Sahajpal",
      "Christina Justice",
      "Inbal Becker-Reshef"
    ],
    "publication_date": "2023-06-01T19:35:13Z",
    "arxiv_id": "http://arxiv.org/abs/2306.04646v1",
    "download_url": "https://arxiv.org/abs/2306.04646v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Deep Learning Based Event Reconstruction for Cyclotron Radiation Emission Spectroscopy",
    "abstract": "The objective of the Cyclotron Radiation Emission Spectroscopy (CRES) technology is to build precise particle energy spectra. This is achieved by identifying the start frequencies of charged particle trajectories which, when exposed to an external magnetic field, leave semi-linear profiles (called tracks) in the time-frequency plane. Due to the need for excellent instrumental energy resolution in application, highly efficient and accurate track reconstruction methods are desired. Deep learning convolutional neural networks (CNNs) - particularly suited to deal with information-sparse data and which offer precise foreground localization - may be utilized to extract track properties from measured CRES signals (called events) with relative computational ease. In this work, we develop a novel machine learning based model which operates a CNN and a support vector machine in tandem to perform this reconstruction. A primary application of our method is shown on simulated CRES signals which mimic those of the Project 8 experiment - a novel effort to extract the unknown absolute neutrino mass value from a precise measurement of tritium $β^-$-decay energy spectrum. When compared to a point-clustering based technique used as a baseline, we show a relative gain of 24.1% in event reconstruction efficiency and comparable performance in accuracy of track parameter reconstruction.",
    "authors": [
      "A. Ashtari Esfahani",
      "S. Böser",
      "N. Buzinsky",
      "M. C. Carmona-Benitez",
      "R. Cervantes",
      "C. Claessens",
      "L. de Viveiros",
      "M. Fertl",
      "J. A. Formaggio",
      "J. K. Gaison",
      "L. Gladstone",
      "M. Grando",
      "M. Guigue",
      "J. Hartse",
      "K. M. Heeger",
      "X. Huyan",
      "A. M. Jones",
      "K. Kazkaz",
      "M. Li",
      "A. Lindman",
      "A. Marsteller",
      "C. Matthé",
      "R. Mohiuddin",
      "B. Monreal",
      "E. C. Morrison",
      "R. Mueller",
      "J. A. Nikkel",
      "E. Novitski",
      "N. S. Oblath",
      "J. I. Peña",
      "W. Pettus",
      "R. Reimann",
      "R. G. H. Robertson",
      "L. Saldaña",
      "M. Schram",
      "P. L. Slocum",
      "J. Stachurska",
      "Y. -H. Sun",
      "P. T. Surukuchi",
      "A. B. Telles",
      "F. Thomas",
      "M. Thomas",
      "L. A. Thorne",
      "T. Thümmler",
      "L. Tvrznikova",
      "W. Van De Pontseele",
      "B. A. VanDevender",
      "T. E. Weiss",
      "T. Wendler",
      "E. Zayas",
      "A. Ziegler"
    ],
    "publication_date": "2024-01-05T15:55:27Z",
    "arxiv_id": "http://arxiv.org/abs/2402.13256v1",
    "download_url": "https://arxiv.org/abs/2402.13256v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Spatially-resolved Thermometry from Line-of-Sight Emission Spectroscopy via Machine Learning",
    "abstract": "A methodology is proposed, which addresses the caveat that line-of-sight emission spectroscopy presents in that it cannot provide spatially resolved temperature measurements in nonhomogeneous temperature fields. The aim of this research is to explore the use of data-driven models in measuring temperature distributions in a spatially resolved manner using emission spectroscopy data. Two categories of data-driven methods are analyzed: (i) Feature engineering and classical machine learning algorithms, and (ii) end-to-end convolutional neural networks (CNN). In total, combinations of fifteen feature groups and fifteen classical machine learning models, and eleven CNN models are considered and their performances explored. The results indicate that the combination of feature engineering and machine learning provides better performance than the direct use of CNN. Notably, feature engineering which is comprised of physics-guided transformation, signal representation-based feature extraction and Principal Component Analysis is found to be the most effective. Moreover, it is shown that when using the extracted features, the ensemble-based, light blender learning model offers the best performance with RMSE, RE, RRMSE and R values of 64.3, 0.017, 0.025 and 0.994, respectively. The proposed method, based on feature engineering and the light blender model, is capable of measuring nonuniform temperature distributions from low-resolution spectra, even when the species concentration distribution in the gas mixtures is unknown.",
    "authors": [
      "Ruiyuan Kang",
      "Dimitrios C. Kyritsis",
      "Panos Liatsis"
    ],
    "publication_date": "2022-12-15T13:46:15Z",
    "arxiv_id": "http://arxiv.org/abs/2212.07836v1",
    "download_url": "https://arxiv.org/abs/2212.07836v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and Asynchronous Machine Learning",
    "abstract": "The joint progress of artificial neural networks (ANNs) and domain specific hardware accelerators such as GPUs and TPUs took over many domains of machine learning research. This development is accompanied by a rapid growth of the required computational demands for larger models and more data. Concurrently, emerging properties of foundation models such as in-context learning drive new opportunities for machine learning applications. However, the computational cost of such applications is a limiting factor of the technology in data centers, and more importantly in mobile devices and edge systems. To mediate the energy footprint and non-trivial latency of contemporary systems, neuromorphic computing systems deeply integrate computational principles of neurobiological systems by leveraging low-power analog and digital technologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable machine learning. The event-based and asynchronous design of SpiNNaker2 allows the composition of large-scale systems involving thousands of chips. This work features the operating principles of SpiNNaker2 systems, outlining the prototype of novel machine learning applications. These applications range from ANNs over bio-inspired spiking neural networks to generalized event-based neural networks. With the successful development and deployment of SpiNNaker2, we aim to facilitate the advancement of event-based and asynchronous algorithms for future generations of machine learning systems.",
    "authors": [
      "Hector A. Gonzalez",
      "Jiaxin Huang",
      "Florian Kelber",
      "Khaleelulla Khan Nazeer",
      "Tim Langer",
      "Chen Liu",
      "Matthias Lohrmann",
      "Amirhossein Rostami",
      "Mark Schöne",
      "Bernhard Vogginger",
      "Timo C. Wunderlich",
      "Yexin Yan",
      "Mahmoud Akl",
      "Christian Mayr"
    ],
    "publication_date": "2024-01-09T11:07:48Z",
    "arxiv_id": "http://arxiv.org/abs/2401.04491v1",
    "download_url": "https://arxiv.org/abs/2401.04491v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Learning interpretable models of phenotypes from whole genome sequences with the Set Covering Machine",
    "abstract": "The increased affordability of whole genome sequencing has motivated its use for phenotypic studies. We address the problem of learning interpretable models for discrete phenotypes from whole genomes. We propose a general approach that relies on the Set Covering Machine and a k-mer representation of the genomes. We show results for the problem of predicting the resistance of Pseudomonas Aeruginosa, an important human pathogen, against 4 antibiotics. Our results demonstrate that extremely sparse models which are biologically relevant can be learnt using this approach.",
    "authors": [
      "Alexandre Drouin",
      "Sébastien Giguère",
      "Vladana Sagatovich",
      "Maxime Déraspe",
      "François Laviolette",
      "Mario Marchand",
      "Jacques Corbeil"
    ],
    "publication_date": "2014-12-02T13:26:50Z",
    "arxiv_id": "http://arxiv.org/abs/1412.1074v1",
    "download_url": "https://arxiv.org/abs/1412.1074v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications",
    "abstract": "Topological analysis of the magnetic field in simulated plasmas allows the study of various physical phenomena in a wide range of settings. One such application is magnetic reconnection, a phenomenon related to the dynamics of the magnetic field topology, which is difficult to detect and characterize in three dimensions. We propose a scalable pipeline for topological data analysis and spatiotemporal graph representation of three-dimensional magnetic vector fields. We demonstrate our methods on simulations of the Earth's magnetosphere produced by Vlasiator, a supercomputer-scale Vlasov theory-based simulation for near-Earth space. The purpose of this work is to challenge the machine learning community to explore graph-based machine learning approaches to address a largely open scientific problem with wide-ranging potential impact.",
    "authors": [
      "Ioanna Bouri",
      "Fanni Franssila",
      "Markku Alho",
      "Giulia Cozzani",
      "Ivan Zaitsev",
      "Minna Palmroth",
      "Teemu Roos"
    ],
    "publication_date": "2023-07-10T18:08:05Z",
    "arxiv_id": "http://arxiv.org/abs/2307.09469v2",
    "download_url": "https://arxiv.org/abs/2307.09469v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Modelling tourism demand to Spain with machine learning techniques. The impact of forecast horizon on model selection",
    "abstract": "This study assesses the influence of the forecast horizon on the forecasting performance of several machine learning techniques. We compare the fo recast accuracy of Support Vector Regression (SVR) to Neural Network (NN) models, using a linear model as a benchmark. We focus on international tourism demand to all seventeen regions of Spain. The SVR with a Gaussian radial basis function kernel outperforms the rest of the models for the longest forecast horizons. We also find that machine learning methods improve their forecasting accuracy with respect to linear models as forecast horizons increase. This result shows the suitability of SVR for medium and long term forecasting.",
    "authors": [
      "Oscar Claveria",
      "Enric Monte",
      "Salvador Torra"
    ],
    "publication_date": "2018-05-02T15:48:11Z",
    "arxiv_id": "http://arxiv.org/abs/1805.00878v1",
    "download_url": "https://arxiv.org/abs/1805.00878v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Disparate Censorship & Undertesting: A Source of Label Bias in Clinical Machine Learning",
    "abstract": "As machine learning (ML) models gain traction in clinical applications, understanding the impact of clinician and societal biases on ML models is increasingly important. While biases can arise in the labels used for model training, the many sources from which these biases arise are not yet well-studied. In this paper, we highlight disparate censorship (i.e., differences in testing rates across patient groups) as a source of label bias that clinical ML models may amplify, potentially causing harm. Many patient risk-stratification models are trained using the results of clinician-ordered diagnostic and laboratory tests of labels. Patients without test results are often assigned a negative label, which assumes that untested patients do not experience the outcome. Since orders are affected by clinical and resource considerations, testing may not be uniform in patient populations, giving rise to disparate censorship. Disparate censorship in patients of equivalent risk leads to undertesting in certain groups, and in turn, more biased labels for such groups. Using such biased labels in standard ML pipelines could contribute to gaps in model performance across patient groups. Here, we theoretically and empirically characterize conditions in which disparate censorship or undertesting affect model performance across subgroups. Our findings call attention to disparate censorship as a source of label bias in clinical ML models.",
    "authors": [
      "Trenton Chang",
      "Michael W. Sjoding",
      "Jenna Wiens"
    ],
    "publication_date": "2022-08-01T20:15:31Z",
    "arxiv_id": "http://arxiv.org/abs/2208.01127v1",
    "download_url": "https://arxiv.org/abs/2208.01127v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Cost-Sensitive Machine Learning Classification for Mass Tuberculosis Verbal Screening",
    "abstract": "Score-based algorithms for tuberculosis (TB) verbal screening perform poorly, causing misclassification that leads to missed cases and unnecessary costly laboratory tests for false positives. We compared score-based classification defined by clinicians to machine learning classification such as SVM-RBF, logistic regression, and XGBoost. We restricted our analyses to data from adults, the population most affected by TB, and investigated the difference between untuned and unweighted classifiers to the cost-sensitive ones. Predictions were compared with the corresponding GeneXpert MTB/Rif results. After adjusting the weight of the positive class to 40 for XGBoost, we achieved 96.64% sensitivity and 35.06% specificity. As such, the sensitivity of our identifier increased by 1.26% while specificity increased by 13.19% in absolute value compared to the traditional score-based method defined by our clinicians. Our approach further demonstrated that only 2000 data points were sufficient to enable the model to converge. The results indicate that even with limited data we can actually devise a better method to identify TB suspects from verbal screening.",
    "authors": [
      "Ali Akbar Septiandri",
      "Aditiawarman",
      "Roy Tjiong",
      "Erlina Burhan",
      "Anuraj Shankar"
    ],
    "publication_date": "2020-11-14T21:41:29Z",
    "arxiv_id": "http://arxiv.org/abs/2011.07396v1",
    "download_url": "https://arxiv.org/abs/2011.07396v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Alleviating Privacy Attacks via Causal Learning",
    "abstract": "Machine learning models, especially deep neural networks have been shown to be susceptible to privacy attacks such as membership inference where an adversary can detect whether a data point was used for training a black-box model. Such privacy risks are exacerbated when a model's predictions are used on an unseen data distribution. To alleviate privacy attacks, we demonstrate the benefit of predictive models that are based on the causal relationships between input features and the outcome. We first show that models learnt using causal structure generalize better to unseen data, especially on data from different distributions than the train distribution. Based on this generalization property, we establish a theoretical link between causality and privacy: compared to associational models, causal models provide stronger differential privacy guarantees and are more robust to membership inference attacks. Experiments on simulated Bayesian networks and the colored-MNIST dataset show that associational models exhibit upto 80% attack accuracy under different test distributions and sample sizes whereas causal models exhibit attack accuracy close to a random guess.",
    "authors": [
      "Shruti Tople",
      "Amit Sharma",
      "Aditya Nori"
    ],
    "publication_date": "2019-09-27T15:06:42Z",
    "arxiv_id": "http://arxiv.org/abs/1909.12732v4",
    "download_url": "https://arxiv.org/abs/1909.12732v4",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Limits of Transfer Learning",
    "abstract": "Transfer learning involves taking information and insight from one problem domain and applying it to a new problem domain. Although widely used in practice, theory for transfer learning remains less well-developed. To address this, we prove several novel results related to transfer learning, showing the need to carefully select which sets of information to transfer and the need for dependence between transferred information and target problems. Furthermore, we prove how the degree of probabilistic change in an algorithm using transfer learning places an upper bound on the amount of improvement possible. These results build on the algorithmic search framework for machine learning, allowing the results to apply to a wide range of learning problems using transfer.",
    "authors": [
      "Jake Williams",
      "Abel Tadesse",
      "Tyler Sam",
      "Huey Sun",
      "George D. Montanez"
    ],
    "publication_date": "2020-06-23T01:48:23Z",
    "arxiv_id": "http://arxiv.org/abs/2006.12694v1",
    "download_url": "https://arxiv.org/abs/2006.12694v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Machine Learning for Fraud Detection in E-Commerce: A Research Agenda",
    "abstract": "Fraud detection and prevention play an important part in ensuring the sustained operation of any e-commerce business. Machine learning (ML) often plays an important role in these anti-fraud operations, but the organizational context in which these ML models operate cannot be ignored. In this paper, we take an organization-centric view on the topic of fraud detection by formulating an operational model of the anti-fraud departments in e-commerce organizations. We derive 6 research topics and 12 practical challenges for fraud detection from this operational model. We summarize the state of the literature for each research topic, discuss potential solutions to the practical challenges, and identify 22 open research challenges.",
    "authors": [
      "Niek Tax",
      "Kees Jan de Vries",
      "Mathijs de Jong",
      "Nikoleta Dosoula",
      "Bram van den Akker",
      "Jon Smith",
      "Olivier Thuong",
      "Lucas Bernardi"
    ],
    "publication_date": "2021-07-05T12:37:29Z",
    "arxiv_id": "http://arxiv.org/abs/2107.01979v1",
    "download_url": "https://arxiv.org/abs/2107.01979v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Evolutionary Machine Learning and Games",
    "abstract": "Evolutionary machine learning (EML) has been applied to games in multiple ways, and for multiple different purposes. Importantly, AI research in games is not only about playing games; it is also about generating game content, modeling players, and many other applications. Many of these applications pose interesting problems for EML. We will structure this chapter on EML for games based on whether evolution is used to augment machine learning (ML) or ML is used to augment evolution. For completeness, we also briefly discuss the usage of ML and evolution separately in games.",
    "authors": [
      "Julian Togelius",
      "Ahmed Khalifa",
      "Sam Earle",
      "Michael Cerny Green",
      "Lisa Soros"
    ],
    "publication_date": "2023-11-20T13:21:39Z",
    "arxiv_id": "http://arxiv.org/abs/2311.16172v1",
    "download_url": "https://arxiv.org/abs/2311.16172v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Gradient Boosting Reinforcement Learning",
    "abstract": "We present Gradient Boosting Reinforcement Learning (GBRL), a framework that adapts the strengths of gradient boosting trees (GBT) to reinforcement learning (RL) tasks. While neural networks (NNs) have become the de facto choice for RL, they face significant challenges with structured and categorical features and tend to generalize poorly to out-of-distribution samples. These are challenges for which GBTs have traditionally excelled in supervised learning. However, GBT's application in RL has been limited. The design of traditional GBT libraries is optimized for static datasets with fixed labels, making them incompatible with RL's dynamic nature, where both state distributions and reward signals evolve during training. GBRL overcomes this limitation by continuously interleaving tree construction with environment interaction. Through extensive experiments, we demonstrate that GBRL outperforms NNs in domains with structured observations and categorical features while maintaining competitive performance on standard continuous control benchmarks. Like its supervised learning counterpart, GBRL demonstrates superior robustness to out-of-distribution samples and better handles irregular state-action relationships.",
    "authors": [
      "Benjamin Fuhrer",
      "Chen Tessler",
      "Gal Dalal"
    ],
    "publication_date": "2024-07-11T07:52:33Z",
    "arxiv_id": "http://arxiv.org/abs/2407.08250v2",
    "download_url": "https://arxiv.org/abs/2407.08250v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using PDEs",
    "abstract": "Many physical processes can be expressed through partial differential equations (PDEs). Real-world measurements of such processes are often collected at irregularly distributed points in space, which can be effectively represented as graphs; however, there are currently only a few existing datasets. Our work aims to make advancements in the field of PDE-modeling accessible to the temporal graph machine learning community, while addressing the data scarcity problem, by creating and utilizing datasets based on PDEs. In this work, we create and use synthetic datasets based on PDEs to support spatio-temporal graph modeling in machine learning for different applications. More precisely, we showcase three equations to model different types of disasters and hazards in the fields of epidemiology, atmospheric particles, and tsunami waves. Further, we show how such created datasets can be used by benchmarking several machine learning models on the epidemiological dataset. Additionally, we show how pre-training on this dataset can improve model performance on real-world epidemiological data. The presented methods enable others to create datasets and benchmarks customized to individual requirements. The source code for our methodology and the three created datasets can be found on https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs.",
    "authors": [
      "Jost Arndt",
      "Utku Isil",
      "Michael Detzel",
      "Wojciech Samek",
      "Jackie Ma"
    ],
    "publication_date": "2025-02-06T15:20:32Z",
    "arxiv_id": "http://arxiv.org/abs/2502.04140v2",
    "download_url": "https://arxiv.org/abs/2502.04140v2",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  },
  {
    "title": "Integrating Reinforcement Learning to Self Training for Pulmonary Nodule Segmentation in Chest X-rays",
    "abstract": "Machine learning applications in medical imaging are frequently limited by the lack of quality labeled data. In this paper, we explore the self training method, a form of semi-supervised learning, to address the labeling burden. By integrating reinforcement learning, we were able to expand the application of self training to complex segmentation networks without any further human annotation. The proposed approach, reinforced self training (ReST), fine tunes a semantic segmentation networks by introducing a policy network that learns to generate pseudolabels. We incorporate an expert demonstration network, based on inverse reinforcement learning, to enhance clinical validity and convergence of the policy network. The model was tested on a pulmonary nodule segmentation task in chest X-rays and achieved the performance of a standard U-Net while using only 50% of the labeled data, by exploiting unlabeled data. When the same number of labeled data was used, a moderate to significant cross validation accuracy improvement was achieved depending on the absolute number of labels used.",
    "authors": [
      "Sejin Park",
      "Woochan Hwang",
      "Kyu-Hwan Jung"
    ],
    "publication_date": "2018-11-21T17:37:22Z",
    "arxiv_id": "http://arxiv.org/abs/1811.08840v1",
    "download_url": "https://arxiv.org/abs/1811.08840v1",
    "field_of_study": "computer_science",
    "document_type": "preprint",
    "citation_count": null,
    "source_repository": "arXiv"
  }
]